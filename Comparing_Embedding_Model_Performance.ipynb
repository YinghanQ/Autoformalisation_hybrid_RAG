{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88218074",
   "metadata": {},
   "source": [
    "# \n",
    "## 1. Comparing the Performance of Text Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e8e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Word Similarity Models Evaluation Framework (Fixed Version)\n",
    "Evaluate the performance of multiple word similarity models on various datasets\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.utils import Bunch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import your data loading functions\n",
    "from load_datasets import *\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Word Similarity Model Evaluator\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def load_gensim_model(self, model_name):\n",
    "        \"\"\"Load Gensim models (Word2Vec, GloVe, etc.)\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading {model_name}...\")\n",
    "            import gensim.downloader as gensim_api\n",
    "            model = gensim_api.load(MODELS[model_name])\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {model_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_sentence_transformer_model(self, model_name):\n",
    "        \"\"\"Load Sentence Transformer models\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading {model_name}...\")\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            model = SentenceTransformer(MODELS[model_name])\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {model_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_bert_model_simple(self, model_name):\n",
    "        \"\"\"Load BERT models using a simplified method\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading {model_name} with simple method...\")\n",
    "            # Attempt to wrap BERT models using sentence-transformers\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            \n",
    "            # For BERT models, we can use sentence-transformers as a wrapper\n",
    "            if \"bert\" in model_name.lower():\n",
    "                model = SentenceTransformer(MODELS[model_name])\n",
    "                return model\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {model_name} with simple method: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def compute_gensim_similarity(self, model, word1, word2):\n",
    "        \"\"\"Compute word similarity using Gensim models\"\"\"\n",
    "        try:\n",
    "            return model.similarity(word1, word2)\n",
    "        except (KeyError, ValueError):\n",
    "            return np.nan\n",
    "    \n",
    "    def compute_sentence_transformer_similarity(self, model, word1, word2):\n",
    "        \"\"\"Compute word similarity using Sentence Transformer models\"\"\"\n",
    "        try:\n",
    "            embeddings = model.encode([word1, word2])\n",
    "            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "            return float(similarity)\n",
    "        except Exception as e:\n",
    "            return np.nan\n",
    "    \n",
    "    def load_all_models(self):\n",
    "        \"\"\"Load all models\"\"\"\n",
    "        # Model configurations\n",
    "        model_configs = {\n",
    "            # Gensim models\n",
    "            \"Glove\": {\"type\": \"gensim\", \"model_key\": \"glove-wiki-gigaword-300\"},\n",
    "            \"Word2Vec\": {\"type\": \"gensim\", \"model_key\": \"word2vec-google-news-300\"},\n",
    "            \n",
    "            # Sentence Transformer models\n",
    "            \"mpnet-base-v2\": {\"type\": \"sentence_transformer\", \"model_key\": \"sentence-transformers/all-mpnet-base-v2\"},\n",
    "            \"distilroberta-v1\": {\"type\": \"sentence_transformer\", \"model_key\": \"sentence-transformers/all-distilroberta-v1\"},\n",
    "            \"defsent-roberta\": {\"type\": \"sentence_transformer\", \"model_key\": \"sentence-transformers/all-roberta-large-v1\"},\n",
    "            \"sentence-t5-large\": {\"type\": \"sentence_transformer\", \"model_key\": \"sentence-transformers/sentence-t5-large\"},\n",
    "            \n",
    "            # Simplified BERT models (wrapped via sentence-transformers)\n",
    "            \"bert-base\": {\"type\": \"bert_simple\", \"model_key\": \"bert-base-uncased\"},\n",
    "            \"bert-large\": {\"type\": \"bert_simple\", \"model_key\": \"bert-large-uncased\"},\n",
    "        }\n",
    "        \n",
    "        for model_name, config in model_configs.items():\n",
    "            try:\n",
    "                if config[\"type\"] == \"gensim\":\n",
    "                    model = self.load_gensim_model_by_key(config[\"model_key\"])\n",
    "                    if model is not None:\n",
    "                        self.models[model_name] = {\"model\": model, \"type\": \"gensim\"}\n",
    "                \n",
    "                elif config[\"type\"] == \"sentence_transformer\":\n",
    "                    model = self.load_sentence_transformer_model_by_key(config[\"model_key\"])\n",
    "                    if model is not None:\n",
    "                        self.models[model_name] = {\"model\": model, \"type\": \"sentence_transformer\"}\n",
    "                \n",
    "                elif config[\"type\"] == \"bert_simple\":\n",
    "                    model = self.load_bert_model_simple_by_key(config[\"model_key\"])\n",
    "                    if model is not None:\n",
    "                        self.models[model_name] = {\"model\": model, \"type\": \"sentence_transformer\"}\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {model_name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    def load_gensim_model_by_key(self, model_key):\n",
    "        \"\"\"Load Gensim models by model key (enhanced version)\"\"\"\n",
    "        try:\n",
    "            import gensim.downloader as gensim_api\n",
    "            \n",
    "            # Check if the model has been downloaded\n",
    "            if model_key not in gensim_api.info()['models']:\n",
    "                print(f\"Model {model_key} not found in gensim repository. Downloading...\")\n",
    "            \n",
    "            # Load the model and print vocabulary information\n",
    "            model = gensim_api.load(model_key)\n",
    "            print(f\"Loaded gensim model: {model_key}\")\n",
    "            print(f\"  Vocabulary size: {len(model.key_to_index)}\")\n",
    "            print(f\"  Sample words: {list(model.key_to_index.keys())[:5]}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading gensim model {model_key}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_sentence_transformer_model_by_key(self, model_key):\n",
    "        \"\"\"Load Sentence Transformer models by model key\"\"\"\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            return SentenceTransformer(model_key)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sentence transformer {model_key}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_bert_model_simple_by_key(self, model_key):\n",
    "        \"\"\"Load BERT models by model key (simplified version)\"\"\"\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            return SentenceTransformer(model_key)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading BERT model {model_key}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def evaluate_on_dataset(self, dataset_name, dataset_loader_func):\n",
    "        \"\"\"Evaluate all models on a specific dataset\"\"\"\n",
    "        print(f\"\\nEvaluating on {dataset_name}...\")\n",
    "        \n",
    "        # Load dataset\n",
    "        try:\n",
    "            data = dataset_loader_func()\n",
    "            word_pairs = data.X\n",
    "            human_scores = data.y\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset {dataset_name}: {e}\")\n",
    "            return\n",
    "        \n",
    "        dataset_results = {}\n",
    "        \n",
    "        for model_name, model_info in self.models.items():\n",
    "            print(f\"  Evaluating {model_name}...\")\n",
    "            \n",
    "            model_scores = []\n",
    "            valid_indices = []\n",
    "            \n",
    "            for i, (word1, word2) in enumerate(word_pairs):\n",
    "                word1, word2 = str(word1).strip(), str(word2).strip()\n",
    "                \n",
    "                if model_info[\"type\"] == \"gensim\":\n",
    "                    similarity = self.compute_gensim_similarity(\n",
    "                        model_info[\"model\"], word1, word2\n",
    "                    )\n",
    "                elif model_info[\"type\"] == \"sentence_transformer\":\n",
    "                    similarity = self.compute_sentence_transformer_similarity(\n",
    "                        model_info[\"model\"], word1, word2\n",
    "                    )\n",
    "                else:\n",
    "                    similarity = np.nan\n",
    "                \n",
    "                if not np.isnan(similarity):\n",
    "                    model_scores.append(similarity)\n",
    "                    valid_indices.append(i)\n",
    "            \n",
    "            if len(model_scores) > 0:\n",
    "                # Filter valid human scores\n",
    "                valid_human_scores = [human_scores[i] for i in valid_indices]\n",
    "                \n",
    "                # Calculate correlations\n",
    "                try:\n",
    "                    pearson_corr, pearson_p = pearsonr(model_scores, valid_human_scores)\n",
    "                    spearman_corr, spearman_p = spearmanr(model_scores, valid_human_scores)\n",
    "                except:\n",
    "                    pearson_corr = spearman_corr = np.nan\n",
    "                \n",
    "                dataset_results[model_name] = {\n",
    "                    \"pearson\": pearson_corr,\n",
    "                    \"spearman\": spearman_corr,\n",
    "                    \"coverage\": len(model_scores) / len(word_pairs),\n",
    "                    \"n_pairs\": len(model_scores)\n",
    "                }\n",
    "                \n",
    "                print(f\"    Pearson: {pearson_corr:.3f}, Spearman: {spearman_corr:.3f}, Coverage: {len(model_scores)}/{len(word_pairs)}\")\n",
    "            else:\n",
    "                print(f\"    No valid predictions for {model_name}\")\n",
    "                dataset_results[model_name] = {\n",
    "                    \"pearson\": np.nan,\n",
    "                    \"spearman\": np.nan,\n",
    "                    \"coverage\": 0.0,\n",
    "                    \"n_pairs\": 0\n",
    "                }\n",
    "        \n",
    "        self.results[dataset_name] = dataset_results\n",
    "    \n",
    "    def run_full_evaluation(self):\n",
    "        \"\"\"Run the full evaluation\"\"\"\n",
    "        print(\"Starting model evaluation...\")\n",
    "        \n",
    "        # Load all models\n",
    "        self.load_all_models()\n",
    "        print(f\"Successfully loaded {len(self.models)} models: {list(self.models.keys())}\")\n",
    "        \n",
    "        if len(self.models) == 0:\n",
    "            print(\"No models were loaded successfully. Please check your environment.\")\n",
    "            return\n",
    "        \n",
    "        # Define datasets and their corresponding loader functions\n",
    "        datasets = {\n",
    "            # \"SimLex999\": fetch_SimLex999,\n",
    "            \"WordSim353\": lambda: fetch_WS353(\"all\"),\n",
    "            \"WordSim353-sim\": lambda: fetch_WS353(\"similarity\"),\n",
    "            \"WordSim353-rel\": lambda: fetch_WS353(\"relatedness\"),\n",
    "            \"MEN-dev\": lambda: fetch_MEN(\"dev\"),\n",
    "            \"MEN-test\": lambda: fetch_MEN(\"test\"),\n",
    "            \"MEN\": lambda: fetch_MEN(\"all\"),\n",
    "            \"RG65\": fetch_RG65,\n",
    "            \"SCWS\": fetch_SCWS,  \n",
    "            \"SimVerb3500\": lambda: fetch_SimVerb3500(\"all\"), \n",
    "            \"SimVerb3500-dev\": lambda: fetch_SimVerb3500(\"dev\"),\n",
    "            \"SimVerb3500-test\": lambda: fetch_SimVerb3500(\"test\"), \n",
    "        }\n",
    "        \n",
    "        # Attempt to add more datasets (if available)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            datasets[\"SimVerb3500\"] = lambda: fetch_SimVerb3500(\"all\")\n",
    "        except:\n",
    "            print(\"SimVerb3500 dataset not available\")\n",
    "            \n",
    "        try:\n",
    "            datasets[\"SCWS\"] = fetch_SCWS\n",
    "        except:\n",
    "            print(\"SCWS dataset not available\")\n",
    "        \"\"\"\n",
    "        # Evaluate on each dataset\n",
    "        for dataset_name, loader_func in datasets.items():\n",
    "            try:\n",
    "                self.evaluate_on_dataset(dataset_name, loader_func)\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {dataset_name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    def print_results_summary(self):\n",
    "        \"\"\"Print results summary\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to display!\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EVALUATION RESULTS SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        all_results = []\n",
    "        \n",
    "        for dataset_name, dataset_results in self.results.items():\n",
    "            for model_name, metrics in dataset_results.items():\n",
    "                all_results.append({\n",
    "                    \"Dataset\": dataset_name,\n",
    "                    \"Model\": model_name,\n",
    "                    \"Pearson\": metrics.get(\"pearson\", np.nan),\n",
    "                    \"Spearman\": metrics.get(\"spearman\", np.nan),\n",
    "                    \"Coverage\": metrics.get(\"coverage\", 0),\n",
    "                    \"N_Pairs\": metrics.get(\"n_pairs\", 0)\n",
    "                })\n",
    "        \n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        # Display results grouped by dataset\n",
    "        for dataset in results_df['Dataset'].unique():\n",
    "            dataset_df = results_df[results_df['Dataset'] == dataset].copy()\n",
    "            dataset_df = dataset_df.sort_values('Spearman', ascending=False, na_position='last')\n",
    "            \n",
    "            print(f\"\\n{dataset}:\")\n",
    "            print(\"-\" * 70)\n",
    "            print(\"Model\".ljust(20), \"Pearson\".rjust(8), \"Spearman\".rjust(8), \n",
    "                  \"Coverage\".rjust(10), \"N_Pairs\".rjust(8))\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            for _, row in dataset_df.iterrows():\n",
    "                pearson_str = f\"{row['Pearson']:.3f}\" if not pd.isna(row['Pearson']) else \"N/A\"\n",
    "                spearman_str = f\"{row['Spearman']:.3f}\" if not pd.isna(row['Spearman']) else \"N/A\"\n",
    "                coverage_str = f\"{row['Coverage']:.2%}\"\n",
    "                \n",
    "                print(f\"{row['Model']:<20} {pearson_str:>8} {spearman_str:>8} {coverage_str:>10} {row['N_Pairs']:>8}\")\n",
    "    \n",
    "    def save_detailed_results(self, filepath=\"model_evaluation_results.csv\"):\n",
    "        \"\"\"Save detailed results to a CSV file\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to save!\")\n",
    "            return\n",
    "        \n",
    "        all_results = []\n",
    "        for dataset_name, dataset_results in self.results.items():\n",
    "            for model_name, metrics in dataset_results.items():\n",
    "                all_results.append({\n",
    "                    \"Dataset\": dataset_name,\n",
    "                    \"Model\": model_name,\n",
    "                    \"Pearson_Correlation\": metrics.get(\"pearson\", np.nan),\n",
    "                    \"Spearman_Correlation\": metrics.get(\"spearman\", np.nan),\n",
    "                    \"Coverage\": metrics.get(\"coverage\", 0),\n",
    "                    \"Valid_Pairs\": metrics.get(\"n_pairs\", 0)\n",
    "                })\n",
    "        \n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv(filepath, index=False)\n",
    "        print(f\"\\nDetailed results saved to: {filepath}\")\n",
    "        \n",
    "        # Create pivot table showing model rankings\n",
    "        pivot_spearman = results_df.pivot(index='Model', columns='Dataset', values='Spearman_Correlation')\n",
    "        \n",
    "        # Calculate average performance (ignoring NaN values)\n",
    "        pivot_spearman['Average'] = pivot_spearman.mean(axis=1, skipna=True)\n",
    "        pivot_spearman = pivot_spearman.sort_values('Average', ascending=False, na_position='last')\n",
    "        \n",
    "        print(\"\\nSpearman Correlation Matrix (sorted by average performance):\")\n",
    "        print(pivot_spearman.round(3))\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    # Check dependencies\n",
    "    missing_deps = []\n",
    "    \n",
    "    try:\n",
    "        import gensim\n",
    "    except ImportError:\n",
    "        missing_deps.append(\"gensim\")\n",
    "    \n",
    "    try:\n",
    "        import sentence_transformers\n",
    "    except ImportError:\n",
    "        missing_deps.append(\"sentence-transformers\")\n",
    "    \n",
    "    if missing_deps:\n",
    "        print(f\"Missing dependencies: {missing_deps}\")\n",
    "        print(\"Please install with: pip install \" + \" \".join(missing_deps))\n",
    "        return None, None\n",
    "    \n",
    "    evaluator = ModelEvaluator()\n",
    "    \n",
    "    # Run full evaluation\n",
    "    evaluator.run_full_evaluation()\n",
    "    \n",
    "    # Display results\n",
    "    evaluator.print_results_summary()\n",
    "    \n",
    "    # Save results\n",
    "    results_df = evaluator.save_detailed_results()\n",
    "    \n",
    "    return evaluator, results_df\n",
    "\n",
    "# Global model mapping (simplified version)\n",
    "MODELS = {\n",
    "    \"glove-wiki-gigaword-300\": \"glove-wiki-gigaword-300\",\n",
    "    \"word2vec-google-news-300\": \"word2vec-google-news-300\", \n",
    "    \"sentence-transformers/all-mpnet-base-v2\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"sentence-transformers/all-distilroberta-v1\": \"sentence-transformers/all-distilroberta-v1\",\n",
    "   \"sentence-transformers/all-roberta-large-v1\": \"sentence-transformers/all-roberta-large-v1\",\n",
    "    \"sentence-transformers/sentence-t5-large\": \"sentence-transformers/sentence-t5-large\",\n",
    "    \"bert-base-uncased\": \"bert-base-uncased\",\n",
    "    \"bert-large-uncased\": \"bert-large-uncased\",\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluator, results = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0731e45a",
   "metadata": {},
   "source": [
    "## 2.  Multi-Relation Hyperbolic Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bbb0714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 20:10:13,094 - INFO - Using device: cpu\n",
      "2025-08-23 20:10:13,094 - INFO - Project directory: /Users/chouyinghan/my_mathlib_project/Demo_in_Matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory not found: /Users/chouyinghan/my_mathlib_project/Demo_in_Matrix/your_model_directory\n",
      "Invalid path. Exiting.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 606\u001b[39m\n\u001b[32m    603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m evaluator, results_df\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m     evaluator, results = main()\n",
      "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Hyperbolic Embeddings Evaluation Framework (Optimized Version)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.utils import Bunch\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure WordNet data is downloaded\n",
    "try:\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Error downloading NLTK resources: {e}\")\n",
    "\n",
    "# Import data loading functions\n",
    "try:\n",
    "    from load_datasets import fetch_SimVerb3500, fetch_MEN, fetch_SimLex999, fetch_SCWS, fetch_WS353, fetch_RG65\n",
    "except ImportError:\n",
    "    logger.warning(\"load_datasets module not found. Using fallback implementations.\")\n",
    "    # Provide simple fallback implementations\n",
    "    def fetch_SimVerb3500():\n",
    "        return Bunch(X=np.array([], dtype=object), y=np.array([]))\n",
    "    def fetch_MEN():\n",
    "        return Bunch(X=np.array([], dtype=object), y=np.array([]))\n",
    "    def fetch_SimLex999():\n",
    "        return Bunch(X=np.array([], dtype=object), y=np.array([]))\n",
    "    def fetch_SCWS():\n",
    "        return Bunch(X=np.array([], dtype=object), y=np.array([]))\n",
    "    def fetch_WS353():\n",
    "        return Bunch(X=np.array([], dtype=object), y=np.array([]))\n",
    "    def fetch_RG65():\n",
    "        return Bunch(X=np.array([], dtype=object), y=np.array([]))\n",
    "\n",
    "class HyperbolicEvaluator:\n",
    "    \"\"\"Hyperbolic Embeddings Evaluator (Optimized Version)\"\"\"\n",
    "    \n",
    "    def __init__(self, project_dir, device=None):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.wordnet_pos_map = {\n",
    "            'n': 'noun', 'v': 'verb', 'a': 'adjective', 'r': 'adverb', \n",
    "            's': 'adjective satellite'\n",
    "        }\n",
    "        self.project_dir = project_dir  # Store project root directory\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        logger.info(f\"Project directory: {self.project_dir}\")\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        self.synset_cache = {}\n",
    "        self.embeddings_cache = {}\n",
    "    \n",
    "    def word_to_synset_ids(self, word, pos=None):\n",
    "        \"\"\"Map word to possible WordNet synset IDs (optimized version)\"\"\"\n",
    "        cache_key = f\"{word}:{pos}\"\n",
    "        if cache_key in self.synset_cache:\n",
    "            return self.synset_cache[cache_key]\n",
    "        \n",
    "        # Convert POS tags to WordNet format\n",
    "        wn_pos = None\n",
    "        if pos:\n",
    "            if pos.startswith('n'):\n",
    "                wn_pos = wn.NOUN\n",
    "            elif pos.startswith('v'):\n",
    "                wn_pos = wn.VERB\n",
    "            elif pos.startswith('a') or pos.startswith('s'):\n",
    "                wn_pos = wn.ADJ\n",
    "            elif pos.startswith('r'):\n",
    "                wn_pos = wn.ADV\n",
    "        \n",
    "        # Get synsets\n",
    "        synsets = wn.synsets(word, pos=wn_pos)\n",
    "        \n",
    "        # Extract 8-digit IDs\n",
    "        synset_ids = []\n",
    "        for synset in synsets:\n",
    "            # Generate 8-digit ID from offset\n",
    "            offset = str(synset.offset()).zfill(8)\n",
    "            synset_ids.append(offset)\n",
    "        \n",
    "        # Cache results\n",
    "        self.synset_cache[cache_key] = synset_ids\n",
    "        return synset_ids\n",
    "    \n",
    "    def load_multirel_model(self, model_path, data_dir):\n",
    "        \"\"\"Load model trained from multirelational-poincare repository (hyperbolic or Euclidean)\"\"\"\n",
    "        try:\n",
    "            # Infer model type and dimension from filename\n",
    "            filename = os.path.basename(model_path)\n",
    "            if \"poincare\" in filename:\n",
    "                model_type = \"MuRP\"\n",
    "            elif \"euclidean\" in filename:\n",
    "                model_type = \"MuRE\"\n",
    "            else:\n",
    "                model_type = \"Unknown\"\n",
    "                \n",
    "            # Extract dimension information\n",
    "            match = re.search(r'_(\\d+)\\.pth$', filename)\n",
    "            dimension = int(match.group(1)) if match else 0\n",
    "            \n",
    "            logger.info(f\"Loading {model_type} model (dim={dimension}) from: {model_path}\")\n",
    "            \n",
    "            # Add project source code directory to system path\n",
    "            source_dir = os.path.join(self.project_dir, \"多关系双曲嵌入/multirelational-poincare\")\n",
    "            if source_dir not in sys.path:\n",
    "                sys.path.insert(0, source_dir)  # Add to beginning of path\n",
    "            \n",
    "            logger.info(f\"Added to sys.path: {source_dir}\")\n",
    "            logger.info(f\"Current sys.path: {sys.path}\")\n",
    "            \n",
    "            try:\n",
    "                from load_data import Data\n",
    "                if model_type == \"MuRP\":\n",
    "                    from model import MuRP\n",
    "                else:\n",
    "                    from model import MuRE\n",
    "                logger.info(\"Successfully imported model modules\")\n",
    "            except ImportError as e:\n",
    "                logger.error(f\"Error importing model modules: {e}\")\n",
    "                logger.error(f\"Current sys.path: {sys.path}\")\n",
    "                return None\n",
    "            \n",
    "            # Load data to get entity mapping\n",
    "            try:\n",
    "                d = Data(data_dir)\n",
    "                entity_list = d.entities\n",
    "                entity_id_map = {entity: idx for idx, entity in enumerate(entity_list)}\n",
    "                logger.info(f\"Loaded {len(entity_list)} entities from dataset\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading data: {e}\")\n",
    "                return None\n",
    "            \n",
    "            # Load model checkpoint\n",
    "            try:\n",
    "                if torch.cuda.is_available():\n",
    "                    checkpoint = torch.load(model_path)\n",
    "                else:\n",
    "                    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "                \n",
    "                # Create model instance\n",
    "                if model_type == \"MuRP\":\n",
    "                    # MuRP requires three parameters\n",
    "                    model = MuRP(d, dimension, self.device)\n",
    "                else:\n",
    "                    # MuRE only requires two parameters\n",
    "                    model = MuRE(d, dimension)\n",
    "                \n",
    "                # Load model state\n",
    "                model.load_state_dict(checkpoint)\n",
    "                model.eval()\n",
    "                model.to(self.device)\n",
    "                \n",
    "                # Extract entity embeddings\n",
    "                with torch.no_grad():\n",
    "                    # Get embeddings based on model type\n",
    "                    if model_type == \"MuRP\":\n",
    "                        entity_embeddings = model.Eh.weight.data.cpu().numpy()\n",
    "                    else:\n",
    "                        entity_embeddings = model.E.weight.data.cpu().numpy()\n",
    "                \n",
    "                logger.info(f\"Extracted entity embeddings of shape: {entity_embeddings.shape}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading model state: {e}\")\n",
    "                traceback.print_exc()\n",
    "                return None\n",
    "            \n",
    "            # Create embedding cache\n",
    "            self.embeddings_cache = {\n",
    "                entity: entity_embeddings[idx] \n",
    "                for entity, idx in entity_id_map.items()\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                \"entity_embeddings\": entity_embeddings,\n",
    "                \"entity_id_map\": entity_id_map,\n",
    "                \"entity_list\": entity_list,\n",
    "                \"type\": model_type,\n",
    "                \"dimension\": dimension,\n",
    "                \"embeddings_cache\": self.embeddings_cache\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def compute_poincare_distance(self, u, v, epsilon=1e-5):\n",
    "        \"\"\"Compute Poincaré hyperbolic distance (optimized version)\"\"\"\n",
    "        try:\n",
    "            # Convert to numpy arrays\n",
    "            u = np.asarray(u, dtype=np.float32)\n",
    "            v = np.asarray(v, dtype=np.float32)\n",
    "            \n",
    "            # Calculate Euclidean norms\n",
    "            norm_u_sq = np.sum(u**2)\n",
    "            norm_v_sq = np.sum(v**2)\n",
    "            norm_diff_sq = np.sum((u - v)**2)\n",
    "            \n",
    "            # Avoid numerical instability\n",
    "            denominator = max((1 - norm_u_sq) * (1 - norm_v_sq), epsilon)\n",
    "            \n",
    "            # Calculate hyperbolic distance\n",
    "            inner_expr = 1 + 2 * norm_diff_sq / denominator\n",
    "            \n",
    "            # Ensure inner expression is greater than 1\n",
    "            if inner_expr <= 1:\n",
    "                inner_expr = 1 + epsilon\n",
    "                \n",
    "            return np.arccosh(inner_expr)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error computing Poincaré distance: {e}\")\n",
    "            return np.linalg.norm(u - v)\n",
    "    \n",
    "    def compute_euclidean_similarity(self, u, v):\n",
    "        \"\"\"Compute cosine similarity for Euclidean embeddings\"\"\"\n",
    "        u = np.asarray(u, dtype=np.float32)\n",
    "        v = np.asarray(v, dtype=np.float32)\n",
    "        norm_u = np.linalg.norm(u)\n",
    "        norm_v = np.linalg.norm(v)\n",
    "        \n",
    "        if norm_u == 0 or norm_v == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        cosine_sim = np.dot(u, v) / (norm_u * norm_v)\n",
    "        return (cosine_sim + 1) / 2  # Normalize to [0,1] range\n",
    "    \n",
    "    def compute_similarity_batch(self, model_info, words1, words2, pos_list=None):\n",
    "        \"\"\"Batch compute similarities (supports both hyperbolic and Euclidean models)\"\"\"\n",
    "        entity_id_map = model_info[\"entity_id_map\"]\n",
    "        entity_list = model_info[\"entity_list\"]\n",
    "        embeddings = model_info[\"entity_embeddings\"]\n",
    "        embeddings_cache = model_info.get(\"embeddings_cache\", {})\n",
    "        model_type = model_info[\"type\"]\n",
    "        \n",
    "        # Batch get synset IDs\n",
    "        all_synsets1 = []\n",
    "        all_synsets2 = []\n",
    "        for i, (word1, word2) in enumerate(zip(words1, words2)):\n",
    "            pos = pos_list[i] if pos_list and i < len(pos_list) else None\n",
    "            all_synsets1.append(self.word_to_synset_ids(word1, pos))\n",
    "            all_synsets2.append(self.word_to_synset_ids(word2, pos))\n",
    "        \n",
    "        # Batch lookup embeddings\n",
    "        similarities = np.full(len(words1), np.nan)\n",
    "        valid_indices = []\n",
    "        \n",
    "        for i, (synsets1, synsets2) in enumerate(zip(all_synsets1, all_synsets2)):\n",
    "            if not synsets1 or not synsets2:\n",
    "                continue\n",
    "                \n",
    "            # Find valid synset IDs\n",
    "            valid_synsets1 = [sid for sid in synsets1 if sid in entity_list]\n",
    "            valid_synsets2 = [sid for sid in synsets2 if sid in entity_list]\n",
    "            \n",
    "            if not valid_synsets1 or not valid_synsets2:\n",
    "                continue\n",
    "                \n",
    "            # Try to get embeddings from cache\n",
    "            emb1 = None\n",
    "            for sid in valid_synsets1:\n",
    "                if sid in embeddings_cache:\n",
    "                    emb1 = embeddings_cache[sid]\n",
    "                    break\n",
    "                elif sid in entity_id_map:\n",
    "                    idx = entity_id_map[sid]\n",
    "                    emb = embeddings[idx]\n",
    "                    embeddings_cache[sid] = emb\n",
    "                    emb1 = emb\n",
    "                    break\n",
    "            \n",
    "            emb2 = None\n",
    "            for sid in valid_synsets2:\n",
    "                if sid in embeddings_cache:\n",
    "                    emb2 = embeddings_cache[sid]\n",
    "                    break\n",
    "                elif sid in entity_id_map:\n",
    "                    idx = entity_id_map[sid]\n",
    "                    emb = embeddings[idx]\n",
    "                    embeddings_cache[sid] = emb\n",
    "                    emb2 = emb\n",
    "                    break\n",
    "            \n",
    "            if emb1 is not None and emb2 is not None:\n",
    "                # Calculate similarity based on model type\n",
    "                if model_type == \"MuRP\":\n",
    "                    # Hyperbolic model: use inverse of Poincaré distance as similarity\n",
    "                    distance = self.compute_poincare_distance(emb1, emb2)\n",
    "                    similarity = 1 / (1 + distance)\n",
    "                else:\n",
    "                    # Euclidean model: use cosine similarity\n",
    "                    similarity = self.compute_euclidean_similarity(emb1, emb2)\n",
    "                \n",
    "                similarities[i] = similarity\n",
    "                valid_indices.append(i)\n",
    "        \n",
    "        return similarities, valid_indices\n",
    "    \n",
    "    def load_models(self, model_dir, data_dir):\n",
    "        \"\"\"Load all models from specified directory\"\"\"\n",
    "        logger.info(f\"Loading models from: {model_dir}\")\n",
    "        \n",
    "        # Find all model files\n",
    "        model_files = glob.glob(os.path.join(model_dir, \"*.pth\"))\n",
    "        logger.info(f\"Found {len(model_files)} model files\")\n",
    "        \n",
    "        if not model_files:\n",
    "            logger.error(\"No model files found!\")\n",
    "            return\n",
    "        \n",
    "        # Load each model\n",
    "        for model_path in model_files:\n",
    "            model_info = self.load_multirel_model(model_path, data_dir)\n",
    "            if model_info:\n",
    "                # Generate unique model name (type-dimension)\n",
    "                model_name = f\"{model_info['type']}-{model_info['dimension']}D\"\n",
    "                self.models[model_name] = model_info\n",
    "                logger.info(f\"Loaded model: {model_name}\")\n",
    "            else:\n",
    "                logger.error(f\"Failed to load model: {model_path}\")\n",
    "    \n",
    "    def evaluate_on_dataset(self, dataset_name, dataset_loader_func):\n",
    "        \"\"\"Evaluate models on specific dataset (optimized version)\"\"\"\n",
    "        logger.info(f\"\\n{'='*50}\\nEvaluating on {dataset_name}\\n{'='*50}\")\n",
    "        \n",
    "        # Load dataset\n",
    "        try:\n",
    "            data = dataset_loader_func()\n",
    "            word_pairs = data.X\n",
    "            human_scores = data.y\n",
    "            \n",
    "            # Extract POS information (if available)\n",
    "            pos_info = []\n",
    "            if hasattr(data, 'pos') and data.pos is not None:\n",
    "                pos_info = data.pos\n",
    "                logger.info(f\"Loaded POS tags for {len(pos_info)} pairs\")\n",
    "            \n",
    "            logger.info(f\"Loaded {len(word_pairs)} word pairs\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading dataset {dataset_name}: {e}\")\n",
    "            return\n",
    "        \n",
    "        dataset_results = {}\n",
    "        \n",
    "        for model_name, model_info in self.models.items():\n",
    "            logger.info(f\"Evaluating {model_name} on {dataset_name}...\")\n",
    "            \n",
    "            words1 = [str(pair[0]).strip() for pair in word_pairs]\n",
    "            words2 = [str(pair[1]).strip() for pair in word_pairs]\n",
    "            \n",
    "            # Batch compute similarities\n",
    "            model_scores, valid_indices = self.compute_similarity_batch(\n",
    "                model_info, words1, words2, pos_info\n",
    "            )\n",
    "            \n",
    "            valid_model_scores = model_scores[valid_indices]\n",
    "            valid_human_scores = human_scores[valid_indices]\n",
    "            \n",
    "            coverage = len(valid_indices) / len(word_pairs)\n",
    "            \n",
    "            # Calculate coverage\n",
    "            missing_count = len(word_pairs) - len(valid_indices)\n",
    "            if missing_count > 0:\n",
    "                logger.info(f\"  Missing {missing_count} word pairs ({coverage:.2%} coverage)\")\n",
    "            \n",
    "            # Calculate correlation metrics\n",
    "            if len(valid_model_scores) > 5:  # Need at least 5 valid data pairs\n",
    "                try:\n",
    "                    pearson_corr, _ = pearsonr(valid_model_scores, valid_human_scores)\n",
    "                    spearman_corr, _ = spearmanr(valid_model_scores, valid_human_scores)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error calculating correlations: {e}\")\n",
    "                    pearson_corr = spearman_corr = np.nan\n",
    "                \n",
    "                dataset_results[model_name] = {\n",
    "                    \"pearson\": pearson_corr,\n",
    "                    \"spearman\": spearman_corr,\n",
    "                    \"coverage\": coverage,\n",
    "                    \"n_pairs\": len(valid_model_scores),\n",
    "                    \"model_type\": model_info[\"type\"],\n",
    "                    \"dimension\": model_info[\"dimension\"]\n",
    "                }\n",
    "                logger.info(f\"  Pearson: {pearson_corr:.4f}, Spearman: {spearman_corr:.4f}, Coverage: {coverage:.2%}\")\n",
    "            else:\n",
    "                logger.warning(f\"  Insufficient valid predictions: {len(valid_model_scores)}/{len(word_pairs)}\")\n",
    "                dataset_results[model_name] = {\n",
    "                    \"pearson\": np.nan, \"spearman\": np.nan,\n",
    "                    \"coverage\": coverage, \"n_pairs\": len(valid_model_scores),\n",
    "                    \"model_type\": model_info[\"type\"],\n",
    "                    \"dimension\": model_info[\"dimension\"]\n",
    "                }\n",
    "        \n",
    "        self.results[dataset_name] = dataset_results\n",
    "    \n",
    "    def run_evaluation(self, datasets):\n",
    "        \"\"\"Run evaluation on specified datasets (optimized version)\"\"\"\n",
    "        logger.info(\"Starting embeddings evaluation...\")\n",
    "        \n",
    "        if len(self.models) == 0:\n",
    "            logger.error(\"No models loaded. Aborting evaluation.\")\n",
    "            return\n",
    "        \n",
    "        # Evaluate each dataset\n",
    "        for dataset_name, loader_func in datasets.items():\n",
    "            try:\n",
    "                self.evaluate_on_dataset(dataset_name, loader_func)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error evaluating {dataset_name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    def print_results_summary(self):\n",
    "        \"\"\"Print results summary (optimized version)\"\"\"\n",
    "        if not self.results:\n",
    "            logger.info(\"No results to display!\")\n",
    "            return None\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EMBEDDINGS EVALUATION RESULTS (MuRP vs MuRE)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Organize results into DataFrame\n",
    "        all_results = []\n",
    "        for dataset_name, dataset_results in self.results.items():\n",
    "            for model_name, metrics in dataset_results.items():\n",
    "                all_results.append({\n",
    "                    \"Dataset\": dataset_name,\n",
    "                    \"Model\": model_name,\n",
    "                    \"Type\": metrics.get(\"model_type\", \"Unknown\"),\n",
    "                    \"Dimension\": metrics.get(\"dimension\", 0),\n",
    "                    \"Pearson\": metrics.get(\"pearson\", np.nan),\n",
    "                    \"Spearman\": metrics.get(\"spearman\", np.nan),\n",
    "                    \"Coverage\": metrics.get(\"coverage\", 0),\n",
    "                    \"N_Pairs\": metrics.get(\"n_pairs\", 0)\n",
    "                })\n",
    "        \n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        # Display by dataset and model type\n",
    "        for dataset in results_df['Dataset'].unique():\n",
    "            dataset_df = results_df[results_df['Dataset'] == dataset]\n",
    "            print(f\"\\n{dataset}:\")\n",
    "            print(\"-\" * 90)\n",
    "            print(f\"{'Model':<15} {'Type':<8} {'Dim':>4} {'Pearson':>8} {'Spearman':>8} {'Coverage':>10} {'N_Pairs':>8}\")\n",
    "            print(\"-\" * 90)\n",
    "            \n",
    "            # Sort by dimension\n",
    "            dataset_df = dataset_df.sort_values(by=['Type', 'Dimension'])\n",
    "            \n",
    "            for _, row in dataset_df.iterrows():\n",
    "                pearson_str = f\"{row['Pearson']:.4f}\" if not pd.isna(row['Pearson']) else \"N/A\"\n",
    "                spearman_str = f\"{row['Spearman']:.4f}\" if not pd.isna(row['Spearman']) else \"N/A\"\n",
    "                coverage_str = f\"{row['Coverage']:.2%}\"\n",
    "                print(f\"{row['Model']:<15} {row['Type']:<8} {row['Dimension']:>4} {pearson_str:>8} {spearman_str:>8} {coverage_str:>10} {row['N_Pairs']:>8}\")\n",
    "                \n",
    "        return results_df\n",
    "    \n",
    "    def save_results(self, filepath=\"dimensionality_results.csv\"):\n",
    "        \"\"\"Save results to CSV (optimized version)\"\"\"\n",
    "        if not self.results:\n",
    "            logger.info(\"No results to save!\")\n",
    "            return None\n",
    "        \n",
    "        all_results = []\n",
    "        for dataset_name, dataset_results in self.results.items():\n",
    "            for model_name, metrics in dataset_results.items():\n",
    "                all_results.append({\n",
    "                    \"Dataset\": dataset_name,\n",
    "                    \"Model\": model_name,\n",
    "                    \"Model_Type\": metrics.get(\"model_type\", \"Unknown\"),\n",
    "                    \"Dimension\": metrics.get(\"dimension\", 0),\n",
    "                    \"Pearson_Correlation\": metrics.get(\"pearson\", np.nan),\n",
    "                    \"Spearman_Correlation\": metrics.get(\"spearman\", np.nan),\n",
    "                    \"Coverage\": metrics.get(\"coverage\", 0),\n",
    "                    \"Valid_Pairs\": metrics.get(\"n_pairs\", 0)\n",
    "                })\n",
    "        \n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        # Create save directory (if it doesn't exist)\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        results_df.to_csv(filepath, index=False)\n",
    "        logger.info(f\"Detailed results saved to: {filepath}\")\n",
    "        \n",
    "        # Save dimensionality analysis summary\n",
    "        dim_summary = results_df.groupby(['Model_Type', 'Dimension']).agg({\n",
    "            'Pearson_Correlation': 'mean',\n",
    "            'Spearman_Correlation': 'mean',\n",
    "            'Coverage': 'mean'\n",
    "        }).reset_index()\n",
    "        dim_summary_file = \"dimensionality_summary.csv\"\n",
    "        dim_summary.to_csv(dim_summary_file, index=False)\n",
    "        logger.info(f\"Dimensionality summary saved to: {dim_summary_file}\")\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function: run evaluation process (optimized version)\"\"\"\n",
    "    # Check dependencies\n",
    "    missing_deps = []\n",
    "    required = [\"torch\", \"scipy\", \"sklearn\", \"pandas\", \"numpy\", \"nltk\"]\n",
    "    for dep in required:\n",
    "        try:\n",
    "            __import__(dep)\n",
    "        except ImportError:\n",
    "            missing_deps.append(dep)\n",
    "    \n",
    "    if missing_deps:\n",
    "        print(f\"Missing dependencies: {missing_deps}\")\n",
    "        print(\"Install with: pip install \" + \" \".join(missing_deps))\n",
    "        return None, None\n",
    "    \n",
    "    # Set project root directory\n",
    "    project_dir = \"/Users/chouyinghan/my_mathlib_project/Demo_in_Matrix\"\n",
    "    \n",
    "    # Initialize evaluator, pass project root directory\n",
    "    evaluator = HyperbolicEvaluator(project_dir)\n",
    "    \n",
    "    # Automatically construct paths\n",
    "    model_dir = os.path.join(project_dir, \"your_model_directory\")\n",
    "    data_dir = os.path.join(project_dir, \"your_dataset_directory\")\n",
    "\n",
    "    # Check if model directory exists\n",
    "    if not os.path.exists(model_dir):\n",
    "        print(f\"Model directory not found: {model_dir}\")\n",
    "        model_dir = input(\"Please enter the full path to the model directory: \")\n",
    "        if not os.path.exists(model_dir):\n",
    "            print(\"Invalid path. Exiting.\")\n",
    "            return\n",
    "    \n",
    "    # Check if data directory exists\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Data directory not found: {data_dir}\")\n",
    "        # Try other possible paths\n",
    "        possible_data_dir = os.path.join(project_dir, \"multirelational-poincare/data/WN18RR/\")\n",
    "        if os.path.exists(possible_data_dir):\n",
    "            data_dir = possible_data_dir\n",
    "            print(f\"Using alternative data directory: {data_dir}\")\n",
    "        else:\n",
    "            data_dir = input(\"Please enter the full path to the data directory: \")\n",
    "            if not os.path.exists(data_dir):\n",
    "                print(\"Invalid path. Exiting.\")\n",
    "                return\n",
    "    \n",
    "    # Batch load models\n",
    "    evaluator.load_models(model_dir, data_dir)\n",
    "    \n",
    "    # Check if any models loaded successfully\n",
    "    if not evaluator.models:\n",
    "        logger.error(\"No models loaded successfully. Aborting evaluation.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Define datasets to evaluate\n",
    "    datasets = {\n",
    "        # \"SimVerb3500\": lambda: fetch_SimVerb3500('all'),\n",
    "        # \"SimVerb3500-dev\": lambda: fetch_SimVerb3500('dev'),\n",
    "        \"SimVerb3500-test\": lambda: fetch_SimVerb3500('test'),\n",
    "        # \"MEN\": lambda: fetch_MEN(\"all\"),\n",
    "        # \"MEN-dev\": lambda: fetch_MEN(\"dev\"),\n",
    "        # \"MEN-test\": lambda: fetch_MEN(\"test\"),\n",
    "        # \"SimLex999\": lambda: fetch_SimLex999(),\n",
    "        # \"SCWS\": lambda: fetch_SCWS(),\n",
    "        # \"WS353\": lambda: fetch_WS353(\"all\"),\n",
    "        # \"RG65\": fetch_RG65()\n",
    "    }\n",
    "    \n",
    "    # Run evaluation\n",
    "    evaluator.run_evaluation(datasets)\n",
    "    results_df = evaluator.print_results_summary()\n",
    "    \n",
    "    if results_df is not None:\n",
    "        # Save results to same directory\n",
    "        results_file = os.path.join(model_dir, \"evaluation_results.csv\")\n",
    "        evaluator.save_results(results_file)\n",
    "        \n",
    "        # Save dimensionality analysis summary\n",
    "        summary_file = os.path.join(model_dir, \"dimensionality_summary.csv\")\n",
    "        results_df.to_csv(summary_file, index=False)\n",
    "        logger.info(f\"Dimension summary saved to: {summary_file}\")\n",
    "    else:\n",
    "        logger.warning(\"No results to save\")\n",
    "    \n",
    "    return evaluator, results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluator, results = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f504b720",
   "metadata": {},
   "source": [
    "## Improved version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487005be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Hyperbolic Embeddings Evaluation Framework (Optimized Version)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.utils import Bunch\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure WordNet data is downloaded\n",
    "try:\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Error downloading NLTK resources: {e}\")\n",
    "\n",
    "# Import data loading functions\n",
    "try:\n",
    "    from load_datasets import fetch_SimVerb3500, fetch_MEN, fetch_SimLex999, fetch_SCWS, fetch_WS353, fetch_RG65\n",
    "except ImportError:\n",
    "    logger.warning(\"load_datasets module not found. Using fallback implementations.\")\n",
    "    # Provide simple fallback implementations\n",
    "    def fetch_SimVerb3500():\n",
    "        return Bunch(X=np.array([], dtype=object), y=np.array([]))\n",
    "    def fetch_MEN():\n",
    "        return Bunch(X=np.array([], dtype=object), y=np.array([]))\n",
    "    def fetch_SimLex999():\n",
    "        return Bunch(X=np.array([], dtype=object), y=np.array([]))\n",
    "    def fetch_SCWS():\n",
    "        return Bunch(X=np.array([], dtype=object), y=np.array([]))\n",
    "    def fetch_WS353():\n",
    "        return Bunch(X=np.array([], dtype=object), y=np.array([]))\n",
    "    def fetch_RG65():\n",
    "        return Bunch(X=np.array([], dtype=object), y=np.array([]))\n",
    "\n",
    "class HyperbolicEvaluator:\n",
    "    \"\"\"Hyperbolic Embeddings Evaluator (Optimized Version)\"\"\"\n",
    "    \n",
    "    def __init__(self, project_dir, device=None):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.wordnet_pos_map = {\n",
    "            'n': 'noun', 'v': 'verb', 'a': 'adjective', 'r': 'adverb', \n",
    "            's': 'adjective satellite'\n",
    "        }\n",
    "        self.project_dir = project_dir  # Store project root directory\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        logger.info(f\"Project directory: {self.project_dir}\")\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        self.synset_cache = {}\n",
    "        self.embeddings_cache = {}\n",
    "    \n",
    "    def word_to_synset_ids(self, word, pos=None):\n",
    "        \"\"\"Map word to possible WordNet synset IDs (optimized version)\"\"\"\n",
    "        cache_key = f\"{word}:{pos}\"\n",
    "        if cache_key in self.synset_cache:\n",
    "            return self.synset_cache[cache_key]\n",
    "        \n",
    "        # Convert POS tags to WordNet format\n",
    "        wn_pos = None\n",
    "        if pos and isinstance(pos, str):  # Ensure pos is string\n",
    "            if pos.startswith('n'):\n",
    "                wn_pos = wn.NOUN\n",
    "            elif pos.startswith('v'):\n",
    "                wn_pos = wn.VERB\n",
    "            elif pos.startswith('a') or pos.startswith('s'):\n",
    "                wn_pos = wn.ADJ\n",
    "            elif pos.startswith('r'):\n",
    "                wn_pos = wn.ADV\n",
    "        \n",
    "        # Get synsets\n",
    "        synsets = wn.synsets(word, pos=wn_pos)\n",
    "        \n",
    "        # Extract 8-digit IDs\n",
    "        synset_ids = []\n",
    "        for synset in synsets:\n",
    "            # Generate 8-digit ID from offset\n",
    "            offset = str(synset.offset()).zfill(8)\n",
    "            synset_ids.append(offset)\n",
    "        \n",
    "        # Cache results\n",
    "        self.synset_cache[cache_key] = synset_ids\n",
    "        return synset_ids\n",
    "    \n",
    "    def load_multirel_model(self, model_path, data_dir):\n",
    "        \"\"\"Load model trained from multirelational-poincare repository (hyperbolic or Euclidean)\"\"\"\n",
    "        try:\n",
    "            # Infer model type and dimension from filename\n",
    "            filename = os.pathbasename(model_path)\n",
    "            if \"poincare\" in filename:\n",
    "                model_type = \"MuRP\"\n",
    "            elif \"euclidean\" in filename:\n",
    "                model_type = \"MuRE\"\n",
    "            else:\n",
    "                model_type = \"Unknown\"\n",
    "                \n",
    "            # Extract dimension information\n",
    "            match = re.search(r'_(\\d+)\\.pth$', filename)\n",
    "            dimension = int(match.group(1)) if match else 0\n",
    "            \n",
    "            logger.info(f\"Loading {model_type} model (dim={dimension}) from: {model_path}\")\n",
    "            \n",
    "            # Add project source code directory to system path\n",
    "            source_dir = os.path.join(self.project_dir, \"多关系双曲嵌入/multirelational-poincare\")\n",
    "            if source_dir not in sys.path:\n",
    "                sys.path.insert(0, source_dir)  # Add to beginning of path\n",
    "            \n",
    "            logger.info(f\"Added to sys.path: {source_dir}\")\n",
    "            \n",
    "            try:\n",
    "                from load_data import Data\n",
    "                if model_type == \"MuRP\":\n",
    "                    from model import MuRP\n",
    "                else:\n",
    "                    from model import MuRE\n",
    "                logger.info(\"Successfully imported model modules\")\n",
    "            except ImportError as e:\n",
    "                logger.error(f\"Error importing model modules: {e}\")\n",
    "                logger.error(f\"Current sys.path: {sys.path}\")\n",
    "                return None\n",
    "            \n",
    "            # Load data to get entity mapping\n",
    "            try:\n",
    "                d = Data(data_dir)\n",
    "                entity_list = d.entities\n",
    "                entity_id_map = {entity: idx for idx, entity in enumerate(entity_list)}\n",
    "                logger.info(f\"Loaded {len(entity_list)} entities from dataset\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading data: {e}\")\n",
    "                return None\n",
    "            \n",
    "            # Load model checkpoint\n",
    "            try:\n",
    "                if torch.cuda.is_available():\n",
    "                    checkpoint = torch.load(model_path)\n",
    "                else:\n",
    "                    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "                \n",
    "                # Create model instance\n",
    "                if model_type == \"MuRP\":\n",
    "                    # MuRP requires three parameters\n",
    "                    model = MuRP(d, dimension, self.device)\n",
    "                else:\n",
    "                    # MuRE only requires two parameters\n",
    "                    model = MuRE(d, dimension)\n",
    "                \n",
    "                # Load model state\n",
    "                model.load_state_dict(checkpoint)\n",
    "                model.eval()\n",
    "                model.to(self.device)\n",
    "                \n",
    "                # Extract entity embeddings\n",
    "                with torch.no_grad():\n",
    "                    # Get embeddings based on model type\n",
    "                    if model_type == \"MuRP\":\n",
    "                        entity_embeddings = model.Eh.weight.data.cpu().numpy()\n",
    "                    else:\n",
    "                        entity_embeddings = model.E.weight.data.cpu().numpy()\n",
    "                \n",
    "                logger.info(f\"Extracted entity embeddings of shape: {entity_embeddings.shape}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading model state: {e}\")\n",
    "                traceback.print_exc()\n",
    "                return None\n",
    "            \n",
    "            # Create embedding cache\n",
    "            self.embeddings_cache = {\n",
    "                entity: entity_embeddings[idx] \n",
    "                for entity, idx in entity_id_map.items()\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                \"entity_embeddings\": entity_embeddings,\n",
    "                \"entity_id_map\": entity_id_map,\n",
    "                \"entity_list\": entity_list,\n",
    "                \"type\": model_type,\n",
    "                \"dimension\": dimension,\n",
    "                \"embeddings_cache\": self.embeddings_cache\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def compute_poincare_distance(self, u, v, epsilon=1e-5):\n",
    "        \"\"\"Compute Poincaré hyperbolic distance (optimized version)\"\"\"\n",
    "        try:\n",
    "            # Convert to numpy arrays\n",
    "            u = np.asarray(u, dtype=np.float32)\n",
    "            v = np.asarray(v, dtype=np.float32)\n",
    "            \n",
    "            # Calculate Euclidean norms\n",
    "            norm_u_sq = np.sum(u**2)\n",
    "            norm_v_sq = np.sum(v**2)\n",
    "            norm_diff_sq = np.sum((u - v)**2)\n",
    "            \n",
    "            # Avoid numerical instability\n",
    "            denominator = max((1 - norm_u_sq) * (1 - norm_v_sq), epsilon)\n",
    "            \n",
    "            # Calculate hyperbolic distance\n",
    "            inner_expr = 1 + 2 * norm_diff_sq / denominator\n",
    "            \n",
    "            # Ensure inner expression is greater than 1\n",
    "            if inner_expr <= 1:\n",
    "                inner_expr = 1 + epsilon\n",
    "                \n",
    "            return np.arccosh(inner_expr)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error computing Poincaré distance: {e}\")\n",
    "            return np.linalg.norm(u - v)\n",
    "    \n",
    "    def compute_euclidean_similarity(self, u, v):\n",
    "        \"\"\"Compute cosine similarity for Euclidean embeddings\"\"\"\n",
    "        u = np.asarray(u, dtype=np.float32)\n",
    "        v = np.asarray(v, dtype=np.float32)\n",
    "        norm_u = np.linalg.norm(u)\n",
    "        norm_v = np.linalg.norm(v)\n",
    "        \n",
    "        if norm_u == 0 or norm_v == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        cosine_sim = np.dot(u, v) / (norm_u * norm_v)\n",
    "        return (cosine_sim + 1) / 2  # Normalize to [0,1] range\n",
    "    \n",
    "    def compute_similarity_batch(self, model_info, words1, words2, pos_list=None):\n",
    "        \"\"\"Batch compute similarities (supports both hyperbolic and Euclidean models)\"\"\"\n",
    "        entity_id_map = model_info[\"entity_id_map\"]\n",
    "        entity_list = model_info[\"entity_list\"]\n",
    "        embeddings = model_info[\"entity_embeddings\"]\n",
    "        embeddings_cache = model_info.get(\"embeddings_cache\", {})\n",
    "        model_type = model_info[\"type\"]\n",
    "        \n",
    "        # Batch get synset IDs\n",
    "        all_synsets1 = []\n",
    "        all_synsets2 = []\n",
    "        for i, (word1, word2) in enumerate(zip(words1, words2)):\n",
    "            # Safely handle POS tags\n",
    "            pos = None\n",
    "            if pos_list is not None and i < len(pos_list):\n",
    "                pos_item = pos_list[i]\n",
    "                # Handle different types of POS tags\n",
    "                if isinstance(pos_item, (list, tuple, np.ndarray)):\n",
    "                    # Take first element as main part of speech\n",
    "                    pos = str(pos_item[0]) if len(pos_item) > 0 else None\n",
    "                elif isinstance(pos_item, str):\n",
    "                    pos = pos_item\n",
    "                else:\n",
    "                    pos = None\n",
    "            \n",
    "            all_synsets1.append(self.word_to_synset_ids(word1, pos))\n",
    "            all_synsets2.append(self.word_to_synset_ids(word2, pos))\n",
    "        \n",
    "        # Batch lookup embeddings\n",
    "        similarities = np.full(len(words1), np.nan)\n",
    "        valid_indices = []\n",
    "        \n",
    "        for i, (synsets1, synsets2) in enumerate(zip(all_synsets1, all_synsets2)):\n",
    "            # Check if empty list\n",
    "            if not synsets1 or not synsets2:\n",
    "                continue\n",
    "                \n",
    "            # Find valid synset IDs\n",
    "            valid_synsets1 = [sid for sid in synsets1 if sid in entity_list]\n",
    "            valid_synsets2 = [sid for sid in synsets2 if sid in entity_list]\n",
    "            \n",
    "            if not valid_synsets1 or not valid_synsets2:\n",
    "                continue\n",
    "                \n",
    "            # Try to get embeddings from cache\n",
    "            emb1 = None\n",
    "            for sid in valid_synsets1:\n",
    "                if sid in embeddings_cache:\n",
    "                    emb1 = embeddings_cache[sid]\n",
    "                    break\n",
    "                elif sid in entity_id_map:\n",
    "                    idx = entity_id_map[sid]\n",
    "                    emb = embeddings[idx]\n",
    "                    embeddings_cache[sid] = emb\n",
    "                    emb1 = emb\n",
    "                    break\n",
    "            \n",
    "            emb2 = None\n",
    "            for sid in valid_synsets2:\n",
    "                if sid in embeddings_cache:\n",
    "                    emb2 = embeddings_cache[sid]\n",
    "                    break\n",
    "                elif sid in entity_id_map:\n",
    "                    idx = entity_id_map[sid]\n",
    "                    emb = embeddings[idx]\n",
    "                    embeddings_cache[sid] = emb\n",
    "                    emb2 = emb\n",
    "                    break\n",
    "            \n",
    "            if emb1 is not None and emb2 is not None:\n",
    "                # Calculate similarity based on model type\n",
    "                if model_type == \"MuRP\":\n",
    "                    # Hyperbolic model: use inverse of Poincaré distance as similarity\n",
    "                    distance = self.compute_poincare_distance(emb1, emb2)\n",
    "                    similarity = 1 / (1 + distance)\n",
    "                else:\n",
    "                    # Euclidean model: use cosine similarity\n",
    "                    similarity = self.compute_euclidean_similarity(emb1, emb2)\n",
    "                \n",
    "                similarities[i] = similarity\n",
    "                valid_indices.append(i)\n",
    "        \n",
    "        return similarities, valid_indices\n",
    "    \n",
    "    def load_models(self, model_dir, data_dir):\n",
    "        \"\"\"Load all models from specified directory\"\"\"\n",
    "        logger.info(f\"Loading models from: {model_dir}\")\n",
    "        \n",
    "        # Find all model files\n",
    "        model_files = glob.glob(os.path.join(model_dir, \"*.pth\"))\n",
    "        logger.info(f\"Found {len(model_files)} model files\")\n",
    "        \n",
    "        if not model_files:\n",
    "            logger.error(\"No model files found!\")\n",
    "            return\n",
    "        \n",
    "        # Load each model\n",
    "        for model_path in model_files:\n",
    "            model_info = self.load_multirel_model(model_path, data_dir)\n",
    "            if model_info:\n",
    "                # Generate unique model name (type-dimension)\n",
    "                model_name = f\"{model_info['type']}-{model_info['dimension']}D\"\n",
    "                self.models[model_name] = model_info\n",
    "                logger.info(f\"Loaded model: {model_name}\")\n",
    "            else:\n",
    "                logger.error(f\"Failed to load model: {model_path}\")\n",
    "    \n",
    "    def evaluate_on_dataset(self, dataset_name, dataset_loader_func):\n",
    "        \"\"\"Evaluate models on specific dataset (optimized version)\"\"\"\n",
    "        logger.info(f\"\\n{'='*50}\\nEvaluating on {dataset_name}\\n{'='*50}\")\n",
    "        \n",
    "        # Load dataset\n",
    "        try:\n",
    "            data = dataset_loader_func()\n",
    "            word_pairs = data.X\n",
    "            human_scores = data.y\n",
    "            \n",
    "            # Ensure human_scores is NumPy array\n",
    "            if not isinstance(human_scores, np.ndarray):\n",
    "                human_scores = np.array(human_scores)\n",
    "            \n",
    "            # Ensure human_scores is 1D array\n",
    "            if human_scores.ndim > 1:\n",
    "                human_scores = human_scores.flatten()\n",
    "            \n",
    "            # Extract POS information (if available)\n",
    "            pos_info = None\n",
    "            if hasattr(data, 'pos') and data.pos is not None:\n",
    "                # Ensure pos_info is list or array\n",
    "                if isinstance(data.pos, (list, tuple, np.ndarray)):\n",
    "                    pos_info = data.pos\n",
    "                    logger.info(f\"Loaded POS tags for {len(pos_info)} pairs\")\n",
    "                else:\n",
    "                    logger.warning(f\"Unexpected POS type: {type(data.pos)}. Ignoring POS info.\")\n",
    "            \n",
    "            logger.info(f\"Loaded {len(word_pairs)} word pairs\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading dataset {dataset_name}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return\n",
    "        \n",
    "        dataset_results = {}\n",
    "        \n",
    "        for model_name, model_info in self.models.items():\n",
    "            logger.info(f\"Evaluating {model_name} on {dataset_name}...\")\n",
    "            \n",
    "            words1 = [str(pair[0]).strip() for pair in word_pairs]\n",
    "            words2 = [str(pair[1]).strip() for pair in word_pairs]\n",
    "            \n",
    "            # Batch compute similarities\n",
    "            model_scores, valid_indices = self.compute_similarity_batch(\n",
    "                model_info, words1, words2, pos_info\n",
    "            )\n",
    "            \n",
    "            # Ensure there are valid indices\n",
    "            if not valid_indices:\n",
    "                logger.warning(f\"No valid predictions for model {model_name} on dataset {dataset_name}\")\n",
    "                dataset_results[model_name] = {\n",
    "                    \"pearson\": np.nan, \"spearman\": np.nan,\n",
    "                    \"coverage\": 0.0, \"n_pairs\": 0,\n",
    "                    \"model_type\": model_info[\"type\"],\n",
    "                    \"dimension\": model_info[\"dimension\"]\n",
    "                }\n",
    "                continue\n",
    "                \n",
    "            valid_model_scores = model_scores[valid_indices]\n",
    "            valid_human_scores = human_scores[valid_indices]\n",
    "            \n",
    "            # Ensure human_scores is numeric array\n",
    "            if not isinstance(valid_human_scores, np.ndarray) or valid_human_scores.dtype.kind not in 'iuf':\n",
    "                try:\n",
    "                    valid_human_scores = np.array(valid_human_scores, dtype=np.float32)\n",
    "                except:\n",
    "                    logger.error(f\"Invalid human scores type: {type(valid_human_scores)}\")\n",
    "                    continue\n",
    "            \n",
    "            coverage = len(valid_indices) / len(word_pairs)\n",
    "            \n",
    "            # Calculate coverage\n",
    "            missing_count = len(word_pairs) - len(valid_indices)\n",
    "            if missing_count > 0:\n",
    "                logger.info(f\"  Missing {missing_count} word pairs ({coverage:.2%} coverage)\")\n",
    "            \n",
    "            # Calculate correlation metrics\n",
    "            if len(valid_model_scores) > 5:  # Need at least 5 valid data pairs\n",
    "                try:\n",
    "                    # Ensure array shapes are consistent\n",
    "                    if valid_model_scores.ndim > 1:\n",
    "                        valid_model_scores = valid_model_scores.flatten()\n",
    "                    if valid_human_scores.ndim > 1:\n",
    "                        valid_human_scores = valid_human_scores.flatten()\n",
    "                    \n",
    "                    # Calculate correlation coefficients\n",
    "                    pearson_corr, _ = pearsonr(valid_model_scores, valid_human_scores)\n",
    "                    spearman_corr, _ = spearmanr(valid_model_scores, valid_human_scores)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error calculating correlations: {e}\")\n",
    "                    traceback.print_exc()\n",
    "                    pearson_corr = spearman_corr = np.nan\n",
    "                \n",
    "                dataset_results[model_name] = {\n",
    "                    \"pearson\": pearson_corr,\n",
    "                    \"spearman\": spearman_corr,\n",
    "                    \"coverage\": coverage,\n",
    "                    \"n_pairs\": len(valid_model_scores),\n",
    "                    \"model_type\": model_info[\"type\"],\n",
    "                    \"dimension\": model_info[\"dimension\"]\n",
    "                }\n",
    "                logger.info(f\"  Pearson: {pearson_corr:.4f}, Spearman: {spearman_corr:.4f}, Coverage: {coverage:.2%}\")\n",
    "            else:\n",
    "                logger.warning(f\"  Insufficient valid predictions: {len(valid_model_scores)}/{len(word_pairs)}\")\n",
    "                dataset_results[model_name] = {\n",
    "                    \"pearson\": np.nan, \"spearman\": np.nan,\n",
    "                    \"coverage\": coverage, \"n_pairs\": len(valid_model_scores),\n",
    "                    \"model_type\": model_info[\"type\"],\n",
    "                    \"dimension\": model_info[\"dimension\"]\n",
    "                }\n",
    "        \n",
    "        self.results[dataset_name] = dataset_results\n",
    "    \n",
    "    def run_evaluation(self, datasets):\n",
    "        \"\"\"Run evaluation on specified datasets (optimized version)\"\"\"\n",
    "        logger.info(\"Starting embeddings evaluation...\")\n",
    "        \n",
    "        if len(self.models) == 0:\n",
    "            logger.error(\"No models loaded. Aborting evaluation.\")\n",
    "            return\n",
    "        \n",
    "        # Evaluate each dataset\n",
    "        for dataset_name, loader_func in datasets.items():\n",
    "            try:\n",
    "                logger.info(f\"Processing dataset: {dataset_name}\")\n",
    "                self.evaluate_on_dataset(dataset_name, loader_func)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error evaluating {dataset_name}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "    \n",
    "    def print_results_summary(self):\n",
    "        \"\"\"Print results summary (optimized version)\"\"\"\n",
    "        if not self.results:\n",
    "            logger.info(\"No results to display!\")\n",
    "            return None\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EMBEDDINGS EVALUATION RESULTS (MuRP vs MuRE)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Organize results into DataFrame\n",
    "        all_results = []\n",
    "        for dataset_name, dataset_results in self.results.items():\n",
    "            for model_name, metrics in dataset_results.items():\n",
    "                all_results.append({\n",
    "                    \"Dataset\": dataset_name,\n",
    "                    \"Model\": model_name,\n",
    "                    \"Type\": metrics.get(\"model_type\", \"Unknown\"),\n",
    "                    \"Dimension\": metrics.get(\"dimension\", 0),\n",
    "                    \"Pearson\": metrics.get(\"pearson\", np.nan),\n",
    "                    \"Spearman\": metrics.get(\"spearman\", np.nan),\n",
    "                    \"Coverage\": metrics.get(\"coverage\", 0),\n",
    "                    \"N_Pairs\": metrics.get(\"n_pairs\", 0)\n",
    "                })\n",
    "        \n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        # Display by dataset and model type\n",
    "        for dataset in results_df['Dataset'].unique():\n",
    "            dataset_df = results_df[results_df['Dataset'] == dataset]\n",
    "            print(f\"\\n{dataset}:\")\n",
    "            print(\"-\" * 90)\n",
    "            print(f\"{'Model':<15} {'Type':<8} {'Dim':>4} {'Pearson':>8} {'Spearman':>8} {'Coverage':>10} {'N_Pairs':>8}\")\n",
    "            print(\"-\" * 90)\n",
    "            \n",
    "            # Sort by dimension\n",
    "            dataset_df = dataset_df.sort_values(by=['Type', 'Dimension'])\n",
    "            \n",
    "            for _, row in dataset_df.iterrows():\n",
    "                pearson_str = f\"{row['Pearson']:.4f}\" if not pd.isna(row['Pearson']) else \"N/A\"\n",
    "                spearman_str = f\"{row['Spearman']:.4f}\" if not pd.isna(row['Spearman']) else \"N/A\"\n",
    "                coverage_str = f\"{row['Coverage']:.2%}\"\n",
    "                print(f\"{row['Model']:<15} {row['Type']:<8} {row['Dimension']:>4} {pearson_str:>8} {spearman_str:>8} {coverage_str:>10} {row['N_Pairs']:>8}\")\n",
    "                \n",
    "        return results_df\n",
    "    \n",
    "    def save_results(self, filepath=\"dimensionality_results.csv\"):\n",
    "        \"\"\"Save results to CSV (optimized version)\"\"\"\n",
    "        if not self.results:\n",
    "            logger.info(\"No results to save!\")\n",
    "            return None\n",
    "        \n",
    "        all_results = []\n",
    "        for dataset_name, dataset_results in self.results.items():\n",
    "            for model_name, metrics in dataset_results.items():\n",
    "                all_results.append({\n",
    "                    \"Dataset\": dataset_name,\n",
    "                    \"Model\": model_name,\n",
    "                    \"Model_Type\": metrics.get(\"model_type\", \"Unknown\"),\n",
    "                    \"Dimension\": metrics.get(\"dimension\", 0),\n",
    "                    \"Pearson_Correlation\": metrics.get(\"pearson\", np.nan),\n",
    "                    \"Spearman_Correlation\": metrics.get(\"spearman\", np.nan),\n",
    "                    \"Coverage\": metrics.get(\"coverage\", 0),\n",
    "                    \"Valid_Pairs\": metrics.get(\"n_pairs\", 0)\n",
    "                })\n",
    "        \n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        # Create save directory (if it doesn't exist)\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        results_df.to_csv(filepath, index=False)\n",
    "        logger.info(f\"Detailed results saved to: {filepath}\")\n",
    "        \n",
    "        # Save dimensionality analysis summary\n",
    "        dim_summary = results_df.groupby(['Model_Type', 'Dimension']).agg({\n",
    "            'Pearson_Correlation': 'mean',\n",
    "            'Spearman_Correlation': 'mean',\n",
    "            'Coverage': 'mean'\n",
    "        }).reset_index()\n",
    "        dim_summary_file = \"dimensionality_summary.csv\"\n",
    "        dim_summary.to_csv(dim_summary_file, index=False)\n",
    "        logger.info(f\"Dimensionality summary saved to: {dim_summary_file}\")\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function: run evaluation process (optimized version)\"\"\"\n",
    "    # Check dependencies\n",
    "    missing_deps = []\n",
    "    required = [\"torch\", \"scipy\", \"sklearn\", \"pandas\", \"numpy\", \"nltk\"]\n",
    "    for dep in required:\n",
    "        try:\n",
    "            __import__(dep)\n",
    "        except ImportError:\n",
    "            missing_deps.append(dep)\n",
    "    \n",
    "    if missing_deps:\n",
    "        print(f\"Missing dependencies: {missing_deps}\")\n",
    "        print(\"Install with: pip install \" + \" \".join(missing_deps))\n",
    "        return None, None\n",
    "    \n",
    "    # Set project root directory\n",
    "    project_dir = \"/Users/chouyinghan/my_mathlib_project/Demo_in_Matrix\"\n",
    "    \n",
    "    # Initialize evaluator, pass project root directory\n",
    "    evaluator = HyperbolicEvaluator(project_dir)\n",
    "    \n",
    "    # Automatically construct paths\n",
    "    model_dir = os.path.join(project_dir, \"your_model_directory\")\n",
    "    data_dir = os.path.join(project_dir, \"your_dataset_directory\")\n",
    "\n",
    "    # Check if model directory exists\n",
    "    if not os.path.exists(model_dir):\n",
    "        print(f\"Model directory not found: {model_dir}\")\n",
    "        model_dir = input(\"Please enter the full path to the model directory: \")\n",
    "        if not os.path.exists(model_dir):\n",
    "            print(\"Invalid path. Exiting.\")\n",
    "            return\n",
    "    \n",
    "    # Check if data directory exists\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Data directory not found: {data_dir}\")\n",
    "        # Try other possible paths\n",
    "        possible_data_dir = os.path.join(project_dir, \"multirelational-poincare/data/WN18RR/\")\n",
    "        if os.path.exists(possible_data_dir):\n",
    "            data_dir = possible_data_dir\n",
    "            print(f\"Using alternative data directory: {data_dir}\")\n",
    "        else:\n",
    "            data_dir = input(\"Please enter the full path to the data directory: \")\n",
    "            if not os.path.exists(data_dir):\n",
    "                print(\"Invalid path. Exiting.\")\n",
    "                return\n",
    "    \n",
    "    # Batch load models\n",
    "    evaluator.load_models(model_dir, data_dir)\n",
    "    \n",
    "    # Check if any models loaded successfully\n",
    "    if not evaluator.models:\n",
    "        logger.error(\"No models loaded successfully. Aborting evaluation.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Define datasets to evaluate\n",
    "    datasets = {\n",
    "        \"SimVerb3500\": lambda: fetch_SimVerb3500('all'),\n",
    "        \"SimVerb3500-dev\": lambda: fetch_SimVerb3500('dev'),\n",
    "        \"SimVerb3500-test\": lambda: fetch_SimVerb3500('test'),\n",
    "        \"MEN\": lambda: fetch_MEN(\"all\"),\n",
    "        \"MEN-dev\": lambda: fetch_MEN(\"dev\"),\n",
    "        \"MEN-test\": lambda: fetch_MEN(\"test\"),\n",
    "        \"SimLex999\": fetch_SimLex999,\n",
    "        \"SCWS\": fetch_SCWS,\n",
    "        \"WS353\": fetch_WS353,\n",
    "        \"RG65\": fetch_RG65\n",
    "    }\n",
    "    \n",
    "    # Run evaluation\n",
    "    evaluator.run_evaluation(datasets)\n",
    "    results_df = evaluator.print_results_summary()\n",
    "    \n",
    "    if results_df is not None:\n",
    "        # Save results to same directory\n",
    "        results_file = os.path.join(model_dir, \"evaluation_results.csv\")\n",
    "        evaluator.save_results(results_file)\n",
    "        \n",
    "        # Save dimensionality analysis summary\n",
    "        summary_file = os.path.join(model_dir, \"dimensionality_summary.csv\")\n",
    "        results_df.to_csv(summary_file, index=False)\n",
    "        logger.info(f\"Dimension summary saved to: {summary_file}\")\n",
    "    else:\n",
    "        logger.warning(\"No results to save\")\n",
    "    \n",
    "    return evaluator, results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluator, results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
