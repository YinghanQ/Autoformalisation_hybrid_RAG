{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8852373",
   "metadata": {},
   "source": [
    "# Mathematical Definition Semantic Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a83bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "OPENAI_API_KEY = \"\"  # Replace with your actual OpenAI API key\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7bcd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import time\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Configuration Section ---\n",
    "mathlib_root_dir = \"path/to/mathlib4/Mathlib\" # change this to your mathlib4 directory, download the repository first https://github.com/leanprover-community/mathlib4.git\n",
    "output_file = \"./informal_data/informal_linear_algebra_Finsupp.json\"\n",
    "\n",
    "# Get all modules in LinearAlgebra directory\n",
    "def get_all_linear_algebra_modules():\n",
    "    \"\"\"Get all module paths under Mathlib.LinearAlgebra directory\"\"\"\n",
    "    linear_algebra_dir = os.path.join(mathlib_root_dir, \"LinearAlgebra/Finsupp\")\n",
    "    modules = set()\n",
    "    \n",
    "    for root, dirs, files in os.walk(linear_algebra_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".lean\"):\n",
    "                full_path = os.path.join(root, file)\n",
    "                module_name = get_module_path(full_path)\n",
    "                if module_name.startswith(\"Mathlib.LinearAlgebra.Finsupp\"):\n",
    "                    modules.add(module_name)\n",
    "    \n",
    "    return sorted(modules)\n",
    "\n",
    "# Convert module name to file path\n",
    "def module_to_filepath(module_name):\n",
    "    \"\"\"Convert module name to file path, supporting versioned files\"\"\"\n",
    "    # Remove possible version suffix\n",
    "    clean_name = re.sub(r'\\.v\\d+$', '', module_name)\n",
    "    \n",
    "    # Build relative path\n",
    "    relative_path = clean_name.replace(\"Mathlib.\", \"\").replace(\".\", os.sep) + \".lean\"\n",
    "    full_path = os.path.join(mathlib_root_dir, relative_path)\n",
    "    \n",
    "    # Check if file exists\n",
    "    if os.path.exists(full_path):\n",
    "        return full_path\n",
    "    \n",
    "    # Try versioned path\n",
    "    versioned_path = full_path.replace(\".lean\", \".v1.lean\")\n",
    "    if os.path.exists(versioned_path):\n",
    "        return versioned_path\n",
    "    \n",
    "    print(f\"‚ö†Ô∏è File does not exist: {full_path}\")\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    print(f\"Mathlib root directory: {os.path.abspath(mathlib_root_dir)}\")\n",
    "    return None\n",
    "\n",
    "# Get module name from file path\n",
    "def get_module_path(file_path):\n",
    "    \"\"\"Get module name from file path\"\"\"\n",
    "    try:\n",
    "        # Get path relative to mathlib_root_dir\n",
    "        relative_path = os.path.relpath(file_path, start=mathlib_root_dir)\n",
    "        \n",
    "        # Remove extension and version suffix\n",
    "        module_path = os.path.splitext(relative_path)[0]\n",
    "        module_path = re.sub(r'\\.v\\d+$', '', module_path)\n",
    "        \n",
    "        # Replace path separators and add Mathlib prefix\n",
    "        return \"Mathlib.\" + module_path.replace(os.sep, '.')\n",
    "    except ValueError:\n",
    "        return file_path\n",
    "\n",
    "# Extract file content\n",
    "def extract_file_content(filepath):\n",
    "    if not filepath or not os.path.exists(filepath):\n",
    "        return \"\"\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read file: {filepath} - {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Extract module docstring\n",
    "def extract_module_docstring(content):\n",
    "    if not content:\n",
    "        return \"\"\n",
    "    try:\n",
    "        match = re.search(r'/-!(.*?)-/s', content, re.DOTALL)\n",
    "        if match:\n",
    "            doc = match.group(1).strip()\n",
    "            doc = re.sub(r'^[/*\\s-]+', '', doc, flags=re.MULTILINE)\n",
    "            doc = re.sub(r'[/*\\s-]+$', '', doc, flags=re.MULTILINE)\n",
    "            return doc\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract documentation: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Extract imports\n",
    "def extract_imports(content):\n",
    "    if not content:\n",
    "        return []\n",
    "    try:\n",
    "        imports = set()\n",
    "        for match in re.finditer(r'import\\s+([\\w\\.]+)(?:\\s+--.*?$)?', content, re.MULTILINE):\n",
    "            imp = match.group(1).strip()\n",
    "            if imp:\n",
    "                imports.add(imp)\n",
    "        return list(imports)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to extract imports: {e}\")\n",
    "        return []\n",
    "\n",
    "# Extract definitions from content\n",
    "def extract_definitions_from_content(content):\n",
    "    pattern = r'(^\\s*(?:def|class|structure|abbrev|notation|lemma|theorem)\\s+\\w+[\\s\\S]*?)(?=\\s*^(?:def|class|structure|abbrev|notation|lemma|theorem)\\s+\\w+|\\Z)'\n",
    "    return [m.group(1).strip() for m in re.finditer(pattern, content, re.MULTILINE | re.DOTALL)]\n",
    "\n",
    "# ================================================================\n",
    "# Semantic Analysis Agent (extracts basic semantic info)\n",
    "# ================================================================\n",
    "BASIC_SEMANTIC_ANALYSIS_PROMPT = \"\"\"\n",
    "You are a mathematical definition semantic analysis expert. Process definitions from Lean files and output in strict JSON format. Extract all definitions and concepts from a Lean file:\n",
    "\n",
    "### Output Requirements (Strict JSON Format)\n",
    "{{\n",
    "  \"definitions\": [\n",
    "    {{\n",
    "      \"name\": \"Definition name\",\n",
    "      \"type\": \"Definition type (structure/class/def etc.)\",\n",
    "      \"signature\": \"Complete definition content in Lean4 code extracted from lean files\",\n",
    "      \"body\": \"Complete definition content\",\n",
    "      \"semantic_analysis\": {{\n",
    "      \"informal\": \"Concise natural language explanation of the core concepts\",\n",
    "        \n",
    "        \"concepts\": [\n",
    "          {{\n",
    "            \"name\": \"Mathematical Concepts and Terminologies name \",\n",
    "            \"informal_definition\": \"Generate a scientifically rigorous and precise natural language description of the core concepts\",\n",
    "            \"signature\": \"Complete definition content in Lean4 code extracted from lean files\",\n",
    "            ]\n",
    "          }}\n",
    "        ]\n",
    "      }}\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "### Instructions\n",
    "1. Focus exclusively on the formal definition content\n",
    "2. Generate concise, non-redundant explanations\n",
    "3. Omit all theorem proofs and implementation details\n",
    "4. Output must be valid JSON\n",
    "\"\"\"\n",
    "\n",
    "# =====================================\n",
    "# API Call Functions\n",
    "# =====================================\n",
    "def call_gpt_api(prompt, max_retries=3, model=\"gpt-4o\"):\n",
    "    \"\"\"Handles GPT API calls with retry logic\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = openai.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert in mathematical semantic analysis\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "                max_tokens=2048,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except openai.RateLimitError:\n",
    "            delay = (attempt + 1) * 5\n",
    "            print(f\"Rate limit hit. Retrying in {delay}s... ({attempt+1}/{max_retries})\")\n",
    "            time.sleep(delay)\n",
    "        except Exception as e:\n",
    "            print(f\"API Error: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                return \"\"\n",
    "    return \"\"\n",
    "\n",
    "def generate_basic_semantic_analysis(definition):\n",
    "    \"\"\"Extracts basic semantic information\"\"\"\n",
    "    prompt = BASIC_SEMANTIC_ANALYSIS_PROMPT + f\"\\n### Input Lean Definition Content\\n{definition}\"\n",
    "    return call_gpt_api(prompt)\n",
    "\n",
    "# =====================================\n",
    "# Utility Functions\n",
    "# =====================================\n",
    "def extract_json_from_response(response_text):\n",
    "    if not response_text:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(response_text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Direct parsing failed: {e}\")\n",
    "    try:\n",
    "        start_idx = response_text.find('{')\n",
    "        end_idx = response_text.rfind('}')\n",
    "        if start_idx != -1 and end_idx != -1 and end_idx > start_idx:\n",
    "            json_str = response_text[start_idx:end_idx+1]\n",
    "            return json.loads(json_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Fragment extraction failed: {e}\")\n",
    "    return None\n",
    "\n",
    "# =====================================\n",
    "# Main Processing Pipeline\n",
    "# =====================================\n",
    "def main():\n",
    "    # Dynamically get all LinearAlgebra modules\n",
    "    target_modules = get_all_linear_algebra_modules()\n",
    "    \n",
    "    if not target_modules:\n",
    "        print(\"‚ùå No modules found under Mathlib.LinearAlgebra\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üîç Found {len(target_modules)} LinearAlgebra modules\")\n",
    "    \n",
    "    grouped_results = defaultdict(lambda: {\n",
    "        \"docstring\": \"\",\n",
    "        \"dependencies\": [],\n",
    "        \"definitions\": []\n",
    "    })\n",
    "    total_def_count = 0\n",
    "    total_concept_count = 0\n",
    "\n",
    "    print(\"Starting processing of LinearAlgebra modules...\\n\")\n",
    "\n",
    "    for module_index, module_name in enumerate(target_modules):\n",
    "        path = module_to_filepath(module_name)\n",
    "        if not path or not os.path.isfile(path):\n",
    "            print(f\"File does not exist: {module_name}\")\n",
    "            continue\n",
    "\n",
    "        module_path = get_module_path(path)\n",
    "        file_content = extract_file_content(path)\n",
    "        if not file_content:\n",
    "            print(f\"File content is empty: {path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing module ({module_index+1}/{len(target_modules)}): {module_name}\")\n",
    "        print(f\"Path: {path}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        module_doc = extract_module_docstring(file_content)\n",
    "        dependencies = extract_imports(file_content)\n",
    "        definitions = extract_definitions_from_content(file_content)\n",
    "        \n",
    "        print(f\"Found {len(definitions)} definitions\")\n",
    "\n",
    "        for def_index, def_body in enumerate(definitions):\n",
    "            print(f\"\\n[Definition {def_index+1}/{len(definitions)}] Processing...\")\n",
    "            \n",
    "            # Semantic analysis\n",
    "            print(\"  Invoking Semantic Analysis Agent...\")\n",
    "            basic_response = generate_basic_semantic_analysis(def_body)\n",
    "            basic_data = extract_json_from_response(basic_response)\n",
    "            \n",
    "            if not basic_data or \"definitions\" not in basic_data:\n",
    "                print(\"  ‚ö†Ô∏è Agent did not return valid data. Skipping definition.\")\n",
    "                continue\n",
    "                \n",
    "            for def_entry in basic_data[\"definitions\"]:\n",
    "                concepts = def_entry.get(\"semantic_analysis\", {}).get(\"concepts\", [])\n",
    "                print(f\"  Found {len(concepts)} concepts\")\n",
    "                total_concept_count += len(concepts)\n",
    "                \n",
    "                # Add to final results\n",
    "                clean_definition = {\n",
    "                    \"name\": def_entry.get(\"name\", \"\"),\n",
    "                    \"type\": def_entry.get(\"type\", \"\"),\n",
    "                    \"signature\": def_entry.get(\"signature\", \"\"),\n",
    "                    \"body\": def_entry.get(\"body\", \"\"),\n",
    "                    \"semantic_analysis\": def_entry.get(\"semantic_analysis\", {})\n",
    "                }\n",
    "                grouped_results[module_path][\"definitions\"].append(clean_definition)\n",
    "                total_def_count += 1\n",
    "            \n",
    "            # Rate control between definitions\n",
    "            time.sleep(1.5)\n",
    "\n",
    "        # Add module metadata\n",
    "        grouped_results[module_path][\"docstring\"] = module_doc\n",
    "        grouped_results[module_path][\"dependencies\"] = dependencies\n",
    "        print(f\"‚úì Completed module: {module_name}\")\n",
    "\n",
    "    # Write final JSON file\n",
    "    print(f\"\\nWriting JSON file: {output_file}\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(grouped_results, f, indent=2, ensure_ascii=False)\n",
    " \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Processing Complete!\")\n",
    "    print(f\"  Modules processed: {len(grouped_results)}\")\n",
    "    print(f\"  Definitions extracted: {total_def_count}\")\n",
    "    print(f\"  Concepts analyzed: {total_concept_count}\")\n",
    "    print(f\"  Output file: {os.path.abspath(output_file)}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47915d95",
   "metadata": {},
   "source": [
    "## Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069b339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "def count_definitions_concepts(json_path):\n",
    "    definitions_count = 0\n",
    "    concepts_count = 0\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for module_name, module_content in data.items():\n",
    "        definitions = module_content.get(\"definitions\", [])\n",
    "        definitions_count += len(definitions)\n",
    "\n",
    "        for definition in definitions:\n",
    "            semantic = definition.get(\"semantic_analysis\", {})\n",
    "            concepts = semantic.get(\"concepts\", [])\n",
    "            concepts_count += len(concepts)\n",
    "\n",
    "    return definitions_count, concepts_count\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    json_folder = \"./informal_data/\"\n",
    "    json_files = glob.glob(os.path.join(json_folder, \"*.json\"))\n",
    "\n",
    "    total_definitions = 0\n",
    "    total_concepts = 0\n",
    "\n",
    "    for json_path in json_files:\n",
    "        defs, cons = count_definitions_concepts(json_path)\n",
    "        total_definitions += defs\n",
    "        total_concepts += cons\n",
    "\n",
    "    print(f\"Total definitions in all JSON files in the folder: {total_definitions}\")\n",
    "    print(f\"Total concepts in all JSON files in the folder: {total_concepts}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
