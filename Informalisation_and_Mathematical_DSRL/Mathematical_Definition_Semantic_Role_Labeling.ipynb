{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff54aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "from openai import OpenAI\n",
    "\n",
    "# ============ Configuration ============\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", \"your_api_key\")  # Replace with your actual API key\n",
    "json_folder = \"./informal_data/\"\n",
    "output_file = \"my_dataset_cleaned.txt\"\n",
    "integrated_output_file = \"integrated_dataset_with_triples.json\"\n",
    "\n",
    "# ============ OpenAI Client ============\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# ============ JSON Style Examples (consistent with target output) ============\n",
    "EXAMPLES_JSON = [\n",
    "    {\n",
    "        \"text\": \"An infinitely extending one-dimensional figure that has no curvature.\",\n",
    "        \"concept\": \"line\",\n",
    "        \"triples\": [\n",
    "            {\n",
    "                \"subject\": \"line\",\n",
    "                \"role\": \"SUPERTYPE\",\n",
    "                \"lemma\": \"figure\",\n",
    "                \"explanation\": \"Innermost leftmost NP contains NN 'figure'.\"\n",
    "            },\n",
    "            {\n",
    "                \"subject\": \"line\",\n",
    "                \"role\": \"DIFFERENTIA_QUALITY\",\n",
    "                \"lemma\": \"infinitely extending\",\n",
    "                \"explanation\": \"JJ/VP modifying the supertype; leftover in leftmost NP.\"\n",
    "            },\n",
    "            {\n",
    "                \"subject\": \"line\",\n",
    "                \"role\": \"DIFFERENTIA_QUALITY\",\n",
    "                \"lemma\": \"one-dimensional\",\n",
    "                \"explanation\": \"JJ indicating a property in the leftmost NP.\"\n",
    "            },\n",
    "            {\n",
    "                \"subject\": \"line\",\n",
    "                \"role\": \"ASSOCIATED_FACT\",\n",
    "                \"lemma\": \"has no curvature\",\n",
    "                \"explanation\": \"SBAR/VP clause expressing associated fact.\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"An algebra constructed from a module over a ring R, which is symmetric in the sense that it is generated by elements of the module with relations that make the multiplication commutative.\",\n",
    "        \"concept\": \"line\",\n",
    "        \"triples\": [\n",
    "            {\n",
    "                    \"subject\": \"symmetric algebra\",\n",
    "                    \"role\": \"DIFFERENTIA_QUALITY\",\n",
    "                    \"lemma\": \"symmetric\",\n",
    "                    \"explanation\": \"Key term describing the symmetric property.\"\n",
    "                },\n",
    "                {\n",
    "                    \"subject\": \"symmetric algebra\",\n",
    "                    \"role\": \"DIFFERENTIA_QUALITY\",\n",
    "                    \"lemma\": \"generated\",\n",
    "                    \"explanation\": \"Key term describing the symmetric property.\"\n",
    "                },\n",
    "                {\n",
    "                    \"subject\": \"symmetric algebra\",\n",
    "                    \"role\": \"DIFFERENTIA_QUALITY\",\n",
    "                    \"lemma\": \"elements\",\n",
    "                    \"explanation\": \"Key term describing the symmetric property.\"\n",
    "                },\n",
    "                {\n",
    "                    \"subject\": \"symmetric algebra\",\n",
    "                    \"role\": \"DIFFERENTIA_QUALITY\",\n",
    "                    \"lemma\": \"module\",\n",
    "                    \"explanation\": \"Key term describing the symmetric property.\"\n",
    "                },\n",
    "                {\n",
    "                    \"subject\": \"symmetric algebra\",\n",
    "                    \"role\": \"DIFFERENTIA_QUALITY\",\n",
    "                    \"lemma\": \"relations\",\n",
    "                    \"explanation\": \"Key term describing the symmetric property.\"\n",
    "                },\n",
    "                {\n",
    "                    \"subject\": \"symmetric algebra\",\n",
    "                    \"role\": \"DIFFERENTIA_QUALITY\",\n",
    "                    \"lemma\": \"multiplication\",\n",
    "                    \"explanation\": \"Key term describing the symmetric property.\"\n",
    "                },\n",
    "                {\n",
    "                    \"subject\": \"symmetric algebra\",\n",
    "                    \"role\": \"DIFFERENTIA_QUALITY\",\n",
    "                    \"lemma\": \"commutative\",\n",
    "                    \"explanation\": \"Key term describing the symmetric property.\"\n",
    "                }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# ============ Prompt ============\n",
    "TRIPLE_EXTRACTION_PROMPT = \"\"\"\n",
    "You are a semantic role labeling expert specializing in natural language definitions.\n",
    "Your task: extract triples (subject, role, lemma) from the given informal definition of a mathematicalconcept.\n",
    "\n",
    "### Semantic Roles and Syntactic Patterns\n",
    "| Role | Most common syntactic patterns |\n",
    "| --- | --- |\n",
    "| SUPERTYPE | innermost and leftmost NP containing at least one NN |\n",
    "| DIFFERENTIA_QUALITY | leftovers in the innermost and leftmost NP; PP beginning with \"of\" |\n",
    "| DIFFERENTIA_EVENT | SBAR; VP |\n",
    "| EVENT_LOCATION | PP inside a SBAR or VP, possibly having a location named entity |\n",
    "| EVENT_TIME | PP inside a SBAR or VP, possibly having a time interval named entity |\n",
    "| ORIGIN_LOCATION | PP not inside a SBAR or VP, possibly having a location named entity |\n",
    "| QUALITY_MODIFIER | NN, JJ or RB referring to an element inside a differentia quality |\n",
    "| PURPOSE | VP beginning with TO; PP beginning with \"for\" with a VP right after |\n",
    "| ASSOCIATED_FACT | SBAR; PP not beginning with \"for\" with a VP right after |\n",
    "| ACCESSORY_DETERMINER | whole expression before supertype; common accessory expression |\n",
    "| ACCESSORY_QUALITY | JJ, presence of a differentia quality, common accessory word |\n",
    "\n",
    "### Requirements\n",
    "1) Subject MUST be exactly the provided concept name: \"{concept_name}\".\n",
    "2) Extract ALL possible triples from the text, adhering to the roles/patterns above.\n",
    "3) For each triple:\n",
    "   - Include a short \"explanation\" of the syntactic cue.\n",
    "   - Lemma MUST be a **short meaningful mathematical term or keyword**, not a full sentence.\n",
    "   - Remove function words (e.g., that, it, is, with) from lemmas.\n",
    "   - If a lemma contains multiple keywords (e.g., \"symmetric; generated; elements\"), **split them into separate triples**, one keyword per triple.\n",
    "4) Return ONLY a JSON object with this shape (no extra text):\n",
    "{{\n",
    "  \"concept\": \"{concept_name}\",\n",
    "  \"triples\": [\n",
    "    {{\"subject\": \"...\", \"role\": \"...\", \"lemma\": \"...\", \"explanation\": \"...\"}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "### Few-shot JSON Examples\n",
    "{examples_block}\n",
    "\n",
    "### Input\n",
    "Concept: {concept_name}\n",
    "Text: {text}\n",
    "\n",
    "### Output\n",
    "Return ONLY the JSON object described above.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _examples_block(examples: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Embed the JSON examples into the prompt (as plain JSON snippets) to help the model align the format.\"\"\"\n",
    "    return json.dumps(examples, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ============ Parsing Utilities ============\n",
    "def clean_json_block(text: str) -> str:\n",
    "    \"\"\"Remove Markdown code blocks and extra wrapping, attempting to retain pure JSON.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Prioritize matching ```json ... ```\n",
    "    m = re.search(r\"```json\\s*(\\{[\\s\\S]*?\\})\\s*```\", text, re.DOTALL)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    # Then match ``` ... ```\n",
    "    m = re.search(r\"```\\s*(\\{[\\s\\S]*?\\})\\s*```\", text, re.DOTALL)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    # Attempt to extract the largest JSON block from the first { to the last }\n",
    "    m = re.search(r\"\\{[\\s\\S]*\\}\", text, re.DOTALL)\n",
    "    if m:\n",
    "        return m.group(0).strip()\n",
    "    return text.strip()\n",
    "\n",
    "def try_parse_json(text: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Attempt to parse text into a JSON object.\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    # Direct attempt\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Attempt after cleaning\n",
    "    cleaned = clean_json_block(text)\n",
    "    if not cleaned:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(cleaned)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def try_parse_tsv_lines(text: str, concept_name: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fallback: If the model outputs TSV (subject \\t role \\t object), parse it back.\n",
    "    Returns a structure consistent with the main JSON: {\"concept\": ..., \"triples\":[...]}\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    triples = []\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "    ok = False\n",
    "    for ln in lines:\n",
    "        parts = ln.split(\"\\t\")\n",
    "        if len(parts) >= 3:\n",
    "            ok = True\n",
    "            subj, role, obj = parts[0], parts[1], \"\\t\".join(parts[2:])  # Handle potential tabs in object\n",
    "            triples.append({\n",
    "                \"subject\": subj.strip(),\n",
    "                \"role\": role.strip(),\n",
    "                \"lemma\": obj.strip(),\n",
    "                \"explanation\": \"parsed from TSV fallback\"\n",
    "            })\n",
    "    if ok:\n",
    "        return {\"concept\": concept_name, \"triples\": triples}\n",
    "    return None\n",
    "\n",
    "# ============ GPT Call ============\n",
    "def annotate_with_gpt(concept_name: str, text: str, model: str = \"gpt-4o\") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Call GPT for triple extraction, attempting to return a JSON object:\n",
    "    {\n",
    "      \"concept\": concept_name,\n",
    "      \"triples\": [{\"subject\":..., \"role\":..., \"lemma\":..., \"explanation\":...}, ...]\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not concept_name or not text:\n",
    "        print(f\"[INPUT ERROR] Empty concept_name or text: '{concept_name}', '{text}'\")\n",
    "        return None\n",
    "    \n",
    "    prompt = TRIPLE_EXTRACTION_PROMPT.format(\n",
    "        concept_name=concept_name,\n",
    "        text=text,\n",
    "        examples_block=_examples_block(EXAMPLES_JSON)\n",
    "    )\n",
    "\n",
    "    # Call GPT API\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0,\n",
    "            max_tokens=2000  # Increased max tokens\n",
    "        )\n",
    "        content = (resp.choices[0].message.content or \"\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[API ERROR] '{concept_name}' -> {e}\")\n",
    "        return None\n",
    "\n",
    "    if not content:\n",
    "        print(f\"[EMPTY RESPONSE] '{concept_name}' -> got empty response from API\")\n",
    "        return None\n",
    "\n",
    "    # Parsing: JSON -> (if failed) cleaned JSON -> (if still failed) TSV fallback\n",
    "    parsed = try_parse_json(content)\n",
    "    if parsed is None:\n",
    "        parsed = try_parse_tsv_lines(content, concept_name)\n",
    "\n",
    "    if parsed is None:\n",
    "        # Print raw output for debugging\n",
    "        print(f\"[PARSE ERROR] '{concept_name}' -> cannot parse output. Raw output (first 500 chars):\\n{content[:500]}...\\n\")\n",
    "        return None\n",
    "\n",
    "    # Basic validation\n",
    "    if not isinstance(parsed, dict):\n",
    "        print(f\"[FORMAT ERROR] '{concept_name}' -> parsed result is not a dict.\")\n",
    "        return None\n",
    "        \n",
    "    if \"triples\" not in parsed:\n",
    "        print(f\"[FORMAT ERROR] '{concept_name}' -> parsed but missing 'triples' field.\")\n",
    "        return None\n",
    "        \n",
    "    if not isinstance(parsed[\"triples\"], list):\n",
    "        print(f\"[FORMAT ERROR] '{concept_name}' -> 'triples' is not a list.\")\n",
    "        return None\n",
    "        \n",
    "    if parsed.get(\"concept\") in (None, \"\"):\n",
    "        parsed[\"concept\"] = concept_name\n",
    "\n",
    "    return parsed\n",
    "\n",
    "# ============ Unified Writing to Triple File ============\n",
    "def normalize_token(s: str, lower=True) -> str:\n",
    "    \"\"\"Convert to underscore, optionally lowercase, and remove extra whitespace.\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = re.sub(r\"\\s+\", \"_\", s.strip())\n",
    "    return s.lower() if lower else s\n",
    "\n",
    "def save_triples_struct(triples_obj: Dict[str, Any], outfile: str):\n",
    "    \"\"\"\n",
    "    Write {\"concept\":..., \"triples\":[{subject,role,object,...},...]} as:\n",
    "    subject \\t ROLE \\t object\n",
    "    \"\"\"\n",
    "    triples = triples_obj.get(\"triples\", [])\n",
    "    if not triples:\n",
    "        return\n",
    "\n",
    "    with open(outfile, \"a\", encoding=\"utf-8\") as f:\n",
    "        for t in triples:\n",
    "            if not isinstance(t, dict):\n",
    "                continue\n",
    "            subj = normalize_token(str(t.get(\"subject\", \"\")), lower=True)\n",
    "            role = normalize_token(str(t.get(\"role\", \"\")), lower=False).upper()\n",
    "            lemma  = normalize_token(str(t.get(\"lemma\", \"\")), lower=True)\n",
    "            if subj and role and lemma:\n",
    "                f.write(f\"{subj}\\t{role}\\t{lemma}\\n\")\n",
    "\n",
    "# ============ New: Integrated JSON Functionality ============\n",
    "def save_integrated_json(all_data: Dict[str, Any], outfile: str):\n",
    "    \"\"\"\n",
    "    Save all data (including original information and newly extracted triples) as a JSON file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(outfile, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"[SUCCESS] Integrated JSON saved to: {outfile}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Cannot save integrated JSON: {e}\")\n",
    "\n",
    "def process_json_with_integration(json_path: str, integrated_data: Dict[str, Any], outfile: str):\n",
    "    \"\"\"\n",
    "    Process a JSON file and integrate the triples into the original structure, while maintaining the original TSV output functionality.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"[LOAD ERROR] {json_path} -> {e}\")\n",
    "        return\n",
    "\n",
    "    processed_count = 0\n",
    "    success_count = 0\n",
    "    \n",
    "    for module_name, module_content in data.items():\n",
    "        print(f\"  Processing module: {module_name}\")\n",
    "        \n",
    "        # Initialize integrated data structure, deep copy original data\n",
    "        if module_name not in integrated_data:\n",
    "            integrated_data[module_name] = json.loads(json.dumps(module_content))\n",
    "        \n",
    "        definitions = module_content.get(\"definitions\", [])\n",
    "        \n",
    "        for def_idx, definition in enumerate(definitions):\n",
    "            semantic = definition.get(\"semantic_analysis\", {})\n",
    "            concepts = semantic.get(\"concepts\", [])\n",
    "            \n",
    "            for concept_idx, concept in enumerate(concepts):\n",
    "                concept_name = concept.get(\"name\")\n",
    "                informal_definition = concept.get(\"informal_definition\")\n",
    "                \n",
    "                if not concept_name or not informal_definition:\n",
    "                    print(f\"    [SKIP] Missing name or informal_definition: name='{concept_name}', definition='{informal_definition}'\")\n",
    "                    continue\n",
    "                \n",
    "                processed_count += 1\n",
    "                print(f\"    Processing concept: {concept_name}\")\n",
    "                \n",
    "                triples_obj = annotate_with_gpt(concept_name, informal_definition)\n",
    "                if triples_obj:\n",
    "                    # Save to TSV file (original functionality)\n",
    "                    save_triples_struct(triples_obj, outfile)\n",
    "                    \n",
    "                    # Add triples to the integrated JSON structure (new functionality)\n",
    "                    try:\n",
    "                        integrated_data[module_name][\"definitions\"][def_idx][\"semantic_analysis\"][\"concepts\"][concept_idx][\"extracted_triples\"] = triples_obj.get(\"triples\", [])\n",
    "                        integrated_data[module_name][\"definitions\"][def_idx][\"semantic_analysis\"][\"concepts\"][concept_idx][\"extraction_metadata\"] = {\n",
    "                            \"extraction_status\": \"success\",\n",
    "                            \"triples_count\": len(triples_obj.get(\"triples\", [])),\n",
    "                            \"extraction_source\": \"gpt-4o\"\n",
    "                        }\n",
    "                    except (KeyError, IndexError) as e:\n",
    "                        print(f\"    [JSON STRUCTURE ERROR] Cannot add triples to integrated data: {e}\")\n",
    "                    \n",
    "                    success_count += 1\n",
    "                    print(f\"    [SUCCESS] '{concept_name}' -> {len(triples_obj.get('triples', []))} triples extracted\")\n",
    "                else:\n",
    "                    # Mark failure in the integrated JSON\n",
    "                    try:\n",
    "                        integrated_data[module_name][\"definitions\"][def_idx][\"semantic_analysis\"][\"concepts\"][concept_idx][\"extracted_triples\"] = []\n",
    "                        integrated_data[module_name][\"definitions\"][def_idx][\"semantic_analysis\"][\"concepts\"][concept_idx][\"extraction_metadata\"] = {\n",
    "                            \"extraction_status\": \"failed\",\n",
    "                            \"triples_count\": 0,\n",
    "                            \"extraction_source\": \"gpt-4o\"\n",
    "                        }\n",
    "                    except (KeyError, IndexError) as e:\n",
    "                        print(f\"    [JSON STRUCTURE ERROR] Cannot add failure marker to integrated data: {e}\")\n",
    "                    \n",
    "                    print(f\"    [FAILED] '{concept_name}' -> no triples extracted.\")\n",
    "    \n",
    "    print(f\"  Module summary: {success_count}/{processed_count} concepts successfully processed\")\n",
    "\n",
    "# ============ Retain original processing function (backup) ============\n",
    "def process_json(json_path: str, outfile: str):\n",
    "    \"\"\"\n",
    "    Original processing function, only outputs TSV format (retained as backup).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"[LOAD ERROR] {json_path} -> {e}\")\n",
    "        return\n",
    "\n",
    "    processed_count = 0\n",
    "    success_count = 0\n",
    "    \n",
    "    for module_name, module_content in data.items():\n",
    "        print(f\"  Processing module: {module_name}\")\n",
    "        definitions = module_content.get(\"definitions\", [])\n",
    "        \n",
    "        for definition in definitions:\n",
    "            semantic = definition.get(\"semantic_analysis\", {})\n",
    "            concepts = semantic.get(\"concepts\", [])\n",
    "            \n",
    "            for concept in concepts:\n",
    "                concept_name = concept.get(\"name\")\n",
    "                informal_definition = concept.get(\"informal_definition\")\n",
    "                \n",
    "                if not concept_name or not informal_definition:\n",
    "                    print(f\"    [SKIP] Missing name or informal_definition: name='{concept_name}', definition='{informal_definition}'\")\n",
    "                    continue\n",
    "                \n",
    "                processed_count += 1\n",
    "                print(f\"    Processing concept: {concept_name}\")\n",
    "                \n",
    "                triples_obj = annotate_with_gpt(concept_name, informal_definition)\n",
    "                if triples_obj:\n",
    "                    save_triples_struct(triples_obj, outfile)\n",
    "                    success_count += 1\n",
    "                    print(f\"    [SUCCESS] '{concept_name}' -> {len(triples_obj.get('triples', []))} triples extracted\")\n",
    "                else:\n",
    "                    print(f\"    [FAILED] '{concept_name}' -> no triples written.\")\n",
    "    \n",
    "    print(f\"  Module summary: {success_count}/{processed_count} concepts successfully processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a039df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ Main Program ============\n",
    "if __name__ == \"__main__\":\n",
    "    # Validate API key\n",
    "    if not api_key or api_key == \"your_api_key\":\n",
    "        print(\"[ERROR] Please set your OpenAI API key in the environment variable OPENAI_API_KEY or modify the code.\")\n",
    "        exit(1)\n",
    "    \n",
    "\n",
    "    # Collect all JSON files\n",
    "    json_files = glob.glob(os.path.join(json_folder, \"*.json\"))\n",
    "    if not json_files:\n",
    "        print(f\"[WARN] No JSON files found in: {json_folder}\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files to process.\")\n",
    "\n",
    "    # For storing all integrated data\n",
    "    integrated_data = {}\n",
    "    total_processed = 0\n",
    "    \n",
    "    # Process all JSON files, generating both TSV and integrated JSON\n",
    "    for json_path in json_files:\n",
    "        print(f\"\\nProcessing {os.path.basename(json_path)} ...\")\n",
    "        process_json_with_integration(json_path, integrated_data, output_file)\n",
    "        total_processed += 1\n",
    "\n",
    "    # Save the integrated JSON file\n",
    "    if integrated_data:\n",
    "        save_integrated_json(integrated_data, integrated_output_file)\n",
    "    else:\n",
    "        print(\"[WARN] No data was integrated, skipping JSON output.\")\n",
    "\n",
    "    print(f\"\\nAnnotation completed! Processed {total_processed} JSON files in total.\")\n",
    "    print(f\"TSV format triples saved to: {output_file}\")\n",
    "    print(f\"Integrated JSON file saved to: {integrated_output_file}\")\n",
    "    \n",
    "    # Display line count of output file\n",
    "    try:\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            line_count = len(f.readlines())\n",
    "        print(f\"TSV output file contains {line_count} lines of triple data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Cannot count lines in TSV output file: {e}\")\n",
    "    \n",
    "    # Display statistics for the integrated JSON\n",
    "    try:\n",
    "        with open(integrated_output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            integrated_json = json.load(f)\n",
    "        \n",
    "        total_concepts = 0\n",
    "        total_extracted_triples = 0\n",
    "        successful_extractions = 0\n",
    "        \n",
    "        for module_name, module_content in integrated_json.items():\n",
    "            definitions = module_content.get(\"definitions\", [])\n",
    "            for definition in definitions:\n",
    "                semantic = definition.get(\"semantic_analysis\", {})\n",
    "                concepts = semantic.get(\"concepts\", [])\n",
    "                for concept in concepts:\n",
    "                    total_concepts += 1\n",
    "                    extracted_triples = concept.get(\"triples\", [])\n",
    "                    if extracted_triples:\n",
    "                        successful_extractions += 1\n",
    "                        total_extracted_triples += len(extracted_triples)\n",
    "        \n",
    "        print(f\"Integrated JSON statistics: {total_concepts} concepts, {successful_extractions} successful extractions, {total_extracted_triples} triples in total.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Cannot generate statistics for integrated JSON file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
