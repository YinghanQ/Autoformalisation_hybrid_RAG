{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236129bf",
   "metadata": {},
   "source": [
    "# Construct Hybrid Datasets and Prepare Training datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e11ef6c",
   "metadata": {},
   "source": [
    "# 1. Construct Hybrid Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1a42c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def clean_entity(entity):\n",
    "    \"\"\"Clean entity (definiendum or lemma) using the same rules, return cleaned form or empty string\"\"\"\n",
    "    if not entity or not isinstance(entity, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Common mathematical symbols to remove\n",
    "    math_symbols = set(\"+-*/=∑∏√∫<>∈∉{}[]()\")\n",
    "    \n",
    "    # Remove mathematical symbols\n",
    "    entity = ''.join(c for c in entity if c not in math_symbols)\n",
    "    \n",
    "    # Remove non-ASCII characters\n",
    "    entity = ''.join(c for c in entity if ord(c) < 128)\n",
    "    \n",
    "    # Replace spaces with underscores\n",
    "    entity = entity.replace(\" \", \"_\")\n",
    "    \n",
    "    # Keep only valid characters (letters, numbers, underscores)\n",
    "    entity = re.sub(r'[^A-Za-z0-9_]', '', entity)\n",
    "    \n",
    "    # If empty after cleaning, consider invalid\n",
    "    if not entity:\n",
    "        return \"\"\n",
    "    \n",
    "    return entity\n",
    "\n",
    "def extract_lemma(token):\n",
    "    \"\"\"Extract lemma and remove ending punctuation, also clean single letters and 'i.e.'\"\"\"\n",
    "    if not token:\n",
    "        return None\n",
    "    \n",
    "    token = token.lower().strip()\n",
    "    token = token.rstrip('.,;:!?')  # Remove ending punctuation\n",
    "\n",
    "    # Basic lemmatization\n",
    "    if token.endswith('ies'):\n",
    "        token = token[:-3] + 'y'\n",
    "    elif token.endswith('es') and len(token) > 3:\n",
    "        token = token[:-2]\n",
    "    elif token.endswith('s') and len(token) > 3:\n",
    "        token = token[:-1]\n",
    "    elif token.endswith('ing') and len(token) > 4:\n",
    "        token = token[:-3]\n",
    "\n",
    "    # Additional cleaning\n",
    "    if len(token) == 1 or token == 'i.e':\n",
    "        return None\n",
    "    \n",
    "    # Apply the same cleaning rules as definiendum\n",
    "    token = clean_entity(token)\n",
    "    if not token:\n",
    "        return None\n",
    "    \n",
    "    return token\n",
    "\n",
    "def clean_triple(triple):\n",
    "    \"\"\"Clean a single triple, return cleaned triple or None (if invalid)\"\"\"\n",
    "    if not isinstance(triple, dict):\n",
    "        return None\n",
    "    \n",
    "    subject = triple.get(\"subject\", \"\")\n",
    "    role = triple.get(\"role\", \"\")\n",
    "    lemma = triple.get(\"lemma\", \"\")\n",
    "    explanation = triple.get(\"explanation\", \"\")\n",
    "    \n",
    "    # Clean subject\n",
    "    cleaned_subject = clean_entity(subject)\n",
    "    if not cleaned_subject:\n",
    "        return None\n",
    "    \n",
    "    # Clean lemma\n",
    "    cleaned_lemma = extract_lemma(lemma)\n",
    "    if not cleaned_lemma:\n",
    "        return None\n",
    "    \n",
    "    # Clean role (ensure uppercase and correct format)\n",
    "    cleaned_role = role.upper().replace(\"-\", \"_\").strip()\n",
    "    if not cleaned_role:\n",
    "        return None\n",
    "    \n",
    "    # Return cleaned triple\n",
    "    return {\n",
    "        \"subject\": cleaned_subject.lower(),\n",
    "        \"role\": cleaned_role,\n",
    "        \"lemma\": cleaned_lemma,\n",
    "        \"explanation\": explanation  # Keep original explanation\n",
    "    }\n",
    "\n",
    "def normalize_concept_name(name):\n",
    "    \"\"\"Standardize concept name for matching\"\"\"\n",
    "    if not name or not isinstance(name, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Use the same cleaning rules\n",
    "    return clean_entity(name)\n",
    "\n",
    "def load_and_index_data(embeddings_file, triples_file):\n",
    "    \"\"\"Load and index two datasets\"\"\"\n",
    "    \n",
    "    # Load embeddings data\n",
    "    with open(embeddings_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        embeddings_data = json.load(f)\n",
    "    \n",
    "    # Load triples data\n",
    "    with open(triples_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        triples_data = json.load(f)\n",
    "    \n",
    "    return embeddings_data, triples_data\n",
    "\n",
    "def debug_triples_structure(triples_data):\n",
    "    \"\"\"Debug the structure of triples data\"\"\"\n",
    "    print(\"=== Debugging Triples Data Structure ===\")\n",
    "    \n",
    "    for module_name, module_content in list(triples_data.items())[:2]:\n",
    "        print(f\"\\nModule: {module_name}\")\n",
    "        for definition in module_content.get(\"definitions\", [])[:2]:\n",
    "            semantic_analysis = definition.get(\"semantic_analysis\", {})\n",
    "            concepts = semantic_analysis.get(\"concepts\", [])\n",
    "            \n",
    "            for concept in concepts[:2]:\n",
    "                print(f\"  Concept: {concept.get('name')}\")\n",
    "                print(f\"  All fields: {list(concept.keys())}\")\n",
    "                \n",
    "                if \"triples\" in concept:\n",
    "                    triples_value = concept[\"triples\"]\n",
    "                    print(f\"  triples type: {type(triples_value)}\")\n",
    "                    if isinstance(triples_value, list):\n",
    "                        print(f\"  triples count: {len(triples_value)}\")\n",
    "                        if triples_value:\n",
    "                            first_triple = triples_value[0]\n",
    "                            print(f\"  First triple - subject: {first_triple.get('subject')}\")\n",
    "                            print(f\"  First triple - role: {first_triple.get('role')}\")\n",
    "                            print(f\"  First triple - lemma: {first_triple.get('lemma')}\")\n",
    "                print()\n",
    "\n",
    "def build_concept_mapping(triples_data):\n",
    "    \"\"\"Build concept name to triples mapping table and perform cleaning\"\"\"\n",
    "    concept_mapping = defaultdict(list)\n",
    "    stats = defaultdict(int)\n",
    "    \n",
    "    for module_name, module_content in triples_data.items():\n",
    "        for definition in module_content.get(\"definitions\", []):\n",
    "            semantic_analysis = definition.get(\"semantic_analysis\", {})\n",
    "            concepts = semantic_analysis.get(\"concepts\", [])\n",
    "            \n",
    "            for concept in concepts:\n",
    "                concept_name = concept.get(\"name\")\n",
    "                \n",
    "                # Safely get triples field\n",
    "                triples = concept.get(\"triples\", [])\n",
    "                \n",
    "                # Handle triples field type issues\n",
    "                if isinstance(triples, str):\n",
    "                    try:\n",
    "                        triples = json.loads(triples)\n",
    "                        stats['string_triples_parsed'] += 1\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Warning: Unable to parse string triples: {triples[:100]}...\")\n",
    "                        triples = []\n",
    "                        stats['invalid_string_triples'] += 1\n",
    "                \n",
    "                elif not isinstance(triples, list):\n",
    "                    triples = []\n",
    "                    stats['non_list_triples'] += 1\n",
    "                \n",
    "                if concept_name and triples:\n",
    "                    normalized_name = normalize_concept_name(concept_name)\n",
    "                    if not normalized_name:\n",
    "                        stats['invalid_concept_names'] += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Clean triples\n",
    "                    cleaned_triples = []\n",
    "                    for triple in triples:\n",
    "                        cleaned_triple = clean_triple(triple)\n",
    "                        if cleaned_triple:\n",
    "                            cleaned_triples.append(cleaned_triple)\n",
    "                    \n",
    "                    stats['original_triples'] += len(triples)\n",
    "                    stats['cleaned_triples'] += len(cleaned_triples)\n",
    "                    stats['filtered_triples'] += (len(triples) - len(cleaned_triples))\n",
    "                    \n",
    "                    if cleaned_triples:\n",
    "                        concept_mapping[normalized_name].append({\n",
    "                            \"triples\": cleaned_triples,\n",
    "                            \"source_module\": module_name,\n",
    "                            \"original_name\": concept_name\n",
    "                        })\n",
    "                        stats['concepts_mapped'] += 1\n",
    "                    else:\n",
    "                        stats['concepts_no_valid_triples'] += 1\n",
    "                    \n",
    "                elif concept_name:\n",
    "                    stats['concepts_no_triples'] += 1\n",
    "                else:\n",
    "                    stats['concepts_no_name'] += 1\n",
    "    \n",
    "    print(f\"Triples data mapping completed: {dict(stats)}\")\n",
    "    return concept_mapping\n",
    "\n",
    "def safe_extend_triples(all_triples, data):\n",
    "    \"\"\"Safely extend triples list\"\"\"\n",
    "    triples = data.get(\"triples\", [])\n",
    "    \n",
    "    if isinstance(triples, list):\n",
    "        all_triples.extend(triples)\n",
    "    elif isinstance(triples, str):\n",
    "        try:\n",
    "            parsed_triples = json.loads(triples)\n",
    "            if isinstance(parsed_triples, list):\n",
    "                all_triples.extend(parsed_triples)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Unable to parse triples string\")\n",
    "    else:\n",
    "        print(f\"Warning: Unknown triples type: {type(triples)}\")\n",
    "\n",
    "def merge_datasets_by_concept_name(embeddings_data, concept_mapping):\n",
    "    \"\"\"Merge datasets by concept name matching\"\"\"\n",
    "    \n",
    "    stats = defaultdict(int)\n",
    "    match_details = []\n",
    "    \n",
    "    for module_name, module_content in embeddings_data.items():\n",
    "        stats['modules_processed'] += 1\n",
    "        \n",
    "        for definition in module_content.get(\"definitions\", []):\n",
    "            semantic_analysis = definition.get(\"semantic_analysis\", {})\n",
    "            concepts = semantic_analysis.get(\"concepts\", [])\n",
    "            \n",
    "            for concept in concepts:\n",
    "                concept_name = concept.get(\"name\")\n",
    "                stats['concepts_total'] += 1\n",
    "                \n",
    "                if not concept_name:\n",
    "                    stats['concepts_no_name'] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Standardize concept name for matching\n",
    "                normalized_name = normalize_concept_name(concept_name)\n",
    "                if not normalized_name:\n",
    "                    stats['invalid_target_names'] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Find matching triples\n",
    "                if normalized_name in concept_mapping:\n",
    "                    matched_data = concept_mapping[normalized_name]\n",
    "                    \n",
    "                    # Merge all matching triples\n",
    "                    all_triples = []\n",
    "                    for data in matched_data:\n",
    "                        safe_extend_triples(all_triples, data)\n",
    "                    \n",
    "                    # Add to embeddings concept\n",
    "                    concept[\"triples\"] = all_triples\n",
    "                    \n",
    "                    # Add matching metadata\n",
    "                    source_modules = set()\n",
    "                    original_names = set()\n",
    "                    for data in matched_data:\n",
    "                        source_modules.add(data.get(\"source_module\", \"\"))\n",
    "                        original_names.add(data.get(\"original_name\", \"\"))\n",
    "                    \n",
    "                    concept[\"triples_metadata\"] = {\n",
    "                        \"match_type\": \"exact_name\",\n",
    "                        \"matched_count\": len(matched_data),\n",
    "                        \"source_modules\": list(source_modules),\n",
    "                        \"original_names\": list(original_names),\n",
    "                        \"cleaned_triples_count\": len(all_triples)\n",
    "                    }\n",
    "                    \n",
    "                    stats['concepts_matched'] += 1\n",
    "                    stats['triples_added'] += len(all_triples)\n",
    "                    \n",
    "                    match_details.append({\n",
    "                        'target_module': module_name,\n",
    "                        'target_concept': concept_name,\n",
    "                        'normalized_name': normalized_name,\n",
    "                        'matched_count': len(matched_data),\n",
    "                        'triples_count': len(all_triples),\n",
    "                        'match_type': 'exact_name'\n",
    "                    })\n",
    "                    \n",
    "                else:\n",
    "                    # No matching triples found\n",
    "                    concept[\"triples\"] = []\n",
    "                    concept[\"triples_metadata\"] = {\n",
    "                        \"match_type\": \"no_match\",\n",
    "                        \"matched_count\": 0,\n",
    "                        \"cleaned_triples_count\": 0\n",
    "                    }\n",
    "                    stats['concepts_not_matched'] += 1\n",
    "                    \n",
    "                    match_details.append({\n",
    "                        'target_module': module_name,\n",
    "                        'target_concept': concept_name,\n",
    "                        'normalized_name': normalized_name,\n",
    "                        'matched_count': 0,\n",
    "                        'triples_count': 0,\n",
    "                        'match_type': 'no_match'\n",
    "                    })\n",
    "    \n",
    "    return embeddings_data, stats, match_details\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    embeddings_file = \"../embedding_results/merged_with_embeddings_sentence-t5-large_cleaned.json\"\n",
    "    triples_file = \"./informal_data_with_triples/merged_with_triples.json\"\n",
    "    output_file = \"test.json\"\n",
    "    \n",
    "    try:\n",
    "        print(\"Loading data...\")\n",
    "        embeddings_data, triples_data = load_and_index_data(embeddings_file, triples_file)\n",
    "        \n",
    "        print(\"Debugging triples data structure...\")\n",
    "        debug_triples_structure(triples_data)\n",
    "        \n",
    "        print(\"Building concept mapping and cleaning triples...\")\n",
    "        concept_mapping = build_concept_mapping(triples_data)\n",
    "        \n",
    "        if not concept_mapping:\n",
    "            print(\"Warning: Concept mapping is empty, please check triples file structure\")\n",
    "            return\n",
    "        \n",
    "        print(\"Merging data by concept name...\")\n",
    "        merged_data, stats, match_details = merge_datasets_by_concept_name(embeddings_data, concept_mapping)\n",
    "        \n",
    "        # Save results\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(merged_data, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\n=== Merge Statistics ===\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        \n",
    "        # Save match details\n",
    "        with open(\"merge_match_details_cleaned.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(match_details, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nMerge completed! Results saved to: {output_file}\")\n",
    "        \n",
    "        # Show cleaning effect\n",
    "        print(\"\\n=== Cleaning Effect ===\")\n",
    "        if 'original_triples' in stats and 'cleaned_triples' in stats:\n",
    "            print(f\"Original triples count: {stats['original_triples']}\")\n",
    "            print(f\"Cleaned triples count: {stats['cleaned_triples']}\")\n",
    "            print(f\"Filtered triples count: {stats['filtered_triples']}\")\n",
    "            retention_rate = (stats['cleaned_triples'] / stats['original_triples'] * 100) if stats['original_triples'] > 0 else 0\n",
    "            print(f\"Retention rate: {retention_rate:.1f}%\")\n",
    "        \n",
    "        # Show example results\n",
    "        print(\"\\n=== Example Merge Results ===\")\n",
    "        for module_name, module_content in list(merged_data.items())[:2]:\n",
    "            for definition in module_content.get(\"definitions\", [])[:1]:\n",
    "                for concept in definition.get(\"semantic_analysis\", {}).get(\"concepts\", [])[:2]:\n",
    "                    print(f\"Concept: {concept.get('name')}\")\n",
    "                    print(f\"Triples count: {len(concept.get('triples', []))}\")\n",
    "                    if concept.get('triples'):\n",
    "                        for triple in concept['triples'][:3]:\n",
    "                            print(f\"  - {triple.get('subject')} {triple.get('role')} {triple.get('lemma')}\")\n",
    "                    print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc570e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output_json = \"test.json\"\n",
    "\n",
    "with open(output_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    merged_data = json.load(f)\n",
    "\n",
    "print(\"\\n=== Example triples output ===\")\n",
    "count = 0\n",
    "for module_name, module_content in merged_data.items():\n",
    "    for definition in module_content.get(\"definitions\", []):\n",
    "        semantic = definition.get(\"semantic_analysis\", {})\n",
    "        for concept in semantic.get(\"concepts\", []):\n",
    "            triples = concept.get(\"triples\", [])\n",
    "            if triples:\n",
    "                print(f\"Concept: {concept.get('name')}\")\n",
    "                print(\"Triples:\")\n",
    "                for t in triples:\n",
    "                    print(f\"  - subject: {t['subject']}, role: {t['role']}, lemma: {t['lemma']}\")\n",
    "                print()\n",
    "                count += 1\n",
    "                if count >= 5:\n",
    "                    break\n",
    "        if count >= 5:\n",
    "            break\n",
    "    if count >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e073231c",
   "metadata": {},
   "source": [
    "## 2. Prepare Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef1e51b",
   "metadata": {},
   "source": [
    "### 2.1 Extract Triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def clean_entity(entity):\n",
    "    \"\"\"Clean entity (definiendum or lemma) using the same rules, return cleaned form or empty string\"\"\"\n",
    "    if not entity or not isinstance(entity, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Common mathematical symbols to remove\n",
    "    math_symbols = set(\"+-*/=∑∏√∫<>∈∉{}[]()\")\n",
    "    \n",
    "    # Remove mathematical symbols\n",
    "    entity = ''.join(c for c in entity if c not in math_symbols)\n",
    "    \n",
    "    # Remove non-ASCII characters\n",
    "    entity = ''.join(c for c in entity if ord(c) < 128)\n",
    "    \n",
    "    # Replace spaces with underscores\n",
    "    entity = entity.replace(\" \", \"_\")\n",
    "    \n",
    "    # Keep only valid characters (letters, numbers, underscores)\n",
    "    entity = re.sub(r'[^A-Za-z0-9_]', '', entity)\n",
    "    \n",
    "    # If empty after cleaning, consider invalid\n",
    "    if not entity:\n",
    "        return \"\"\n",
    "    \n",
    "    return entity\n",
    "\n",
    "def extract_lemma(token):\n",
    "    \"\"\"Extract lemma and remove ending punctuation, also clean single letters and 'i.e.'\"\"\"\n",
    "    if not token:\n",
    "        return None\n",
    "    \n",
    "    token = token.lower().strip()\n",
    "    token = token.rstrip('.,;:!?')  # Remove ending punctuation\n",
    "\n",
    "    # Basic lemmatization\n",
    "    if token.endswith('ies'):\n",
    "        token = token[:-3] + 'y'\n",
    "    elif token.endswith('es') and len(token) > 3:\n",
    "        token = token[:-2]\n",
    "    elif token.endswith('s') and len(token) > 3:\n",
    "        token = token[:-1]\n",
    "    elif token.endswith('ing') and len(token) > 4:\n",
    "        token = token[:-3]\n",
    "\n",
    "    # Additional cleaning\n",
    "    if len(token) == 1 or token == 'i.e':\n",
    "        return None\n",
    "    \n",
    "    # Apply the same cleaning rules as definiendum\n",
    "    token = clean_entity(token)\n",
    "    if not token:\n",
    "        return None\n",
    "    \n",
    "    return token\n",
    "\n",
    "def process_json_to_triples(json_file_path, output_file):\n",
    "    \"\"\"\n",
    "    Extract triples from JSON file and save in specified format\n",
    "    \n",
    "    Parameters:\n",
    "    json_file_path: JSON file path\n",
    "    output_file: Output file path\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read JSON file\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    triples_count = 0\n",
    "    extracted_triples = []\n",
    "    \n",
    "    # Iterate through all modules\n",
    "    for module_name, module_content in data.items():\n",
    "        definitions = module_content.get(\"definitions\", [])\n",
    "        \n",
    "        # Iterate through all definitions\n",
    "        for definition in definitions:\n",
    "            semantic_analysis = definition.get(\"semantic_analysis\", {})\n",
    "            concepts = semantic_analysis.get(\"concepts\", [])\n",
    "            \n",
    "            # Iterate through all concepts\n",
    "            for concept in concepts:\n",
    "                concept_name = concept.get(\"name\", \"\")\n",
    "                extracted_triples_list = concept.get(\"extracted_triples\", [])\n",
    "                \n",
    "                # Clean concept name\n",
    "                cleaned_concept = clean_entity(concept_name)\n",
    "                if not cleaned_concept:\n",
    "                    continue\n",
    "                \n",
    "                # Process each triple\n",
    "                for triple in extracted_triples_list:\n",
    "                    subject = triple.get(\"subject\", \"\")\n",
    "                    role = triple.get(\"role\", \"\")\n",
    "                    lemma = triple.get(\"lemma\", \"\")\n",
    "                    \n",
    "                    # Use cleaned concept name as subject\n",
    "                    subject_clean = cleaned_concept.lower()\n",
    "                    \n",
    "                    # Clean lemma using the same rules as definiendum\n",
    "                    lemma_clean = extract_lemma(lemma)\n",
    "                    if not lemma_clean:\n",
    "                        continue\n",
    "                    \n",
    "                    # Format role (ensure uppercase)\n",
    "                    role_clean = role.upper().replace(\"-\", \"_\")\n",
    "                    \n",
    "                    # Add to triple list\n",
    "                    triple_line = f\"{subject_clean}\\t{role_clean}\\t{lemma_clean}\"\n",
    "                    extracted_triples.append(triple_line)\n",
    "                    triples_count += 1\n",
    "    \n",
    "    # Write to output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for triple in extracted_triples:\n",
    "            f.write(triple + '\\n')\n",
    "    \n",
    "    print(f\"Extracted {triples_count} triples from {json_file_path}\")\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "    \n",
    "    return extracted_triples\n",
    "\n",
    "def process_json_folder(json_folder_path, output_file):\n",
    "    \"\"\"\n",
    "    Process all JSON files in a folder\n",
    "    \n",
    "    Parameters:\n",
    "    json_folder_path: JSON folder path\n",
    "    output_file: Output file path\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get all JSON files\n",
    "    json_files = [f for f in os.listdir(json_folder_path) if f.endswith('.json')]\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in folder {json_folder_path}\")\n",
    "        return\n",
    "    \n",
    "    all_triples = []\n",
    "    total_triples = 0\n",
    "    \n",
    "    # Clear output file\n",
    "    open(output_file, 'w', encoding='utf-8').close()\n",
    "    \n",
    "    # Process each JSON file\n",
    "    for json_file in json_files:\n",
    "        json_path = os.path.join(json_folder_path, json_file)\n",
    "        try:\n",
    "            triples = process_json_to_triples(json_path, output_file)\n",
    "            all_triples.extend(triples)\n",
    "            total_triples += len(triples)\n",
    "            \n",
    "            # Append to file\n",
    "            with open(output_file, 'a', encoding='utf-8') as f:\n",
    "                for triple in triples:\n",
    "                    f.write(triple + '\\n')\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {json_file}: {e}\")\n",
    "    \n",
    "    print(f\"\\nExtracted {total_triples} triples from {len(json_files)} JSON files\")\n",
    "    print(f\"All results saved to {output_file}\")\n",
    "    \n",
    "    # Statistics\n",
    "    print_statistics(all_triples)\n",
    "    \n",
    "    return all_triples\n",
    "\n",
    "def print_statistics(triples):\n",
    "    \"\"\"Print statistics\"\"\"\n",
    "    if not triples:\n",
    "        return\n",
    "    \n",
    "    # Count relation types\n",
    "    relation_counts = defaultdict(int)\n",
    "    concept_counts = defaultdict(int)\n",
    "    lemma_counts = defaultdict(int)\n",
    "    \n",
    "    for triple in triples:\n",
    "        parts = triple.split('\\t')\n",
    "        if len(parts) == 3:\n",
    "            concept, relation, lemma = parts\n",
    "            relation_counts[relation] += 1\n",
    "            concept_counts[concept] += 1\n",
    "            lemma_counts[lemma] += 1\n",
    "    \n",
    "    print(\"\\n=== Statistics ===\")\n",
    "    print(f\"Total triples: {len(triples)}\")\n",
    "    print(f\"Unique concepts: {len(concept_counts)}\")\n",
    "    print(f\"Unique lemmas: {len(lemma_counts)}\")\n",
    "    print(f\"Unique relation types: {len(relation_counts)}\")\n",
    "    \n",
    "    print(\"\\nRelation type distribution:\")\n",
    "    for relation, count in sorted(relation_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {relation}: {count}\")\n",
    "    \n",
    "    print(\"\\nTop 10 most frequent concepts:\")\n",
    "    for concept, count in sorted(concept_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"  {concept}: {count}\")\n",
    "    \n",
    "    print(\"\\nTop 10 most frequent lemmas:\")\n",
    "    for lemma, count in sorted(lemma_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"  {lemma}: {count}\")\n",
    "\n",
    "def validate_triples(input_file):\n",
    "    \"\"\"\n",
    "    Validate the format of the generated triples file\n",
    "    \"\"\"\n",
    "    valid_count = 0\n",
    "    invalid_count = 0\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) != 3:\n",
    "                print(f\"Invalid format at line {line_num}: {line}\")\n",
    "                invalid_count += 1\n",
    "                continue\n",
    "            \n",
    "            concept, relation, lemma = parts\n",
    "            # Check if all fields are non-empty and contain only valid characters\n",
    "            if (concept and relation and lemma and \n",
    "                re.match(r'^[a-z0-9_]+$', concept) and \n",
    "                re.match(r'^[A-Z_]+$', relation) and \n",
    "                re.match(r'^[a-z0-9_]+$', lemma)):\n",
    "                valid_count += 1\n",
    "            else:\n",
    "                print(f\"Invalid content at line {line_num}: {line}\")\n",
    "                invalid_count += 1\n",
    "    \n",
    "    print(f\"\\nValidation results for {input_file}:\")\n",
    "    print(f\"Valid triples: {valid_count}\")\n",
    "    print(f\"Invalid triples: {invalid_count}\")\n",
    "    print(f\"Total lines: {valid_count + invalid_count}\")\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Process single JSON file\n",
    "    # json_file_path = \"path/to/your/file.json\"\n",
    "    # output_file = \"my_dataset.txt\"\n",
    "    # process_json_to_triples(json_file_path, output_file)\n",
    "    \n",
    "\n",
    "    # Process entire folder of JSON files\n",
    "    json_folder_path = \"./informal_data_with_triples\"  # Replace with your JSON folder path\n",
    "    output_file = \"../multi_relational_hyperbolic_embeddings/data/my_dataset_cleaned/my_dataset.txt\"\n",
    "\n",
    "    # Process all JSON files\n",
    "    all_triples = process_json_folder(json_folder_path, output_file)\n",
    "    \n",
    "    # Validate the output file\n",
    "    validate_triples(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126928d2",
   "metadata": {},
   "source": [
    "## 2.2 Cleaning the data for Multi-Relation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef5e0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Basic Cleaning with Mathematical Function Preservation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chouyinghan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "# Download stopwords (required for the first run)\n",
    "download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define regex for valid mathematical concepts: only letters, numbers, underscores allowed, with a minimum length of 2\n",
    "valid_definiendum_pattern = re.compile(r\"^[A-Za-z0-9_]{2,}$\")\n",
    "\n",
    "# Common mathematical symbols;剔除 if present\n",
    "math_symbols = set(\"+-*/=∑∏√∫<>∈∉{}[]()\")\n",
    "\n",
    "def clean_definiendum(definiendum):\n",
    "    \"\"\"Clean the definiendum, return a valid form or an empty string\"\"\"\n",
    "    if not definiendum or not isinstance(definiendum, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove mathematical symbols\n",
    "    definiendum = ''.join(c for c in definiendum if c not in math_symbols)\n",
    "    \n",
    "    # Remove non-ASCII characters\n",
    "    definiendum = ''.join(c for c in definiendum if ord(c) < 128)\n",
    "    \n",
    "    # Replace spaces with underscores\n",
    "    definiendum = definiendum.replace(\" \", \"_\")\n",
    "    \n",
    "    # Keep only valid characters (letters, numbers, underscores)\n",
    "    definiendum = re.sub(r'[^A-Za-z0-9_]', '', definiendum)\n",
    "    \n",
    "    # If empty after cleaning, consider it invalid\n",
    "    if not definiendum:\n",
    "        return \"\"\n",
    "    \n",
    "    return definiendum\n",
    "\n",
    "def extract_lemma(token):\n",
    "    \"\"\"Extract lemma and remove trailing punctuation, also clean single letters and 'i.e.'\"\"\"\n",
    "    token = token.lower().strip()\n",
    "    token = token.rstrip('.,;:!?')  # Remove trailing punctuation\n",
    "\n",
    "    # Basic lemmatization\n",
    "    if token.endswith('ies'):\n",
    "        token = token[:-3] + 'y'\n",
    "    elif token.endswith('es') and len(token) > 3:\n",
    "        token = token[:-2]\n",
    "    elif token.endswith('s') and len(token) > 3:\n",
    "        token = token[:-1]\n",
    "    elif token.endswith('ing') and len(token) > 4:\n",
    "        token = token[:-3]\n",
    "\n",
    "    # Additional cleaning\n",
    "    if len(token) == 1 or token == 'i.e':\n",
    "        return None  # Return None to discard\n",
    "    return token\n",
    "\n",
    "\n",
    "def clean_triples_file(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Clean triple file directly and show what was removed\n",
    "    Format: concept\\trelation\\tlemma\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read all triples\n",
    "    triples = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                concept, relation, lemma = line.split('\\t')\n",
    "                triples.append((concept, relation, lemma))\n",
    "            except ValueError:\n",
    "                print(f\"Skip malformed line: {line}\")\n",
    "                continue\n",
    "    \n",
    "    # Track removed data\n",
    "    removed_data = {\n",
    "        'too_short': [],\n",
    "        'stopwords': [],\n",
    "        'special_chars_removed': [],\n",
    "        'invalid_definiendum': [],\n",
    "        'malformed_lines': 0\n",
    "    }\n",
    "    \n",
    "    # Cleaning rules\n",
    "    def clean_lemma(lemma, original_lemma):\n",
    "        \"\"\"Clean lemma and track changes\"\"\"\n",
    "        if not lemma:\n",
    "            return None\n",
    "        \n",
    "        cleaned_lemma = extract_lemma(lemma)\n",
    "        if cleaned_lemma is None:\n",
    "            removed_data['too_short'].append((original_lemma, lemma))\n",
    "            return None\n",
    "        \n",
    "        if cleaned_lemma in stop_words:\n",
    "            removed_data['stopwords'].append((original_lemma, cleaned_lemma))\n",
    "            return None\n",
    "        \n",
    "        return cleaned_lemma\n",
    "    \n",
    "    def clean_concept(concept):\n",
    "        \"\"\"Clean concept using the same rules as clean_definiendum\"\"\"\n",
    "        cleaned_concept = clean_definiendum(concept)\n",
    "        if not cleaned_concept:\n",
    "            removed_data['invalid_definiendum'].append(concept)\n",
    "        return cleaned_concept\n",
    "    \n",
    "    # Apply cleaning\n",
    "    cleaned_triples = []\n",
    "    removed_triples = []\n",
    "    \n",
    "    for concept, relation, lemma in triples:\n",
    "        # Clean concept\n",
    "        cleaned_concept = clean_concept(concept)\n",
    "        if not cleaned_concept:\n",
    "            removed_triples.append((concept, relation, lemma))\n",
    "            continue\n",
    "        \n",
    "        # Clean lemma\n",
    "        cleaned_lemma = clean_lemma(lemma, lemma)\n",
    "        if not cleaned_lemma:\n",
    "            removed_triples.append((concept, relation, lemma))\n",
    "            continue\n",
    "        \n",
    "        # 关系标签格式化（与原始代码一致）\n",
    "        # 合并 B-/I- 前缀，转为全大写\n",
    "        if \"-\" in relation:\n",
    "            relation = relation.split(\"-\", 1)[1].replace(\"-\", \"\").upper()\n",
    "        else:\n",
    "            relation = relation.upper()\n",
    "        \n",
    "        cleaned_triples.append((cleaned_concept.lower(), relation, cleaned_lemma))\n",
    "    \n",
    "    # Write cleaned file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for concept, relation, lemma in cleaned_triples:\n",
    "            f.write(f\"{concept}\\t{relation}\\t{lemma}\\n\")\n",
    "    \n",
    "    # Statistics and removal report\n",
    "    print(f\"Original triples count: {len(triples)}\")\n",
    "    print(f\"Cleaned triples count: {len(cleaned_triples)}\")\n",
    "    print(f\"Removed triples count: {len(removed_triples)}\")\n",
    "    print(f\"Retention rate: {len(cleaned_triples)/len(triples)*100:.1f}%\")\n",
    "    \n",
    "    # Detailed removal report\n",
    "    print(\"\\n=== REMOVAL REPORT ===\")\n",
    "    \n",
    "    # Removed triples\n",
    "    if removed_triples:\n",
    "        print(f\"\\nRemoved triples ({len(removed_triples)}):\")\n",
    "        for i, (concept, relation, lemma) in enumerate(removed_triples[:10]):\n",
    "            print(f\"  {concept}\\t{relation}\\t{lemma}\")\n",
    "        if len(removed_triples) > 10:\n",
    "            print(f\"  ... and {len(removed_triples) - 10} more\")\n",
    "    \n",
    "    # Invalid definienda\n",
    "    if removed_data['invalid_definiendum']:\n",
    "        print(f\"\\nInvalid definienda removed ({len(removed_data['invalid_definiendum'])}):\")\n",
    "        unique_invalid = set(removed_data['invalid_definiendum'])\n",
    "        for concept in sorted(unique_invalid)[:10]:\n",
    "            print(f\"  '{concept}'\")\n",
    "        if len(unique_invalid) > 10:\n",
    "            print(f\"  ... and {len(unique_invalid) - 10} more\")\n",
    "    \n",
    "    # Too short lemmas\n",
    "    if removed_data['too_short']:\n",
    "        print(f\"\\nToo short lemmas removed ({len(removed_data['too_short'])}):\")\n",
    "        unique_short = set(lemma for _, lemma in removed_data['too_short'])\n",
    "        for lemma in sorted(unique_short)[:10]:\n",
    "            print(f\"  '{lemma}'\")\n",
    "        if len(unique_short) > 10:\n",
    "            print(f\"  ... and {len(unique_short) - 10} more\")\n",
    "    \n",
    "    # Stopwords\n",
    "    if removed_data['stopwords']:\n",
    "        print(f\"\\nStopwords removed ({len(removed_data['stopwords'])}):\")\n",
    "        unique_stopwords = set(lemma for _, lemma in removed_data['stopwords'])\n",
    "        for lemma in sorted(unique_stopwords):\n",
    "            print(f\"  '{lemma}'\")\n",
    "    \n",
    "    # Summary by removal reason\n",
    "    print(f\"\\n=== SUMMARY BY REMOVAL REASON ===\")\n",
    "    print(f\"Total removed: {len(removed_triples)}\")\n",
    "    print(f\"- Invalid definienda: {len(removed_data['invalid_definiendum'])}\")\n",
    "    print(f\"- Too short lemmas: {len(removed_data['too_short'])}\")\n",
    "    print(f\"- Stopwords: {len(removed_data['stopwords'])}\")\n",
    "    \n",
    "    return removed_triples, removed_data\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"../multi_relational_hyperbolic_embeddings/data/my_dataset_cleaned/my_dataset.txt\"\n",
    "    cleaned_file = \"../multi_relational_hyperbolic_embeddings/data/my_dataset_cleaned/train.txt\"\n",
    "\n",
    "    # Basic cleaning with preservation of mathematical functions\n",
    "    print(\"=== Basic Cleaning with Mathematical Function Preservation ===\")\n",
    "    removed_triples, removal_data = clean_triples_file(input_file, cleaned_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
