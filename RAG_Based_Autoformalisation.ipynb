{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a607737",
   "metadata": {},
   "source": [
    "# Autoformalisation Based on Large Language Models via Retrieval-Augmented Generation\n",
    "\n",
    "This notebook demonstrates the process of autoformalisation based on large language models (LLMs) combined with retrieval-augmented generation (RAG) on Mathlib4. The objective is to explore how unstructured natural language input can be transformed into structured, formalised Lean4 representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af815c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "import os\n",
    "import torch\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34910e4",
   "metadata": {},
   "source": [
    "## Preparation: Initialize OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\"  # Replace with your actual OpenAI API key\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b86116f",
   "metadata": {},
   "source": [
    "## 1.  Text Embedding Model Retrieval\n",
    "### 1.1.  Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a8551a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the currently selected model\n",
    "SELECTED_MODEL_KEY = \"sentence-t5-large\"  # Available model\n",
    "\n",
    "# Dictionary of supported models and their corresponding identifiers\n",
    "MODELS = {\n",
    "    \"Glove\": \"glove-wiki-gigaword-100\",\n",
    "    \"Word2Vec\": \"word2vec-google-news-300\",\n",
    "    \"bert-base\": \"bert-base-uncased\",\n",
    "    \"bert-large\": \"bert-large-uncased\",\n",
    "    \"defsent-bert\": \"princeton-nlp/defsent-bert\",\n",
    "    \"defsent-roberta\": \"princeton-nlp/defsent-roberta\",\n",
    "    \"distilroberta-vl\": \"sentence-transformers/distilroberta-base-v1\",\n",
    "    \"mpnet-base-v2\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"sentence-t5-large\": \"sentence-transformers/sentence-t5-large\",\n",
    "    \"HIT\": \"HIT/some-hit-model\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2917992c",
   "metadata": {},
   "source": [
    "### 1.2 Load Embedding Models and Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b6ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Load Embedding Models\n",
    "# -------------------------------\n",
    "def load_embedding_model(model_key):\n",
    "    if model_key not in MODELS:\n",
    "        raise ValueError(f\"Unknown model key: {model_key}\")\n",
    "    \n",
    "    model_name = MODELS[model_key]\n",
    "    \n",
    "    if model_key in [\"Word2Vec\", \"Glove\"]:\n",
    "        class WordVectorWrapper:\n",
    "            def __init__(self):\n",
    "                print(f\"Loading word vector model: {model_name}\")\n",
    "                self.wv = api.load(model_name)\n",
    "                self.dim = self.wv.vector_size\n",
    "\n",
    "            def encode(self, texts):\n",
    "                if isinstance(texts, str):\n",
    "                    texts = [texts]\n",
    "                embeddings = []\n",
    "                for text in texts:\n",
    "                    words = text.lower().split()\n",
    "                    word_vectors = [self.wv[word] for word in words if word in self.wv]\n",
    "                    if word_vectors:\n",
    "                        embeddings.append(np.mean(word_vectors, axis=0))\n",
    "                    else:\n",
    "                        embeddings.append(np.zeros(self.dim))\n",
    "                return np.array(embeddings)\n",
    "        return WordVectorWrapper()\n",
    "\n",
    "    elif model_key.startswith(\"HIT\"):\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        class HierarchyTransformerWrapper:\n",
    "            def __init__(self):\n",
    "                print(f\"Loading HIT model: {model_name}\")\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                self.model = AutoModel.from_pretrained(model_name)\n",
    "                self.model.eval()\n",
    "            def encode(self, texts):\n",
    "                if isinstance(texts, str):\n",
    "                    texts = [texts]\n",
    "                inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(**inputs)\n",
    "                return outputs.last_hidden_state[:,0,:].numpy()\n",
    "        return HierarchyTransformerWrapper()\n",
    "\n",
    "    elif model_key in [\"bert-base\", \"bert-large\", \"defsent-bert\", \"defsent-roberta\"]:\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        class BertWrapper:\n",
    "            def __init__(self):\n",
    "                print(f\"Loading BERT model: {model_name}\")\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                self.model = AutoModel.from_pretrained(model_name)\n",
    "                self.model.eval()\n",
    "            def encode(self, texts):\n",
    "                if isinstance(texts, str):\n",
    "                    texts = [texts]\n",
    "                embeddings = []\n",
    "                for text in texts:\n",
    "                    inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.model(**inputs)\n",
    "                    last_hidden = outputs.last_hidden_state\n",
    "                    attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "                    masked_hidden = last_hidden * attention_mask\n",
    "                    pooled = masked_hidden.sum(dim=1) / attention_mask.sum(dim=1)\n",
    "                    embeddings.append(pooled.squeeze().numpy())\n",
    "                return np.array(embeddings)\n",
    "        return BertWrapper()\n",
    "\n",
    "    else:\n",
    "        print(f\"Loading Sentence Transformer model: {model_name}\")\n",
    "        return SentenceTransformer(model_name)\n",
    "\n",
    "# -------------------------------\n",
    "# Initialize Embedding Model\n",
    "# -------------------------------\n",
    "try:\n",
    "    embedding_model = load_embedding_model(SELECTED_MODEL_KEY)\n",
    "except Exception as e:\n",
    "    print(f\"Model loading failed: {e}\")\n",
    "    SELECTED_MODEL_KEY = \"sentence-t5-large\"\n",
    "    print(f\"Fallback to default model: {SELECTED_MODEL_KEY}\")\n",
    "    embedding_model = load_embedding_model(SELECTED_MODEL_KEY)\n",
    "\n",
    "# -------------------------------\n",
    "# Load JSON File and Extract Embeddings\n",
    "# -------------------------------\n",
    "json_path = \"./Informalisation_and_Mathematical_DSRL/merged_with_embeddings_and_triples.json\"\n",
    "if not os.path.exists(json_path):\n",
    "    raise FileNotFoundError(f\"Embedding file not found: {json_path}\")\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    concept_data_raw = json.load(f)\n",
    "\n",
    "flattened_concepts = []\n",
    "embeddings = []\n",
    "\n",
    "# Iterate over JSON entries and extract embedding vectors\n",
    "for module_content in concept_data_raw.values():\n",
    "    for definition in module_content.get(\"definitions\", []):\n",
    "        for concept in definition.get(\"semantic_analysis\", {}).get(\"concepts\", []):\n",
    "            vec = concept.get(\"embedding_vector\")\n",
    "            if vec is not None:\n",
    "                embeddings.append(vec)\n",
    "                flattened_concepts.append(concept)\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "print(f\"Extracted {len(embeddings)} concept embedding vectors\")\n",
    "\n",
    "# -------------------------------\n",
    "# Search for Similar Concepts\n",
    "# -------------------------------\n",
    "def search_similar_concepts(query, top_k=5):\n",
    "    query_vec = embedding_model.encode(query)\n",
    "    if isinstance(query_vec, list):\n",
    "        query_vec = np.array(query_vec)\n",
    "    if query_vec.ndim > 1:\n",
    "        query_vec = query_vec[0]\n",
    "    query_vec = query_vec / np.linalg.norm(query_vec)\n",
    "    norm_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    cosine_scores = np.dot(norm_embeddings, query_vec)\n",
    "    top_indices = np.argsort(cosine_scores)[-top_k:][::-1]\n",
    "    results = []\n",
    "    for i in top_indices:\n",
    "        concept = flattened_concepts[i]\n",
    "        results.append(concept)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd343909",
   "metadata": {},
   "source": [
    "### 1.3  Retrieval of Relevant Mathlib4 Module Imports based on Natural Language Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57522839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Avoid parallelism warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "class SemanticSearch:\n",
    "    def __init__(self, json_path, model_name=\"sentence-transformers/sentence-t5-large\"):\n",
    "        self.json_path = json_path\n",
    "        self.model_name = model_name\n",
    "        self.embedding_model = None\n",
    "        self.embeddings = None\n",
    "        self.flattened_concepts = []\n",
    "        \n",
    "        self.load_data()\n",
    "        self.load_model()\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and extract embeddings from JSON file\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        \n",
    "        if not os.path.exists(self.json_path):\n",
    "            raise FileNotFoundError(f\"File not found: {self.json_path}\")\n",
    "        \n",
    "        with open(self.json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            concept_data_raw = json.load(f)\n",
    "        \n",
    "        embeddings_list = []\n",
    "        \n",
    "        for module_name, module_content in concept_data_raw.items():\n",
    "            definitions = module_content.get(\"definitions\", [])\n",
    "            \n",
    "            for definition in definitions:\n",
    "                semantic_analysis = definition.get(\"semantic_analysis\", {})\n",
    "                concepts = semantic_analysis.get(\"concepts\", [])\n",
    "                \n",
    "                for concept in concepts:\n",
    "                    vec = concept.get(\"embedding_vector\")\n",
    "                    if vec is not None:\n",
    "                        embeddings_list.append(vec)\n",
    "                        # Ensure concept has required fields with fallbacks\n",
    "                        concept_copy = concept.copy()\n",
    "                        if \"name\" not in concept_copy:\n",
    "                            concept_copy[\"name\"] = concept_copy.get(\"concept_name\", \"Unknown\")\n",
    "                        if \"module_path\" not in concept_copy:\n",
    "                            concept_copy[\"module_path\"] = module_name\n",
    "                        \n",
    "                        self.flattened_concepts.append(concept_copy)\n",
    "        \n",
    "        if embeddings_list:\n",
    "            self.embeddings = np.array(embeddings_list)\n",
    "            print(f\"Loaded {len(self.embeddings)} concept embeddings\")\n",
    "            print(f\"Embedding dimension: {self.embeddings.shape[1]}\")\n",
    "        else:\n",
    "            raise ValueError(\"No valid embeddings found in the JSON file\")\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the sentence transformer model\"\"\"\n",
    "        print(f\"Loading model: {self.model_name}\")\n",
    "        try:\n",
    "            self.embedding_model = SentenceTransformer(self.model_name)\n",
    "            print(\"Model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load model: {e}\")\n",
    "            print(\"Falling back to default model...\")\n",
    "            self.embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    \n",
    "    def search_similar_concepts(self, query, top_k=5, min_score=0.0):\n",
    "        \"\"\"\n",
    "        Search for similar concepts using cosine similarity\n",
    "        \n",
    "        Args:\n",
    "            query (str): Search query\n",
    "            top_k (int): Number of top results to return\n",
    "            min_score (float): Minimum similarity score threshold\n",
    "        \n",
    "        Returns:\n",
    "            list: List of similar concepts with similarity scores\n",
    "        \"\"\"\n",
    "        if self.embeddings is None or len(self.embeddings) == 0:\n",
    "            print(\"No embeddings available\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Searching for: '{query}'\")\n",
    "        \n",
    "        try:\n",
    "            # Generate query embedding\n",
    "            query_vec = self.embedding_model.encode([query])\n",
    "            if isinstance(query_vec, list):\n",
    "                query_vec = np.array(query_vec)\n",
    "            if query_vec.ndim > 1:\n",
    "                query_vec = query_vec[0]\n",
    "            \n",
    "            print(f\"Query embedding shape: {query_vec.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to encode query: {e}\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Normalize vectors\n",
    "            query_vec = query_vec / (np.linalg.norm(query_vec) + 1e-8)\n",
    "            norm_embeddings = self.embeddings / (np.linalg.norm(self.embeddings, axis=1, keepdims=True) + 1e-8)\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            cosine_scores = np.dot(norm_embeddings, query_vec)\n",
    "            \n",
    "            # Get top_k indices\n",
    "            top_indices = np.argsort(cosine_scores)[-top_k:][::-1]\n",
    "            \n",
    "            print(f\"Top {min(top_k, len(top_indices))} similarity scores: {cosine_scores[top_indices]}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to calculate similarity: {e}\")\n",
    "            return []\n",
    "        \n",
    "        # Collect results\n",
    "        results = []\n",
    "        for i in top_indices:\n",
    "            if i < len(self.flattened_concepts):\n",
    "                concept = self.flattened_concepts[i]\n",
    "                score = cosine_scores[i]\n",
    "                \n",
    "                # Apply minimum score threshold\n",
    "                if score >= min_score:\n",
    "                    results.append({\n",
    "                        'concept': concept,\n",
    "                        'similarity_score': float(score),\n",
    "                        'index': int(i)\n",
    "                    })\n",
    "        \n",
    "        print(f\"Found {len(results)} results above threshold {min_score}\")\n",
    "        return results\n",
    "    \n",
    "    def print_results(self, results, show_details=False):\n",
    "        \"\"\"Pretty print search results\"\"\"\n",
    "        if not results:\n",
    "            print(\"No results found\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nSearch Results ({len(results)} found):\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        seen = set()\n",
    "        unique_results = []\n",
    "        \n",
    "        for result in results:\n",
    "            concept = result['concept']\n",
    "            score = result['similarity_score']\n",
    "            \n",
    "            name = concept.get('name', 'Unknown')\n",
    "            module = concept.get('module_path', 'Unknown')\n",
    "            \n",
    "            # Deduplicate by name and module\n",
    "            key = (name, module)\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique_results.append((name, module, score, concept))\n",
    "        \n",
    "        for i, (name, module, score, concept) in enumerate(unique_results, 1):\n",
    "            print(f\"{i}. Name: {name}\")\n",
    "            print(f\"   Module: {module}\")\n",
    "            print(f\"   Similarity: {score:.4f}\")\n",
    "            \n",
    "            if show_details:\n",
    "                # Show additional fields if available\n",
    "                for key, value in concept.items():\n",
    "                    if key not in ['name', 'module_path', 'embedding_vector'] and value:\n",
    "                        print(f\"   {key}: {str(value)[:100]}...\")\n",
    "            print()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Usage Example\n",
    "# -------------------------------\n",
    "def main():\n",
    "    json_path = \"./Informalisation_and_Mathematical_DSRL/merged_with_embeddings_and_triples.json\"\n",
    "\n",
    "    try:\n",
    "        # Initialize search system\n",
    "        search_system = SemanticSearch(json_path)\n",
    "        \n",
    "        # Example queries\n",
    "        queries = [\n",
    "            \"For any real matrix A: Matrix m × n, if the columns of A are pairwise orthogonal, then the matrix Aᵀ * A is a diagonal matrix.\",\n",
    "            # \"orthogonal matrix\",\n",
    "            # \"diagonal matrix\",\n",
    "            # \"matrix transpose\"\n",
    "        ]\n",
    "        \n",
    "        for query in queries:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            results = search_system.search_similar_concepts(\n",
    "                query, \n",
    "                top_k=10, \n",
    "                min_score=0.1  # Only show results with similarity > 0.1\n",
    "            )\n",
    "            search_system.print_results(results, show_details=False)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc78bf16",
   "metadata": {},
   "source": [
    "## 2. Multi-Relational Hyperbolic (Poincaré) Embeddings\n",
    "\n",
    "This section introduces multi-relational hyperbolic embeddings in the Poincaré ball model, which enable structured representation and semantic querying of mathematical knowledge. By integrating textual definitions with hierarchical relationships, hyperbolic space naturally captures tree-like structures and accommodates a large number of concepts and relations. Its geometric properties, including negative curvature and exponentially expanding volume, facilitate efficient management of complex mathematical knowledge, supporting enhanced reasoning and autoformalisation within LLM-based systems.\n",
    "\n",
    "### 2.1 Hybrid Moudle Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8456f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from web.embeddings import load_embedding\n",
    "from web.evaluate import poincare_distance\n",
    "from scipy.spatial.distance import euclidean\n",
    "from collections import defaultdict\n",
    "\n",
    "# Avoid parallelism warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Regular expression for valid mathematical concepts: only letters, numbers, underscores, minimum length 2\n",
    "valid_definiendum_pattern = re.compile(r\"^[a-z0-9_]{2,}$\")\n",
    "math_symbols = set(\"+-*/=∑∏√∫<>∈∉{}[]()\")\n",
    "\n",
    "def clean_definiendum(definiendum):\n",
    "    \"\"\"Clean the definiendum, return a valid form or an empty string\"\"\"\n",
    "    if not definiendum or not isinstance(definiendum, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # First, replace dots with underscores (correcting dot handling)\n",
    "    definiendum = definiendum.replace(\".\", \"_\")\n",
    "    \n",
    "    # Remove mathematical symbols\n",
    "    definiendum = ''.join(c for c in definiendum if c not in math_symbols)\n",
    "    \n",
    "    # Remove non-ASCII characters\n",
    "    definiendum = ''.join(c for c in definiendum if ord(c) < 128)\n",
    "    \n",
    "    # Replace spaces with underscores\n",
    "    definiendum = definiendum.replace(\" \", \"_\")\n",
    "    \n",
    "    # Keep only valid characters (letters, numbers, underscores)\n",
    "    definiendum = re.sub(r'[^A-Za-z0-9_]', '', definiendum)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    definiendum = definiendum.lower()\n",
    "    \n",
    "    # If empty after cleaning, consider invalid\n",
    "    if not definiendum:\n",
    "        return \"\"\n",
    "    \n",
    "    return definiendum\n",
    "\n",
    "def normalize_entity_name(name):\n",
    "    \"\"\"Normalize entity name for matching raw data\"\"\"\n",
    "    return \"_\".join(name.lower().strip().split())\n",
    "\n",
    "class HybridSemanticSearch:\n",
    "    def __init__(self, json_path, murp_embedding_path, \n",
    "                 semantic_model=\"sentence-transformers/sentence-t5-large\",\n",
    "                 murp_model_type=\"poincare\"):\n",
    "        \"\"\"\n",
    "        Hybrid retrieval system: semantic search first, then MuRP for secondary retrieval\n",
    "        \n",
    "        Args:\n",
    "            json_path: JSON file path (contains concepts and embeddings)\n",
    "            murp_embedding_path: MuRP embedding file path\n",
    "            semantic_model: Sentence Transformer model name\n",
    "            murp_model_type: MuRP model type (\"poincare\" or \"euclidean\")\n",
    "        \"\"\"\n",
    "        self.json_path = json_path\n",
    "        self.murp_embedding_path = murp_embedding_path\n",
    "        self.semantic_model_name = semantic_model\n",
    "        self.murp_model_type = murp_model_type\n",
    "        \n",
    "        # Initialize components\n",
    "        self.semantic_model = None\n",
    "        self.murp_embeddings = None\n",
    "        self.semantic_embeddings = None\n",
    "        self.flattened_concepts = []\n",
    "        self.murp_vocab = []\n",
    "        self.original_dataset = None\n",
    "        \n",
    "        # Load data\n",
    "        self.load_semantic_data()\n",
    "        self.load_semantic_model()\n",
    "        self.load_murp_embeddings()\n",
    "    \n",
    "    def load_semantic_data(self):\n",
    "        \"\"\"Load semantic search data\"\"\"\n",
    "        print(\"Loading semantic search data...\")\n",
    "        \n",
    "        if not os.path.exists(self.json_path):\n",
    "            raise FileNotFoundError(f\"File not found: {self.json_path}\")\n",
    "        \n",
    "        with open(self.json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.original_dataset = json.load(f)\n",
    "        \n",
    "        embeddings_list = []\n",
    "        \n",
    "        for module_name, module_content in self.original_dataset.items():\n",
    "            definitions = module_content.get(\"definitions\", [])\n",
    "            \n",
    "            for definition in definitions:\n",
    "                semantic_analysis = definition.get(\"semantic_analysis\", {})\n",
    "                concepts = semantic_analysis.get(\"concepts\", [])\n",
    "                \n",
    "                for concept in concepts:\n",
    "                    vec = concept.get(\"embedding_vector\")\n",
    "                    if vec is not None:\n",
    "                        embeddings_list.append(vec)\n",
    "                        concept_copy = concept.copy()\n",
    "                        if \"name\" not in concept_copy:\n",
    "                            concept_copy[\"name\"] = concept_copy.get(\"concept_name\", \"Unknown\")\n",
    "                        if \"module_path\" not in concept_copy:\n",
    "                            concept_copy[\"module_path\"] = module_name\n",
    "                        \n",
    "                        self.flattened_concepts.append(concept_copy)\n",
    "        \n",
    "        if embeddings_list:\n",
    "            self.semantic_embeddings = np.array(embeddings_list)\n",
    "            print(f\"Loaded {len(self.semantic_embeddings)} semantic concept embeddings\")\n",
    "        else:\n",
    "            raise ValueError(\"No valid semantic embeddings found\")\n",
    "    \n",
    "    def load_semantic_model(self):\n",
    "        \"\"\"Load Sentence Transformer model\"\"\"\n",
    "        print(f\"Loading semantic model: {self.semantic_model_name}\")\n",
    "        try:\n",
    "            self.semantic_model = SentenceTransformer(self.semantic_model_name)\n",
    "            print(\"Semantic model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load semantic model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_murp_embeddings(self):\n",
    "        \"\"\"Load MuRP embeddings\"\"\"\n",
    "        print(f\"Loading MuRP embeddings from: {self.murp_embedding_path}\")\n",
    "        print(f\"Model type: {self.murp_model_type}\")\n",
    "        \n",
    "        try:\n",
    "            self.murp_embeddings = load_embedding(\n",
    "                self.murp_embedding_path,\n",
    "                format=\"dict\",\n",
    "                normalize=True,\n",
    "                lower=True,\n",
    "                clean_words=False\n",
    "            )\n",
    "            \n",
    "            self.murp_vocab = self.murp_embeddings.words\n",
    "            print(f\"Loaded MuRP embeddings with {len(self.murp_vocab)} vocabulary items\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load MuRP embeddings: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def semantic_search(self, query, top_k=20, min_score=0.0):\n",
    "        \"\"\"Phase 1: Semantic search (with deduplication)\"\"\"\n",
    "        print(f\"Phase 1: Semantic search for: '{query}'\")\n",
    "        \n",
    "        if self.semantic_embeddings is None or len(self.semantic_embeddings) == 0:\n",
    "            print(\"No semantic embeddings available\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Generate query embedding\n",
    "            query_vec = self.semantic_model.encode([query])\n",
    "            if isinstance(query_vec, list):\n",
    "                query_vec = np.array(query_vec)\n",
    "            if query_vec.ndim > 1:\n",
    "                query_vec = query_vec[0]\n",
    "            \n",
    "            # Normalize vectors\n",
    "            query_vec = query_vec / (np.linalg.norm(query_vec) + 1e-8)\n",
    "            norm_embeddings = self.semantic_embeddings / (np.linalg.norm(self.semantic_embeddings, axis=1, keepdims=True) + 1e-8)\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            cosine_scores = np.dot(norm_embeddings, query_vec)\n",
    "            \n",
    "            # Get indices of all results (sorted by similarity descending)\n",
    "            all_indices = np.argsort(cosine_scores)[::-1]\n",
    "            \n",
    "            print(f\"Found {len(all_indices)} potential semantic results\")\n",
    "            print(f\"Similarity range: {cosine_scores[all_indices[-1]]:.4f} to {cosine_scores[all_indices[0]]:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Semantic search failed: {e}\")\n",
    "            return []\n",
    "        \n",
    "        # Collect results (with deduplication)\n",
    "        results = []\n",
    "        seen_concepts = set()\n",
    "        \n",
    "        for i in all_indices:\n",
    "            if len(results) >= top_k:\n",
    "                break\n",
    "                \n",
    "            if i < len(self.flattened_concepts):\n",
    "                concept = self.flattened_concepts[i]\n",
    "                score = cosine_scores[i]\n",
    "                \n",
    "                # Skip scores below threshold\n",
    "                if score < min_score:\n",
    "                    continue\n",
    "                \n",
    "                # Get and clean concept name\n",
    "                concept_name = concept.get('name', '')\n",
    "                cleaned_name = clean_definiendum(concept_name)\n",
    "                \n",
    "                # Check for duplicates\n",
    "                if cleaned_name and cleaned_name not in seen_concepts:\n",
    "                    # 确保返回的是字典格式，包含 'concept' 键\n",
    "                    result_dict = {\n",
    "                        'concept': concept,\n",
    "                        'semantic_score': float(score),\n",
    "                        'index': int(i)\n",
    "                    }\n",
    "                    results.append(result_dict)\n",
    "                    seen_concepts.add(cleaned_name)\n",
    "        \n",
    "        print(f\"{len(results)} unique results above threshold {min_score}\")\n",
    "        return results\n",
    "    \n",
    "    def murp_search(self, concept_names, top_k=10):\n",
    "        \"\"\"Phase 2: MuRP retrieval\"\"\"\n",
    "        print(f\"Phase 2: MuRP search for {len(concept_names)} concepts\")\n",
    "        \n",
    "        if not self.murp_embeddings:\n",
    "            print(\"No MuRP embeddings available\")\n",
    "            return {}\n",
    "        \n",
    "        murp_results = {}\n",
    "        found_concepts = []\n",
    "        missing_concepts = []\n",
    "        \n",
    "        for concept_name in concept_names:\n",
    "            possible_names = [\n",
    "                concept_name,\n",
    "                concept_name.replace('.', '_'),\n",
    "                concept_name.replace('_', '.'),\n",
    "                concept_name.split('.')[-1] if '.' in concept_name else concept_name\n",
    "            ]\n",
    "            \n",
    "            query_name = None\n",
    "            for name in possible_names:\n",
    "                if name in self.murp_embeddings:\n",
    "                    query_name = name\n",
    "                    break\n",
    "            \n",
    "            if query_name is None:\n",
    "                missing_concepts.append(concept_name)\n",
    "                continue\n",
    "            \n",
    "            found_concepts.append((concept_name, query_name))\n",
    "            \n",
    "            try:\n",
    "                neighbors = self.retrieve_murp_neighbors(query_name, top_k)\n",
    "                murp_results[concept_name] = {\n",
    "                    'query_name_used': query_name,\n",
    "                    'neighbors': neighbors\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"MuRP search failed for {concept_name}: {e}\")\n",
    "        \n",
    "        print(f\"Found MuRP results for {len(found_concepts)} concepts\")\n",
    "        if missing_concepts:\n",
    "            print(f\"Missing in MuRP vocab: {len(missing_concepts)} concepts\")\n",
    "        \n",
    "        return murp_results\n",
    "    \n",
    "    def retrieve_murp_neighbors(self, query_word, top_k=10):\n",
    "        \"\"\"MuRP neighbor retrieval\"\"\"\n",
    "        if query_word not in self.murp_embeddings:\n",
    "            raise ValueError(f\"Query word '{query_word}' not found in MuRP vocabulary\")\n",
    "        \n",
    "        query_vec = self.murp_embeddings[query_word]\n",
    "        distances = []\n",
    "        \n",
    "        for word in self.murp_vocab:\n",
    "            vec = self.murp_embeddings[word]\n",
    "            if self.murp_model_type == \"poincare\":\n",
    "                dist = poincare_distance(query_vec, vec)\n",
    "            else:\n",
    "                dist = euclidean(query_vec, vec)\n",
    "            distances.append((word, dist))\n",
    "        \n",
    "        # Sort distances and exclude the query word itself\n",
    "        distances_sorted = sorted(distances, key=lambda x: x[1])\n",
    "        distances_sorted = [item for item in distances_sorted if item[0] != query_word]\n",
    "        \n",
    "        # Take top-k nearest neighbors\n",
    "        top_neighbors = distances_sorted[:top_k]\n",
    "        \n",
    "        # Normalize distances\n",
    "        if top_neighbors:\n",
    "            dists = np.array([dist for _, dist in top_neighbors])\n",
    "            min_dist, max_dist = dists.min(), dists.max()\n",
    "            if max_dist > min_dist:\n",
    "                norm_dists = (dists - min_dist) / (max_dist - min_dist)\n",
    "            else:\n",
    "                norm_dists = np.zeros_like(dists)\n",
    "            \n",
    "            return [(word, dist, norm) for (word, dist), norm in zip(top_neighbors, norm_dists)]\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def hybrid_search(self, query, semantic_top_k=20, murp_top_k=10, semantic_min_score=0.1, return_top_k=10):\n",
    "        \"\"\"Hybrid retrieval: semantic search + MuRP secondary retrieval\"\"\"\n",
    "        print(f\"Starting hybrid search for: '{query}'\")\n",
    "        \n",
    "        # Phase 1: Semantic search (with deduplication)\n",
    "        semantic_results = self.semantic_search(query, semantic_top_k, semantic_min_score)\n",
    "        \n",
    "        if not semantic_results:\n",
    "            print(\"No semantic results found\")\n",
    "            return {\n",
    "                \"semantic_results\": [], \n",
    "                \"murp_results\": {},\n",
    "                \"import_modules\": []\n",
    "            }\n",
    "        \n",
    "        # Extract and clean concept names for MuRP retrieval\n",
    "        raw_concept_names = [result['concept'].get('name', '') for result in semantic_results]\n",
    "        raw_concept_names = [name for name in raw_concept_names if name and name != 'Unknown']\n",
    "        \n",
    "        # Apply cleaning function (includes dot correction)\n",
    "        cleaned_concept_names = []\n",
    "        for name in raw_concept_names:\n",
    "            cleaned = clean_definiendum(name)\n",
    "            if cleaned:\n",
    "                cleaned_concept_names.append(cleaned)\n",
    "        \n",
    "        if not cleaned_concept_names:\n",
    "            print(\"All concept names were invalid after cleaning\")\n",
    "            return {\n",
    "                \"semantic_results\": semantic_results,\n",
    "                \"murp_results\": {},\n",
    "                \"import_modules\": []\n",
    "            }\n",
    "        \n",
    "        print(f\"Using {len(cleaned_concept_names)} cleaned concepts for MuRP search\")\n",
    "        \n",
    "        # Phase 2: MuRP retrieval (using cleaned lowercase names)\n",
    "        murp_results = self.murp_search(cleaned_concept_names, murp_top_k)\n",
    "        \n",
    "        # Create a dictionary to store modules and their relevance scores\n",
    "        module_scores = defaultdict(float)\n",
    "        \n",
    "        # 1. Assign scores to modules from semantic search results\n",
    "        for result in semantic_results:\n",
    "            module = result['concept'].get('module_path', '')\n",
    "            score = result.get('semantic_score', 0)\n",
    "            if module:\n",
    "                module_scores[module] += score  # Accumulate scores\n",
    "        \n",
    "        # 2. Assign scores to modules from MuRP results (based on distance)\n",
    "        for concept_name, murp_data in murp_results.items():\n",
    "            for neighbor, _, norm_dist in murp_data['neighbors']:\n",
    "                # Find the module to which the neighbor belongs\n",
    "                neighbor_module = None\n",
    "                for module_name, module_data in self.original_dataset.items():\n",
    "                    for definition in module_data.get(\"definitions\", []):\n",
    "                        def_name = definition.get(\"name\", \"\")\n",
    "                        if def_name and clean_definiendum(def_name) == neighbor:\n",
    "                            neighbor_module = module_name\n",
    "                            break\n",
    "                    if neighbor_module:\n",
    "                        break\n",
    "                \n",
    "                if neighbor_module:\n",
    "                    # Smaller distance means higher score (1 - normalized distance)\n",
    "                    module_scores[neighbor_module] += (1 - norm_dist)\n",
    "        \n",
    "        # 3. Sort by score and select top return_top_k\n",
    "        sorted_modules = sorted(module_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_modules = [module for module, score in sorted_modules[:return_top_k]]\n",
    "        \n",
    "        # 保持与原来相同的返回值格式\n",
    "        return {\n",
    "            \"semantic_results\": semantic_results,\n",
    "            \"murp_results\": murp_results,\n",
    "            \"import_modules\": top_modules\n",
    "        }\n",
    "\n",
    "# Usage Example\n",
    "def main():\n",
    "    # Configure paths\n",
    "    json_path = \"./Informalisation_and_Mathematical_DSRL/merged_with_embeddings_and_triples.json\"\n",
    "    murp_embedding_path = \"./Mathlib4_embeddings/outputs_cleaned/embeddings/model_dict_poincare_300_my_dataset_cleaned\"\n",
    "\n",
    "    try:\n",
    "        # Initialize hybrid search system\n",
    "        print(\"Initializing Hybrid Search System...\")\n",
    "        search_system = HybridSemanticSearch(\n",
    "            json_path=json_path,\n",
    "            murp_embedding_path=murp_embedding_path,\n",
    "            semantic_model=\"sentence-transformers/sentence-t5-large\",\n",
    "            murp_model_type=\"poincare\"\n",
    "        )\n",
    "        \n",
    "        # Execute hybrid search\n",
    "        query = \"For any real matrix A: Matrix m × n, if the columns of A are pairwise orthogonal, then the matrix Aᵀ * A is a diagonal matrix.\"\n",
    "        \n",
    "        results = search_system.hybrid_search(\n",
    "            query=query,\n",
    "            semantic_top_k=5,\n",
    "            murp_top_k=5,\n",
    "            semantic_min_score=0.1,\n",
    "            return_top_k=10\n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        print(\"Hybrid Search Results:\")\n",
    "        print(f\"Retrieved {len(results['import_modules'])} modules:\")\n",
    "        for i, module in enumerate(results['import_modules'], 1):\n",
    "            print(f\" {i}. {module}\")\n",
    "        \n",
    "        # 测试后续代码需要的格式\n",
    "        print(f\"\\nTesting format compatibility:\")\n",
    "        print(f\"Semantic results type: {type(results['semantic_results'])}\")\n",
    "        print(f\"Semantic results length: {len(results['semantic_results'])}\")\n",
    "        \n",
    "        if results['semantic_results']:\n",
    "            first_result = results['semantic_results'][0]\n",
    "            print(f\"First result type: {type(first_result)}\")\n",
    "            print(f\"First result keys: {first_result.keys() if isinstance(first_result, dict) else 'N/A'}\")\n",
    "            print(f\"Has 'concept' key: {'concept' in first_result if isinstance(first_result, dict) else False}\")\n",
    "            \n",
    "            # 测试后续代码的访问方式\n",
    "            related_concepts = [result['concept'] for result in results['semantic_results']]\n",
    "            print(f\"Successfully extracted {len(related_concepts)} concepts\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4803e5",
   "metadata": {},
   "source": [
    "## 3. Autoformalisation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8b8c6d",
   "metadata": {},
   "source": [
    "### 3.1 Prompt Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02771c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "import numpy as np\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def build_prompt(user_query, matched_concepts, all_imports=None):\n",
    "    import_modules = set(all_imports) if all_imports else set()\n",
    "    concept_text_blocks = []\n",
    "\n",
    "    for c in matched_concepts:\n",
    "        # 1. Extract module path (if module information comes from the concept itself)\n",
    "        module_path = c.get(\"module_path\", \"\")\n",
    "        if module_path:\n",
    "            import_modules.add(f\"import {module_path}\")\n",
    "\n",
    "        # 2. Extract concept attributes\n",
    "        concept_name = c.get(\"name\", \"\")\n",
    "        signature = c.get(\"signature\", \"\")\n",
    "        definition = c.get(\"definition\", \"\")\n",
    "        informal = c.get(\"informal\", \"\")\n",
    "        semantic_type = c.get(\"semantic_type\", \"\")\n",
    "        genus = c.get(\"genus\", \"\")\n",
    "        key_lemmas = c.get(\"Key lemmas\", [])\n",
    "        properties = c.get(\"properties\", [])\n",
    "        triples = c.get(\"triples\", [])\n",
    "\n",
    "        # 3. Format key lemmas\n",
    "        lemmas_text = \"\\n\".join(f\"- {lemma}\" for lemma in key_lemmas) if key_lemmas else \"N/A\"\n",
    "\n",
    "        # 4. Format properties\n",
    "        props_text = \"\\n\".join(f\"- {p}\" for p in properties) if properties else \"N/A\"\n",
    "\n",
    "        # 5. Format triples\n",
    "        triples_text = (\n",
    "            \"\\n\".join(\n",
    "                f\"- {t.get('subject')} --[{t.get('role')}]--> {t.get('object')}\"\n",
    "                for t in triples if all(k in t for k in (\"subject\", \"role\", \"object\"))\n",
    "            ) if triples else \"N/A\"\n",
    "        )\n",
    "\n",
    "        # 6. Concept text block\n",
    "        block = (\n",
    "            f\"Concept: {concept_name}\\n\"\n",
    "            f\"Lean Signature: {signature}\\n\"\n",
    "            f\"Definition: {definition}\\n\"\n",
    "            f\"Informal: {informal}\\n\"\n",
    "            f\"Genus: {genus}\\n\"\n",
    "            f\"Type: {semantic_type}\\n\"\n",
    "            f\"Key Lemmas:\\n{lemmas_text}\\n\"\n",
    "            f\"Properties:\\n{props_text}\\n\"\n",
    "            f\"Semantic Triples:\\n{triples_text}\"\n",
    "        )\n",
    "\n",
    "        concept_text_blocks.append(block)\n",
    "\n",
    "    # Concatenate import block\n",
    "    import_block = \"\\n\".join(sorted(import_modules))\n",
    "    # Concatenate context block\n",
    "    context = \"\\n\\n---\\n\\n\".join(concept_text_blocks)\n",
    "\n",
    "    # Read prompt template\n",
    "    from string import Template\n",
    "    with open(\"./prompts/prompt_template.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        template_str = f.read()\n",
    "    prompt_template = Template(template_str)\n",
    "\n",
    "    # Replace template variables\n",
    "    filled_prompt = prompt_template.substitute(\n",
    "        import_block=import_block,\n",
    "        user_query=user_query,\n",
    "        context=context,\n",
    "        matched_concepts=context\n",
    "    )\n",
    "    return filled_prompt\n",
    "\n",
    "\n",
    "# 3. Call OpenAI API with exception handling\n",
    "def ask_openai(prompt):\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in mathematics and Lean 4. You will help formalize mathematical statements.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=800\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API call failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 4. Main process\n",
    "def rag_autoformalize(user_query, top_k=5, all_imports=None):\n",
    "    related_concepts = search_similar_concepts(user_query, top_k)\n",
    "    prompt = build_prompt(user_query, related_concepts, all_imports=all_imports)\n",
    "    result = ask_openai(prompt)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444a7875",
   "metadata": {},
   "source": [
    "### 3.2 Preparation for Lean Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99c1534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_output(output: str) -> str:\n",
    "    \"\"\"Remove markdown code block markers\"\"\"\n",
    "    text = output.strip()\n",
    "    if text.startswith(\"```lean\"):\n",
    "        text = text[text.find('\\n')+1:]\n",
    "        if text.endswith(\"```\"):\n",
    "            text = text[:text.rfind(\"```\")]\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def find_lean_project_root(start_path: str = \".\") -> str:\n",
    "    \"\"\"\n",
    "    Search upwards for Lean project root directory (directory containing lakefile.lean)\n",
    "    \"\"\"\n",
    "    current = Path(start_path).resolve()\n",
    "    \n",
    "    while current != current.parent:\n",
    "        if (current / \"lakefile.lean\").exists():\n",
    "            return str(current)\n",
    "        current = current.parent\n",
    "    \n",
    "    # If not found, return current directory\n",
    "    return os.getcwd()\n",
    "\n",
    "\n",
    "def validate_lean_output(content: str = None, file_path: str = \"auto_output.lean\", \n",
    "                        project_dir: str = None) -> bool:\n",
    "    \"\"\"\n",
    "    Improved Lean validation function, automatically handles project paths and dependencies\n",
    "    \"\"\"\n",
    "    # Determine project directory\n",
    "    if project_dir is None:\n",
    "        project_dir = find_lean_project_root()\n",
    "    \n",
    "    # If content is provided, write to file\n",
    "    if content is not None:\n",
    "        full_path = os.path.join(project_dir, file_path)\n",
    "        with open(full_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    # Try different validation methods\n",
    "    validation_methods = [\n",
    "        (\"lake env lean\", [\"lake\", \"env\", \"lean\", file_path]),\n",
    "        (\"lean\", [\"lean\", file_path]),\n",
    "        (\"lake build specific\", [\"lake\", \"build\", file_path.replace('.lean', '')])\n",
    "    ]\n",
    "    \n",
    "    for method_name, command in validation_methods:\n",
    "        try:\n",
    "            print(f\"Trying validation method: {method_name}\")\n",
    "            \n",
    "            result = subprocess.run(\n",
    "                command,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=30,\n",
    "                cwd=project_dir\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(f\"Lean 4 validation passed using {method_name}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"{method_name} failed:\")\n",
    "                if result.stdout.strip():\n",
    "                    print(f\"STDOUT: {result.stdout.strip()}\")\n",
    "                if result.stderr.strip():\n",
    "                    print(f\"STDERR: {result.stderr.strip()}\")\n",
    "                    \n",
    "                # If it's a Mathlib path issue, try next method\n",
    "                if \"unknown module prefix 'Mathlib'\" in result.stderr:\n",
    "                    print(\"Mathlib not found, trying next method...\")\n",
    "                    continue\n",
    "                else:\n",
    "                    # Other errors, return failure directly\n",
    "                    return False\n",
    "                    \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(f\"{method_name} timed out, trying next method...\")\n",
    "            continue\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Command not found: {command[0]}, trying next method...\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Exception with {method_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"All validation methods failed\")\n",
    "    return False\n",
    "\n",
    "\n",
    "def setup_lean_environment(project_dir: str):\n",
    "    \"\"\"\n",
    "    Set up Lean environment, download dependencies\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Setting up Lean environment...\")\n",
    "        \n",
    "        # Try to get cached dependencies\n",
    "        cache_result = subprocess.run(\n",
    "            [\"lake\", \"exe\", \"cache\", \"get\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=300,\n",
    "            cwd=project_dir\n",
    "        )\n",
    "        \n",
    "        if cache_result.returncode != 0:\n",
    "            print(\"Cache get failed, trying lake update...\")\n",
    "            # If cache fails, try updating\n",
    "            update_result = subprocess.run(\n",
    "                [\"lake\", \"update\"],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=600,\n",
    "                cwd=project_dir\n",
    "            )\n",
    "            \n",
    "            if update_result.returncode != 0:\n",
    "                print(f\"Lake update failed: {update_result.stderr}\")\n",
    "                return False\n",
    "        \n",
    "        print(\"Lean environment setup complete\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to setup Lean environment: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def iterative_refine(user_query, max_iters=20, project_dir=None):\n",
    "    if project_dir is None:\n",
    "        project_dir = find_lean_project_root()\n",
    "    \n",
    "    if not setup_lean_environment(project_dir):\n",
    "        print(\"Warning: Lean environment setup failed, continuing anyway...\")\n",
    "    \n",
    "    last_output = None\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        print(f\"\\n--- Iteration {i+1} ---\")\n",
    "        \n",
    "        if i == 0:\n",
    "            prompt = build_prompt(user_query, search_similar_concepts(user_query, top_k=5))\n",
    "        else:\n",
    "            prompt = (\n",
    "                build_prompt(user_query, search_similar_concepts(user_query, top_k=5))\n",
    "                + f\"\\n\\n# The previous output had validation errors. Please fix:\\n{last_output}\\n\\n# Provide a corrected version:\"\n",
    "            )\n",
    "        \n",
    "        output = ask_openai(prompt)\n",
    "        cleaned = clean_output(output)\n",
    "\n",
    "        if validate_lean_output(cleaned, \"auto_output.lean\", project_dir):\n",
    "            print(f\"Iteration {i+1}: Valid output found!\")\n",
    "            return cleaned\n",
    "        else:\n",
    "            print(f\"Iteration {i+1}: Output invalid, retrying...\")\n",
    "            last_output = cleaned\n",
    "    \n",
    "    print(\"Max iterations reached, returning last output.\")\n",
    "    return last_output or \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b3bf8c",
   "metadata": {},
   "source": [
    "### 3.3. Hybrid RAG Autoformalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6aa742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the rag_autoformalize function to accept search_system parameter\n",
    "def rag_autoformalize(user_query, search_system, top_k=5, all_imports=None):\n",
    "    # Use hybrid search system to retrieve related concepts and modules\n",
    "    hybrid_results = search_system.hybrid_search(user_query, semantic_top_k=top_k)\n",
    "    \n",
    "    # Extract related concepts from hybrid search results\n",
    "    related_concepts = [result['concept'] for result in hybrid_results['semantic_results']]\n",
    "    \n",
    "    # Extract import modules from hybrid search results (if all_imports is not provided)\n",
    "    if all_imports is None:\n",
    "        all_imports = hybrid_results['import_modules']\n",
    "    \n",
    "    # Build prompt and call OpenAI\n",
    "    prompt = build_prompt(user_query, related_concepts, all_imports=all_imports)\n",
    "    result = ask_openai(prompt)\n",
    "    return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Configure paths\n",
    "    json_path = \"./Informalisation_and_Mathematical_DSRL/merged_with_embeddings_and_triples.json\"\n",
    "    murp_embedding_path = \"./Mathlib4_embeddings/outputs_cleaned/embeddings/model_dict_poincare_300_my_dataset_cleaned\"\n",
    "\n",
    "    try:\n",
    "        # Initialize hybrid search system\n",
    "        print(\"Initializing Hybrid Search System...\")\n",
    "        search_system = HybridSemanticSearch(\n",
    "            json_path=json_path,\n",
    "            murp_embedding_path=murp_embedding_path,\n",
    "            semantic_model=\"sentence-transformers/sentence-t5-large\",\n",
    "            murp_model_type=\"poincare\"\n",
    "        )\n",
    "        \n",
    "        # Execute hybrid search and get autoformalization results\n",
    "        query = \"For any real matrix A: Matrix m × n, if the columns of A are pairwise orthogonal, then the matrix Aᵀ * A is a diagonal matrix.\"\n",
    "        \n",
    "        # Use hybrid search system to get autoformalization results\n",
    "        output = rag_autoformalize(query, search_system, top_k=5)\n",
    "        \n",
    "        # Clean output\n",
    "        if output.strip().startswith(\"```lean\"):\n",
    "            output = output.strip()\n",
    "            output = output[output.find('\\n')+1:]  # Remove first line ```lean\n",
    "            if output.endswith(\"```\"):\n",
    "                output = output[:output.rfind(\"```\")]\n",
    "        \n",
    "        # Write to file\n",
    "        with open(\"auto_output.lean\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(output)\n",
    "\n",
    "        print(output)\n",
    "        \n",
    "        # Validate output\n",
    "        result = validate_lean_output(output)\n",
    "        print(f\"Validation result: {result}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a55508e",
   "metadata": {},
   "source": [
    "### RAG-Based Autoformalization with Iterative Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79709407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_refine(user_query, search_system, max_iters=20, project_dir=None):\n",
    "    \"\"\"\n",
    "    Iterative refinement function using hybrid search system\n",
    "    \n",
    "    Args:\n",
    "        user_query: User query\n",
    "        search_system: Hybrid search system instance\n",
    "        max_iters: Maximum number of iterations\n",
    "        project_dir: Lean project directory\n",
    "    \"\"\"\n",
    "    if project_dir is None:\n",
    "        project_dir = find_lean_project_root()\n",
    "    \n",
    "    if not setup_lean_environment(project_dir):\n",
    "        print(\"Warning: Lean environment setup failed, continuing anyway...\")\n",
    "\n",
    "    last_output = None\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        print(f\"\\n--- Iteration {i+1} ---\")\n",
    "        \n",
    "        # Use hybrid search system to get results\n",
    "        hybrid_results = search_system.hybrid_search(user_query, semantic_top_k=5)\n",
    "        related_concepts = [result['concept'] for result in hybrid_results['semantic_results']]\n",
    "        all_imports = hybrid_results['import_modules']\n",
    "        \n",
    "        if i == 0:\n",
    "            prompt = build_prompt(user_query, related_concepts, all_imports=all_imports)\n",
    "        else:\n",
    "            prompt = (\n",
    "                build_prompt(user_query, related_concepts, all_imports=all_imports)\n",
    "                + f\"\\n\\n# The previous output had validation errors. Please fix:\\n{last_output}\\n\\n# Provide a corrected version:\"\n",
    "            )\n",
    "        \n",
    "        output = ask_openai(prompt)\n",
    "        cleaned = clean_output(output)\n",
    "\n",
    "        if validate_lean_output(cleaned, \"auto_output.lean\", project_dir):\n",
    "            print(f\"Iteration {i+1}: Valid output found!\")\n",
    "            return cleaned\n",
    "        else:\n",
    "            print(f\"Iteration {i+1}: Output invalid, retrying...\")\n",
    "            last_output = cleaned\n",
    "    \n",
    "    print(\"⚠️ Max iterations reached, returning last output.\")\n",
    "    return last_output or \"\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure paths\n",
    "    json_path = \"./Informalisation_and_Mathematical_DSRL/merged_with_embeddings_and_triples.json\"\n",
    "    murp_embedding_path = \"./Mathlib4_embeddings/outputs_cleaned/embeddings/model_dict_poincare_300_my_dataset_cleaned\"\n",
    "\n",
    "    try:\n",
    "        # Initialize hybrid search system\n",
    "        print(\"Initializing Hybrid Search System...\")\n",
    "        search_system = HybridSemanticSearch(\n",
    "            json_path=json_path,\n",
    "            murp_embedding_path=murp_embedding_path,\n",
    "            semantic_model=\"sentence-transformers/sentence-t5-large\",\n",
    "            murp_model_type=\"poincare\"\n",
    "        )\n",
    "        \n",
    "        # Natural language description of matrix associativity\n",
    "        matrix_statement = (\n",
    "           \"For any real matrix A: Matrix m × n, if the columns of A are pairwise orthogonal, then the matrix Aᵀ * A is a diagonal matrix.\"\n",
    "           # \"For any two integers a and b, if a divides b and b divides a, then a is equal to b or a is equal to -b.\"\n",
    "        )\n",
    "        \n",
    "        # Call iterative refinement, passing hybrid search system instance\n",
    "        final_code = iterative_refine(\n",
    "            user_query=matrix_statement, \n",
    "            search_system=search_system, \n",
    "            max_iters=5\n",
    "        )\n",
    "        \n",
    "        # Directly validate final output\n",
    "        result = validate_lean_output(final_code)\n",
    "        print(f\"Validation result: {result}\")\n",
    "        \n",
    "        # Print final code\n",
    "        print(f\"\\nFinal validated code:\\n{final_code}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
