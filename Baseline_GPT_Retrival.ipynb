{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b8b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from openai import OpenAI  # Using the new OpenAI client\n",
    "\n",
    "# Define a valid regex pattern for mathematical concepts: only letters, digits, underscores, length at least 2\n",
    "valid_definiendum_pattern = re.compile(r\"^[a-z0-9_]{2,}$\")\n",
    "math_symbols = set(\"+-*/=∑∏√∫<>∈∉{}[]()\")\n",
    "\n",
    "def clean_definiendum(definiendum):\n",
    "    \"\"\"Clean definiendum, returning a valid form or empty string.\"\"\"\n",
    "    if not definiendum or not isinstance(definiendum, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Replace periods with underscores\n",
    "    definiendum = definiendum.replace(\".\", \"_\")\n",
    "    \n",
    "    # Remove mathematical symbols\n",
    "    definiendum = ''.join(c for c in definiendum if c not in math_symbols)\n",
    "    \n",
    "    # Remove non-ASCII characters\n",
    "    definiendum = ''.join(c for c in definiendum if ord(c) < 128)\n",
    "    \n",
    "    # Replace spaces with underscores\n",
    "    definiendum = definiendum.replace(\" \", \"_\")\n",
    "    \n",
    "    # Retain only valid characters (letters, digits, underscores)\n",
    "    definiendum = re.sub(r'[^A-Za-z0-9_]', '', definiendum)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    definiendum = definiendum.lower()\n",
    "    \n",
    "    if not definiendum:\n",
    "        return \"\"\n",
    "    \n",
    "    return definiendum\n",
    "\n",
    "def normalize_entity_name(name):\n",
    "    \"\"\"Standardize entity names for matching in the original dataset.\"\"\"\n",
    "    return \"_\".join(name.lower().strip().split())\n",
    "\n",
    "class GPTBaseline:\n",
    "    \"\"\"GPT retrieval baseline: directly retrieves relevant Mathlib4 modules using GPT.\"\"\"\n",
    "    def __init__(self, api_key=None, model=\"gpt-4o\", verbose=True):\n",
    "        self.model = model\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Set OpenAI API key\n",
    "        if api_key:\n",
    "            self.client = OpenAI(api_key=api_key)\n",
    "        elif \"OPENAI_API_KEY\" in os.environ:\n",
    "            self.client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "        else:\n",
    "            raise ValueError(\"OpenAI API key not provided and OPENAI_API_KEY environment variable not set\")\n",
    "    \n",
    "    def log(self, message, level=\"info\"):\n",
    "        \"\"\"Log messages according to verbosity settings.\"\"\"\n",
    "        if self.verbose or level == \"error\":\n",
    "            print(message)\n",
    "    \n",
    "    def search_modules(self, query, max_retries=3):\n",
    "        \"\"\"Retrieve relevant Mathlib4 modules for a query using GPT.\"\"\"\n",
    "        if self.verbose:\n",
    "            self.log(f\"GPT searching for: '{query[:50]}...'\")\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert in the Lean 4 Theorem Prover and Mathlib4 library. \n",
    "        Identify the 10 most relevant Mathlib4 modules needed to formalize a given mathematical concept in Lean 4.\n",
    "\n",
    "        Query: {query}\n",
    "\n",
    "        Return EXACTLY 10 module names in a JSON array format.\n",
    "        Only include modules that exist in Mathlib4.\n",
    "        Rank modules by relevance, most essential first.\n",
    "        Prioritize core definitions, key theorems, algebraic structures, notation, and important constructions.\n",
    "        \"\"\"\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in Lean and Mathlib4.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0.1,\n",
    "                    max_tokens=500\n",
    "                )\n",
    "                \n",
    "                # Extract content\n",
    "                content = response.choices[0].message.content.strip()\n",
    "                \n",
    "                # Try parsing JSON\n",
    "                try:\n",
    "                    json_match = re.search(r'\\[.*\\]', content, re.DOTALL)\n",
    "                    if json_match:\n",
    "                        modules = json.loads(json_match.group())\n",
    "                    else:\n",
    "                        modules = json.loads(content)\n",
    "                    \n",
    "                    if self.verbose:\n",
    "                        self.log(f\"GPT found {len(modules)} modules: {modules}\")\n",
    "                    \n",
    "                    return modules\n",
    "                \n",
    "                except json.JSONDecodeError as e:\n",
    "                    if attempt == max_retries - 1:\n",
    "                        self.log(f\"Failed to parse GPT response after {max_retries} attempts: {e}\", \"error\")\n",
    "                        module_pattern = r'[A-Za-z][A-Za-z0-9_]*(\\.[A-Za-z][A-Za-z0-9_]*)*'\n",
    "                        potential_modules = re.findall(module_pattern, content)\n",
    "                        return list(set(potential_modules))[:5]\n",
    "                    \n",
    "                    time.sleep(1)\n",
    "            \n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    self.log(f\"GPT search failed after {max_retries} attempts: {e}\", \"error\")\n",
    "                    return []\n",
    "                time.sleep(2)\n",
    "        \n",
    "        return []\n",
    "\n",
    "class GPTEvaluator:\n",
    "    \"\"\"GPT Baseline evaluator - aligned with original experimental design.\"\"\"\n",
    "    def __init__(self, json_path, openai_api_key=None, sample_size=50, random_seed=42):\n",
    "        self.json_path = json_path\n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.sample_size = sample_size\n",
    "        self.random_seed = random_seed\n",
    "        self.evaluation_data = None\n",
    "        \n",
    "        random.seed(random_seed)\n",
    "        self.prepare_evaluation_data()\n",
    "    \n",
    "    def prepare_evaluation_data(self):\n",
    "        \"\"\"Prepare evaluation samples from JSON data.\"\"\"\n",
    "        print(\"\\nPreparing evaluation data...\")\n",
    "        if not os.path.exists(self.json_path):\n",
    "            raise FileNotFoundError(f\"File not found: {self.json_path}\")\n",
    "        \n",
    "        with open(self.json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            original_dataset = json.load(f)\n",
    "        \n",
    "        all_definitions = []\n",
    "        for module_name, module_data in original_dataset.items():\n",
    "            for definition in module_data.get(\"definitions\", []):\n",
    "                if not isinstance(definition, dict):\n",
    "                    continue\n",
    "                semantic_analysis = definition.get(\"semantic_analysis\", {})\n",
    "                informal = semantic_analysis.get(\"informal\", \"\") if isinstance(semantic_analysis, dict) else \"\"\n",
    "                def_name = definition.get(\"name\", \"\")\n",
    "                if informal and def_name:\n",
    "                    all_definitions.append({\n",
    "                        \"module\": module_name,\n",
    "                        \"name\": def_name,\n",
    "                        \"query\": informal\n",
    "                    })\n",
    "        \n",
    "        print(f\"Found {len(all_definitions)} valid definitions with informal descriptions\")\n",
    "        \n",
    "        if len(all_definitions) < self.sample_size:\n",
    "            print(f\"Only {len(all_definitions)} definitions available, using all\")\n",
    "            self.sample_size = len(all_definitions)\n",
    "        \n",
    "        random.shuffle(all_definitions)\n",
    "        self.evaluation_data = all_definitions[:self.sample_size]\n",
    "        \n",
    "        print(f\"Prepared {len(self.evaluation_data)} evaluation samples\")\n",
    "        return self.evaluation_data\n",
    "    \n",
    "    def find_entity_details(self, entities):\n",
    "        \"\"\"Find the module and dependencies of each entity in the dataset.\"\"\"\n",
    "        entity_details = {}\n",
    "        if not os.path.exists(self.json_path):\n",
    "            raise FileNotFoundError(f\"File not found: {self.json_path}\")\n",
    "        \n",
    "        with open(self.json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            original_dataset = json.load(f)\n",
    "        \n",
    "        normalized_entities = {normalize_entity_name(ent): ent for ent in entities}\n",
    "        norm_keys = set(normalized_entities.keys())\n",
    "        \n",
    "        for module_name, module_data in original_dataset.items():\n",
    "            module_dependencies = module_data.get(\"dependencies\", [])\n",
    "            for definition in module_data.get(\"definitions\", []):\n",
    "                if not isinstance(definition, dict):\n",
    "                    continue\n",
    "                raw_name = definition.get(\"concept_name\") or definition.get(\"name\")\n",
    "                if not raw_name:\n",
    "                    continue\n",
    "                norm_name = normalize_entity_name(raw_name)\n",
    "                if norm_name in norm_keys:\n",
    "                    original_entity = normalized_entities[norm_name]\n",
    "                    entity_details[original_entity] = {\n",
    "                        \"module\": module_name,\n",
    "                        \"dependencies\": module_dependencies\n",
    "                    }\n",
    "                    \n",
    "        return entity_details\n",
    "    \n",
    "    def evaluate_gpt_baseline(self):\n",
    "        \"\"\"Evaluate GPT baseline performance.\"\"\"\n",
    "        if not self.evaluation_data:\n",
    "            self.prepare_evaluation_data()\n",
    "        \n",
    "        sample_size = len(self.evaluation_data)\n",
    "        print(f\"\\nEvaluating GPT Baseline with {sample_size} samples...\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        gpt_model = GPTBaseline(api_key=self.openai_api_key, model=\"gpt-4o\", verbose=True)\n",
    "        \n",
    "        results = {\n",
    "            \"model_name\": \"GPT Baseline\",\n",
    "            \"total_samples\": sample_size,\n",
    "            \"correct_predictions\": 0,\n",
    "            \"incorrect_predictions\": 0,\n",
    "            \"module_recall\": 0.0,\n",
    "            \"query_times\": [],\n",
    "            \"module_coverage\": defaultdict(int),\n",
    "            \"detailed_results\": []\n",
    "        }\n",
    "        \n",
    "        progress_interval = max(1, sample_size // 10)\n",
    "        \n",
    "        for i, sample in enumerate(self.evaluation_data):\n",
    "            if i % progress_interval == 0:\n",
    "                print(f\"Processing sample {i+1}/{sample_size} ({((i+1)/sample_size)*100:.1f}%)\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                retrieved_modules = gpt_model.search_modules(sample['query'])\n",
    "                elapsed_time = time.time() - start_time\n",
    "                results[\"query_times\"].append(elapsed_time)\n",
    "                \n",
    "                retrieved_modules_set = set(retrieved_modules)\n",
    "                target_module = sample[\"module\"]\n",
    "                is_correct = target_module in retrieved_modules_set\n",
    "                \n",
    "                if is_correct:\n",
    "                    results[\"correct_predictions\"] += 1\n",
    "                    results[\"module_coverage\"][target_module] += 1\n",
    "                else:\n",
    "                    results[\"incorrect_predictions\"] += 1\n",
    "                \n",
    "                detailed_result = {\n",
    "                    \"sample_id\": i,\n",
    "                    \"definition_name\": sample[\"name\"],\n",
    "                    \"target_module\": target_module,\n",
    "                    \"query\": sample[\"query\"],\n",
    "                    \"retrieved_modules\": list(retrieved_modules_set),\n",
    "                    \"is_correct\": is_correct,\n",
    "                    \"time_taken\": elapsed_time\n",
    "                }\n",
    "                results[\"detailed_results\"].append(detailed_result)\n",
    "                time.sleep(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                results[\"query_times\"].append(elapsed_time)\n",
    "                results[\"incorrect_predictions\"] += 1\n",
    "                print(f\"Error during search for sample {i+1}: {e}\")\n",
    "                detailed_result = {\n",
    "                    \"sample_id\": i,\n",
    "                    \"definition_name\": sample[\"name\"],\n",
    "                    \"target_module\": target_module,\n",
    "                    \"query\": sample[\"query\"],\n",
    "                    \"error\": str(e),\n",
    "                    \"is_correct\": False,\n",
    "                    \"time_taken\": elapsed_time\n",
    "                }\n",
    "                results[\"detailed_results\"].append(detailed_result)\n",
    "        \n",
    "        results[\"module_recall\"] = results[\"correct_predictions\"] / sample_size if sample_size > 0 else 0\n",
    "        results[\"avg_query_time\"] = sum(results[\"query_times\"]) / sample_size if sample_size > 0 else 0\n",
    "        \n",
    "        total_correct = results[\"correct_predictions\"]\n",
    "        if total_correct > 0:\n",
    "            for module, count in results[\"module_coverage\"].items():\n",
    "                results[\"module_coverage\"][module] = count / total_correct\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"GPT BASELINE EVALUATION SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Total Samples: {results['total_samples']}\")\n",
    "        print(f\"Correct Predictions: {results['correct_predictions']}\")\n",
    "        print(f\"Incorrect Predictions: {results['incorrect_predictions']}\")\n",
    "        print(f\"Module Recall: {results['module_recall']:.4f}\")\n",
    "        print(f\"Average Query Time: {results['avg_query_time']:.2f} seconds\")\n",
    "        \n",
    "        if results[\"module_coverage\"]:\n",
    "            print(\"\\nTop Modules by Coverage:\")\n",
    "            sorted_modules = sorted(results[\"module_coverage\"].items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "            for module, coverage in sorted_modules:\n",
    "                print(f\"  {module}: {coverage:.2f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_evaluation_results(self, results, output_path=\"gpt_baseline_evaluation.json\"):\n",
    "        \"\"\"Save evaluation results to JSON.\"\"\"\n",
    "        print(f\"\\nSaving evaluation results to {output_path}\")\n",
    "        save_data = {\n",
    "            \"evaluation_summary\": {\n",
    "                \"model_name\": results[\"model_name\"],\n",
    "                \"total_samples\": results[\"total_samples\"],\n",
    "                \"correct_predictions\": results[\"correct_predictions\"],\n",
    "                \"incorrect_predictions\": results[\"incorrect_predictions\"],\n",
    "                \"module_recall\": results[\"module_recall\"],\n",
    "                \"avg_query_time\": results[\"avg_query_time\"],\n",
    "                \"module_coverage\": dict(results[\"module_coverage\"])\n",
    "            },\n",
    "            \"detailed_results\": results[\"detailed_results\"]\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(save_data, f, indent=2, ensure_ascii=False)\n",
    "            print(\"Evaluation results saved successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save evaluation results: {e}\")\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Example usage\n",
    "# ----------------------\n",
    "def main():\n",
    "    json_path = \"./Informalisation_and_Mathematical_DSRL/merged_with_embeddings_and_triples.json\"\n",
    "    OPENAI_API_KEY = \"<YOUR_API_KEY>\"  # Replace with your actual OpenAI API key\n",
    "    \n",
    "    try:\n",
    "        print(\"Initializing GPT Baseline Evaluator...\")\n",
    "        evaluator = GPTEvaluator(json_path, OPENAI_API_KEY, sample_size=10)\n",
    "        \n",
    "        print(\"\\nStarting GPT Baseline evaluation...\")\n",
    "        gpt_results = evaluator.evaluate_gpt_baseline()\n",
    "        \n",
    "        evaluator.save_evaluation_results(gpt_results)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"Running single query example with GPT Baseline...\")\n",
    "        gpt_model = GPTBaseline(api_key=OPENAI_API_KEY, model=\"gpt-4o\", verbose=True)\n",
    "        \n",
    "        query = \"For any real matrix A: Matrix m × n, if the columns of A are pairwise orthogonal, then the matrix Aᵀ * A is a diagonal matrix.\"\n",
    "        gpt_modules = gpt_model.search_modules(query)\n",
    "        \n",
    "        print(\"\\nRetrieved modules:\")\n",
    "        for module in gpt_modules:\n",
    "            print(f\" - {module}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
