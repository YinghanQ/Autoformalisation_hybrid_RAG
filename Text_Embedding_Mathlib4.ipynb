{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85afe34b",
   "metadata": {},
   "source": [
    "# Text Embeddings\n",
    "## * Model Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from transformers import AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "MODELS = {\n",
    "    \"Glove\": \"glove-wiki-gigaword-300\",\n",
    "    \"Word2Vec\": \"word2vec-google-news-300\",\n",
    "    \"bert-base\": \"bert-base-uncased\",\n",
    "    \"bert-large\": \"bert-large-uncased\",\n",
    "    \"defsent-bert\": \"bert-base-uncased\",\n",
    "    \"defsent-roberta\": \"sentence-transformers/all-roberta-large-v1\",\n",
    "    \"distilroberta-vl\": \"sentence-transformers/all-distilroberta-v1\",\n",
    "    \"mpnet-base-v2\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"sentence-t5-large\": \"sentence-transformers/sentence-t5-large\"\n",
    "}\n",
    "\n",
    "def get_model_dimension(model_name: str) -> int:\n",
    "    if model_name in [\"Glove\", \"Word2Vec\"]:\n",
    "        wv = api.load(MODELS[model_name])\n",
    "        return wv.vector_size\n",
    "\n",
    "    elif model_name in [\"bert-base\", \"bert-large\", \"defsent-bert\", \"defsent-roberta\"]:\n",
    "        model = AutoModel.from_pretrained(MODELS[model_name])\n",
    "        return model.config.hidden_size\n",
    "\n",
    "    else:  # sentence-transformers \n",
    "        st_model = SentenceTransformer(MODELS[model_name])\n",
    "        return st_model.get_sentence_embedding_dimension()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_list = [\n",
    "        \"Glove\", \"Word2Vec\", \"bert-base\", \"bert-large\",\n",
    "        \"defsent-bert\", \"defsent-roberta\",\n",
    "        \"distilroberta-vl\", \"mpnet-base-v2\", \"sentence-t5-large\"\n",
    "    ]\n",
    "    for model_name in model_list:\n",
    "        dim = get_model_dimension(model_name)\n",
    "        print(f\"{model_name}: embedding dimension = {dim}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813523aa",
   "metadata": {},
   "source": [
    "## 1. Set Up Opnenai API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "OPENAI_API_KEY = \"your_openai_api_key_here\"\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f3195",
   "metadata": {},
   "source": [
    "## 2. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17218cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import gensim.downloader as api\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# ==== Model Mapping ====\n",
    "MODELS = {\n",
    "    \"Glove\": \"glove-wiki-gigaword-300\",\n",
    "    \"Word2Vec\": \"word2vec-google-news-300\",\n",
    "    \"bert-base\": \"bert-base-uncased\",\n",
    "    \"bert-large\": \"bert-large-uncased\",\n",
    "    \"defsent-bert\": \"bert-base-uncased\",\n",
    "    \"defsent-roberta\": \"sentence-transformers/all-roberta-large-v1\",\n",
    "    \"distilroberta-vl\": \"sentence-transformers/all-distilroberta-v1\",\n",
    "    \"mpnet-base-v2\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"sentence-t5-large\": \"sentence-transformers/sentence-t5-large\"\n",
    "}\n",
    "\n",
    "# --- Static Word Vector Embedding ---\n",
    "def embed_with_static_vectors(texts: List[str], wv) -> List[np.ndarray]:\n",
    "    vectors = []\n",
    "    for text in tqdm(texts):\n",
    "        words = text.lower().split()\n",
    "        word_vectors = [wv[word] for word in words if word in wv]\n",
    "        if word_vectors:\n",
    "            avg_vector = np.mean(word_vectors, axis=0)\n",
    "        else:\n",
    "            avg_vector = np.zeros(wv.vector_size)\n",
    "        vectors.append(avg_vector)\n",
    "    return vectors\n",
    "\n",
    "# --- BERT Class Model Embedding ---\n",
    "def embed_with_bert(text: str, tokenizer, model) -> np.ndarray:\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden = outputs.last_hidden_state  # (1, seq_len, hidden)\n",
    "        attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "        masked_hidden = last_hidden * attention_mask\n",
    "        pooled = masked_hidden.sum(dim=1) / attention_mask.sum(dim=1)\n",
    "        return pooled.squeeze().numpy()\n",
    "\n",
    "# --- Main Embedding Entry ---\n",
    "def embed_texts(model_name: str, texts: List[str]) -> List[np.ndarray]:\n",
    "    print(f\"\\nEmbedding with model: {model_name}\")\n",
    "    if model_name in [\"Glove\", \"Word2Vec\"]:\n",
    "        return embed_with_static_vectors(texts, api.load(MODELS[model_name]))\n",
    "    elif model_name in [\"bert-base\", \"bert-large\", \"defsent-bert\", \"defsent-roberta\"]:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODELS[model_name])\n",
    "        model = AutoModel.from_pretrained(MODELS[model_name])\n",
    "        model.eval()\n",
    "        return [embed_with_bert(text, tokenizer, model) for text in tqdm(texts)]\n",
    "    else:\n",
    "        st_model = SentenceTransformer(MODELS[model_name])\n",
    "        return st_model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# --- Batch File + Merge Processing ---\n",
    "def process_all_files(input_folder: str, model_name: str, output_path: str):\n",
    "    merged_data = {}\n",
    "    texts = []\n",
    "    concept_refs = []\n",
    "\n",
    "    # Traverse all JSON files in the folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # Merge into main dictionary\n",
    "            for key, value in data.items():\n",
    "                merged_data[key] = value\n",
    "                for definition in value.get(\"definitions\", []):\n",
    "                    concepts = definition.get(\"semantic_analysis\", {}).get(\"concepts\", [])\n",
    "                    for concept in concepts:\n",
    "                        if \"informal_definition\" in concept and concept[\"informal_definition\"].strip():\n",
    "                            texts.append(concept[\"informal_definition\"])\n",
    "                            concept_refs.append(concept)\n",
    "\n",
    "    print(f\"🔍 {model_name}: Total {len(texts)} texts to embed\")\n",
    "\n",
    "    # Generate embeddings\n",
    "    embeddings = embed_texts(model_name, texts)\n",
    "\n",
    "    # Write back to JSON\n",
    "    for concept, emb in zip(concept_refs, embeddings):\n",
    "        concept[\"embedding_vector\"] = emb.tolist()\n",
    "\n",
    "    # Save merged results\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"{model_name} embeddings saved to {output_path}\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"./Informalisation_and_Mathematical_DSRL/informal_data\"  # Folder containing multiple JSON files\n",
    "    output_dir = \"./embedding_results\"\n",
    "\n",
    "    model_list = [\n",
    "        \"Glove\", \"Word2Vec\", \"bert-base\", \"bert-large\",\n",
    "        \"defsent-bert\", \"defsent-roberta\",\n",
    "        \"distilroberta-vl\", \"mpnet-base-v2\", \"sentence-t5-large\"\n",
    "    ]\n",
    "\n",
    "    for model_name in model_list:\n",
    "        output_file = os.path.join(output_dir, f\"merged_with_embeddings_{model_name}.json\")\n",
    "        process_all_files(input_folder, model_name, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35311ebf",
   "metadata": {},
   "source": [
    "## 3. Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def clean_concept_name(name: str) -> str:\n",
    "    # Remove leading/trailing spaces and convert to lowercase\n",
    "    name = name.strip().lower()\n",
    "    # Remove special characters (keep alphanumeric and spaces)\n",
    "    name = re.sub(r'[^a-z0-9\\s]', '', name)\n",
    "    # Compress extra spaces\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    return name\n",
    "\n",
    "# Input file path\n",
    "input_path = \"./embedding_results/merged_with_embeddings_sentence-t5-large.json\"\n",
    "output_path = \"./embedding_results/merged_with_embeddings_sentence-t5-large_cleaned.json\"\n",
    "\n",
    "# Read JSON\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Traverse concepts for cleaning\n",
    "for key, value in data.items():\n",
    "    for definition in value.get(\"definitions\", []):\n",
    "        concepts = definition.get(\"semantic_analysis\", {}).get(\"concepts\", [])\n",
    "        for concept in concepts:\n",
    "            if \"name\" in concept:\n",
    "                concept[\"name_cleaned\"] = clean_concept_name(concept[\"name\"])\n",
    "\n",
    "# Save cleaned results\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Cleaning completed, saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
