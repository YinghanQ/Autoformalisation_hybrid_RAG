{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f85d33",
   "metadata": {},
   "source": [
    "# Ablation Study: Semantic vs. Hybrid Embedding Methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15832038",
   "metadata": {},
   "source": [
    "##  MuRP and MuRE Nearest Neighbor Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be429f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from web.embeddings import load_embedding\n",
    "from web.evaluate import poincare_distance  # Hyperbolic distance (MuRP)\n",
    "from scipy.spatial.distance import euclidean  # Euclidean distance (MuRE)\n",
    "\n",
    "# ----------------------\n",
    "# 1. Load Embeddings (using MuRP as example)\n",
    "# ----------------------\n",
    "embedding_path = \"./Mathlib4_embeddings/outputs_cleaned/embeddings/model_dict_poincare_300_my_dataset_cleaned\"\n",
    "model_type = \"poincare\"  # \"poincare\" for MuRP, \"euclidean\" for MuRE\n",
    "\n",
    "# Load embeddings (returns an Embedding instance)\n",
    "embeddings = load_embedding(\n",
    "    embedding_path,\n",
    "    format=\"dict\",\n",
    "    normalize=True,   # Required for hyperbolic embeddings (projected inside unit ball)\n",
    "    lower=True,\n",
    "    clean_words=False\n",
    ")\n",
    "\n",
    "# Extract vocabulary and vector list\n",
    "vocab = embeddings.words\n",
    "vectors = np.array([embeddings[word] for word in vocab])\n",
    "\n",
    "# ----------------------\n",
    "# 2. Define Retrieval Function (with distance normalization)\n",
    "# ----------------------\n",
    "def retrieve_nearest_neighbors(query_word, top_k=5):\n",
    "    if query_word not in embeddings:\n",
    "        raise ValueError(f\"Query word '{query_word}' not found in the embedding vocabulary.\")\n",
    "\n",
    "    query_vec = embeddings[query_word]\n",
    "    distances = []\n",
    "    for word in vocab:\n",
    "        vec = embeddings[word]\n",
    "        if model_type == \"poincare\":\n",
    "            dist = poincare_distance(query_vec, vec)\n",
    "        else:\n",
    "            dist = euclidean(query_vec, vec)\n",
    "        distances.append((word, dist))\n",
    "\n",
    "    # Sort distances and exclude the query word itself\n",
    "    distances_sorted = sorted(distances, key=lambda x: x[1])\n",
    "    distances_sorted = [item for item in distances_sorted if item[0] != query_word]\n",
    "\n",
    "    # Take top-k nearest neighbors\n",
    "    top_neighbors = distances_sorted[:top_k]\n",
    "\n",
    "    # Normalize distances using min-max scaling\n",
    "    dists = np.array([dist for _, dist in top_neighbors])\n",
    "    min_dist, max_dist = dists.min(), dists.max()\n",
    "    norm_dists = (dists - min_dist) / (max_dist - min_dist + 1e-9)  # +1e-9 to avoid division by zero\n",
    "\n",
    "    # Return list with normalized distances\n",
    "    return [(word, dist, norm) for (word, dist), norm in zip(top_neighbors, norm_dists)]\n",
    "\n",
    "# ----------------------\n",
    "# 3. Retrieval Example\n",
    "# ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"transpose\"  # Replace with a valid word from your vocabulary\n",
    "    top_k = 5\n",
    "\n",
    "    try:\n",
    "        neighbors = retrieve_nearest_neighbors(query, top_k=top_k)\n",
    "        print(f\"Top {top_k} most similar words to '{query}' in {model_type} space:\")\n",
    "        for i, (word, dist, norm) in enumerate(neighbors, 1):\n",
    "            print(f\"{i}. {word} (distance: {dist:.4f}, normalized: {norm:.4f})\")\n",
    "    except ValueError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d4b6f9",
   "metadata": {},
   "source": [
    "This module implements an ablation study to evaluate the effectiveness of different embedding strategies \n",
    "on Mathlib4 concept retrieval. Specifically, it compares:\n",
    "\n",
    "1. **Only Semantic Text Embeddings** — using Sentence Transformer embeddings for retrieval.\n",
    "2. **Hybrid Method** — combining semantic embeddings with MuRP-based hyperbolic embeddings for enhanced relational reasoning.\n",
    "\n",
    "The study measures retrieval performance, coverage of relevant concepts, and quality of extracted modules,\n",
    "providing insights into the contribution of each component in the hybrid approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04883bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from web.embeddings import load_embedding\n",
    "from web.evaluate import poincare_distance\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Avoid parallelism warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Define regular expression for valid mathematical concepts: only letters, numbers, underscores, minimum length 2\n",
    "valid_definiendum_pattern = re.compile(r\"^[a-z0-9_]{2,}$\")  # Updated regex to match lowercase only\n",
    "math_symbols = set(\"+-*/=∑∏√∫<>∈∉{}[]()\")\n",
    "\n",
    "def clean_definiendum(definiendum):\n",
    "    \"\"\"Clean the definiendum, return a valid form or an empty string\"\"\"\n",
    "    if not definiendum or not isinstance(definiendum, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # First, replace dots with underscores (correcting dot handling)\n",
    "    definiendum = definiendum.replace(\".\", \"_\")\n",
    "    \n",
    "    # Remove mathematical symbols\n",
    "    definiendum = ''.join(c for c in definiendum if c not in math_symbols)\n",
    "    \n",
    "    # Remove non-ASCII characters\n",
    "    definiendum = ''.join(c for c in definiendum if ord(c) < 128)\n",
    "    \n",
    "    # Replace spaces with underscores\n",
    "    definiendum = definiendum.replace(\" \", \"_\")\n",
    "    \n",
    "    # Keep only valid characters (letters, numbers, underscores)\n",
    "    definiendum = re.sub(r'[^A-Za-z0-9_]', '', definiendum)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    definiendum = definiendum.lower()\n",
    "    \n",
    "    # If empty after cleaning, consider invalid\n",
    "    if not definiendum:\n",
    "        return \"\"\n",
    "    \n",
    "    return definiendum\n",
    "\n",
    "def normalize_entity_name(name):\n",
    "    \"\"\"Normalize entity name for matching raw data\"\"\"\n",
    "    return \"_\".join(name.lower().strip().split())\n",
    "\n",
    "class SemanticSearch:\n",
    "    \"\"\"Pure semantic search model (for ablation experiment comparison)\"\"\"\n",
    "    def __init__(self, json_path, model_name=\"sentence-transformers/sentence-t5-large\", verbose=True):\n",
    "        self.json_path = json_path\n",
    "        self.model_name = model_name\n",
    "        self.verbose = verbose\n",
    "        self.embedding_model = None\n",
    "        self.embeddings = None\n",
    "        self.flattened_concepts = []\n",
    "        self.original_dataset = None\n",
    "        \n",
    "        self.load_data()\n",
    "        self.load_model()\n",
    "    \n",
    "    def log(self, message, level=\"info\"):\n",
    "        \"\"\"Log messages based on verbose setting\"\"\"\n",
    "        if self.verbose or level == \"error\":\n",
    "            print(message)\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load data\"\"\"\n",
    "        self.log(\"Loading semantic search data...\")\n",
    "        \n",
    "        if not os.path.exists(self.json_path):\n",
    "            raise FileNotFoundError(f\"File not found: {self.json_path}\")\n",
    "        \n",
    "        with open(self.json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.original_dataset = json.load(f)\n",
    "        \n",
    "        embeddings_list = []\n",
    "        \n",
    "        for module_name, module_content in self.original_dataset.items():\n",
    "            definitions = module_content.get(\"definitions\", [])\n",
    "            \n",
    "            for definition in definitions:\n",
    "                semantic_analysis = definition.get(\"semantic_analysis\", {})\n",
    "                concepts = semantic_analysis.get(\"concepts\", [])\n",
    "                \n",
    "                for concept in concepts:\n",
    "                    vec = concept.get(\"embedding_vector\")\n",
    "                    if vec is not None:\n",
    "                        embeddings_list.append(vec)\n",
    "                        concept_copy = concept.copy()\n",
    "                        if \"name\" not in concept_copy:\n",
    "                            concept_copy[\"name\"] = concept_copy.get(\"concept_name\", \"Unknown\")\n",
    "                        if \"module_path\" not in concept_copy:\n",
    "                            concept_copy[\"module_path\"] = module_name\n",
    "                        \n",
    "                        self.flattened_concepts.append(concept_copy)\n",
    "        \n",
    "        if embeddings_list:\n",
    "            self.embeddings = np.array(embeddings_list)\n",
    "            self.log(f\"Loaded {len(self.embeddings)} semantic concept embeddings\")\n",
    "        else:\n",
    "            raise ValueError(\"No valid semantic embeddings found\")\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load Sentence Transformer model\"\"\"\n",
    "        self.log(f\"Loading semantic model: {self.model_name}\")\n",
    "        try:\n",
    "            self.embedding_model = SentenceTransformer(self.model_name)\n",
    "            self.log(\"Semantic model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            self.log(f\"Failed to load semantic model: {e}\", \"error\")\n",
    "            self.log(\"Falling back to default model...\")\n",
    "            self.embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    \n",
    "    def search_similar_concepts(self, query, top_k=10, min_score=0.1):\n",
    "        \"\"\"Execute semantic search\"\"\"\n",
    "        if self.verbose:\n",
    "            self.log(f\"Semantic search for: '{query[:50]}...'\")\n",
    "        \n",
    "        if self.embeddings is None or len(self.embeddings) == 0:\n",
    "            self.log(\"No embeddings available\", \"error\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Generate query embedding\n",
    "            query_vec = self.embedding_model.encode([query])\n",
    "            if isinstance(query_vec, list):\n",
    "                query_vec = np.array(query_vec)\n",
    "            if query_vec.ndim > 1:\n",
    "                query_vec = query_vec[0]\n",
    "            \n",
    "            # Normalize vectors\n",
    "            query_vec = query_vec / (np.linalg.norm(query_vec) + 1e-8)\n",
    "            norm_embeddings = self.embeddings / (np.linalg.norm(self.embeddings, axis=1, keepdims=True) + 1e-8)\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            cosine_scores = np.dot(norm_embeddings, query_vec)\n",
    "            \n",
    "            # Get indices of all results (sorted by similarity descending)\n",
    "            all_indices = np.argsort(cosine_scores)[::-1]\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log(f\"Semantic search failed: {e}\", \"error\")\n",
    "            return []\n",
    "        \n",
    "        # Collect results (with deduplication)\n",
    "        results = []\n",
    "        seen_concepts = set()  # Track seen concept names\n",
    "        \n",
    "        # Iterate through all results until desired count is reached or all results are processed\n",
    "        for i in all_indices:\n",
    "            if len(results) >= top_k:\n",
    "                break\n",
    "                \n",
    "            if i < len(self.flattened_concepts):\n",
    "                concept = self.flattened_concepts[i]\n",
    "                score = cosine_scores[i]\n",
    "                \n",
    "                # Skip scores below threshold\n",
    "                if score < min_score:\n",
    "                    continue\n",
    "                \n",
    "                # Get and clean concept name\n",
    "                concept_name = concept.get('name', '')\n",
    "                cleaned_name = clean_definiendum(concept_name)\n",
    "                \n",
    "                # Check for duplicates\n",
    "                if cleaned_name and cleaned_name not in seen_concepts:\n",
    "                    results.append({\n",
    "                        'concept': concept,\n",
    "                        'semantic_score': float(score),\n",
    "                        'index': int(i)\n",
    "                    })\n",
    "                    seen_concepts.add(cleaned_name)\n",
    "        \n",
    "        if self.verbose:\n",
    "            self.log(f\"   {len(results)} unique results above threshold {min_score}\")\n",
    "        return results\n",
    "    \n",
    "    def semantic_search(self, query, top_k=10, min_score=0.1, return_top_k=None):\n",
    "        \"\"\"Execute semantic search and return related modules\"\"\"\n",
    "        results = self.search_similar_concepts(query, top_k, min_score)\n",
    "        \n",
    "        # Extract all related modules\n",
    "        import_modules = set()\n",
    "        for result in results:\n",
    "            module = result['concept'].get('module_path', '')\n",
    "            if module:\n",
    "                import_modules.add(module)\n",
    "        \n",
    "        import_modules = sorted(import_modules)\n",
    "        \n",
    "        # Control number of returned modules\n",
    "        if return_top_k is not None:\n",
    "            import_modules = import_modules[:return_top_k]\n",
    "        \n",
    "        return import_modules\n",
    "\n",
    "class HybridSemanticSearch:\n",
    "    \"\"\"Hybrid search model (semantic search + MuRP)\"\"\"\n",
    "    def __init__(self, json_path, murp_embedding_path, \n",
    "                 semantic_model=\"sentence-transformers/sentence-t5-large\",\n",
    "                 murp_model_type=\"poincare\",\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        Hybrid retrieval system: semantic search first, then MuRP for secondary retrieval\n",
    "        \n",
    "        Args:\n",
    "            json_path: JSON file path (contains concepts and embeddings)\n",
    "            murp_embedding_path: MuRP embedding file path\n",
    "            semantic_model: Sentence Transformer model name\n",
    "            murp_model_type: MuRP model type (\"poincare\" or \"euclidean\")\n",
    "            verbose: Whether to show detailed logs\n",
    "        \"\"\"\n",
    "        self.json_path = json_path\n",
    "        self.murp_embedding_path = murp_embedding_path\n",
    "        self.semantic_model_name = semantic_model\n",
    "        self.murp_model_type = murp_model_type\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Initialize components\n",
    "        self.semantic_model = None\n",
    "        self.murp_embeddings = None\n",
    "        self.semantic_embeddings = None\n",
    "        self.flattened_concepts = []\n",
    "        self.murp_vocab = []\n",
    "        self.original_dataset = None  # Store original dataset\n",
    "        self.evaluation_data = None   # Store evaluation data\n",
    "        \n",
    "        # Load data\n",
    "        self.load_semantic_data()\n",
    "        self.load_semantic_model()\n",
    "        self.load_murp_embeddings()\n",
    "    \n",
    "    def log(self, message, level=\"info\"):\n",
    "        \"\"\"Log messages based on verbose setting\"\"\"\n",
    "        if self.verbose or level == \"error\":\n",
    "            print(message)\n",
    "    \n",
    "    def load_semantic_data(self):\n",
    "        \"\"\"Load semantic search data\"\"\"\n",
    "        self.log(\"Loading semantic search data...\")\n",
    "        \n",
    "        if not os.path.exists(self.json_path):\n",
    "            raise FileNotFoundError(f\"File not found: {self.json_path}\")\n",
    "        \n",
    "        with open(self.json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.original_dataset = json.load(f)\n",
    "        \n",
    "        embeddings_list = []\n",
    "        \n",
    "        for module_name, module_content in self.original_dataset.items():\n",
    "            definitions = module_content.get(\"definitions\", [])\n",
    "            \n",
    "            for definition in definitions:\n",
    "                semantic_analysis = definition.get(\"semantic_analysis\", {})\n",
    "                concepts = semantic_analysis.get(\"concepts\", [])\n",
    "                \n",
    "                for concept in concepts:\n",
    "                    vec = concept.get(\"embedding_vector\")\n",
    "                    if vec is not None:\n",
    "                        embeddings_list.append(vec)\n",
    "                        concept_copy = concept.copy()\n",
    "                        if \"name\" not in concept_copy:\n",
    "                            concept_copy[\"name\"] = concept_copy.get(\"concept_name\", \"Unknown\")\n",
    "                        if \"module_path\" not in concept_copy:\n",
    "                            concept_copy[\"module_path\"] = module_name\n",
    "                        \n",
    "                        self.flattened_concepts.append(concept_copy)\n",
    "        \n",
    "        if embeddings_list:\n",
    "            self.semantic_embeddings = np.array(embeddings_list)\n",
    "            self.log(f\"Loaded {len(self.semantic_embeddings)} semantic concept embeddings\")\n",
    "        else:\n",
    "            raise ValueError(\"No valid semantic embeddings found\")\n",
    "    \n",
    "    def load_semantic_model(self):\n",
    "        \"\"\"Load Sentence Transformer model\"\"\"\n",
    "        self.log(f\"Loading semantic model: {self.semantic_model_name}\")\n",
    "        try:\n",
    "            self.semantic_model = SentenceTransformer(self.semantic_model_name)\n",
    "            self.log(\"Semantic model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            self.log(f\"Failed to load semantic model: {e}\", \"error\")\n",
    "            raise\n",
    "    \n",
    "    def load_murp_embeddings(self):\n",
    "        \"\"\"Load MuRP embeddings\"\"\"\n",
    "        self.log(f\"Loading MuRP embeddings from: {self.murp_embedding_path}\")\n",
    "        self.log(f\"   Model type: {self.murp_model_type}\")\n",
    "        \n",
    "        try:\n",
    "            self.murp_embeddings = load_embedding(\n",
    "                self.murp_embedding_path,\n",
    "                format=\"dict\",\n",
    "                normalize=True,   # Required for hyperbolic embeddings\n",
    "                lower=True,        # Ensure loaded embeddings are lowercase\n",
    "                clean_words=False\n",
    "            )\n",
    "            \n",
    "            self.murp_vocab = self.murp_embeddings.words\n",
    "            self.log(f\"Loaded MuRP embeddings with {len(self.murp_vocab)} vocabulary items\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log(f\"Failed to load MuRP embeddings: {e}\", \"error\")\n",
    "            raise\n",
    "    \n",
    "    def semantic_search(self, query, top_k=20, min_score=0.0):\n",
    "        \"\"\"Phase 1: Semantic search (with deduplication)\"\"\"\n",
    "        if self.verbose:\n",
    "            self.log(f\"Phase 1: Semantic search for: '{query[:50]}...'\")\n",
    "        \n",
    "        if self.semantic_embeddings is None or len(self.semantic_embeddings) == 0:\n",
    "            self.log(\"No semantic embeddings available\", \"error\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Generate query embedding\n",
    "            query_vec = self.semantic_model.encode([query])\n",
    "            if isinstance(query_vec, list):\n",
    "                query_vec = np.array(query_vec)\n",
    "            if query_vec.ndim > 1:\n",
    "                query_vec = query_vec[0]\n",
    "            \n",
    "            # Normalize vectors\n",
    "            query_vec = query_vec / (np.linalg.norm(query_vec) + 1e-8)\n",
    "            norm_embeddings = self.semantic_embeddings / (np.linalg.norm(self.semantic_embeddings, axis=1, keepdims=True) + 1e-8)\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            cosine_scores = np.dot(norm_embeddings, query_vec)\n",
    "            \n",
    "            # Get indices of all results (sorted by similarity descending)\n",
    "            all_indices = np.argsort(cosine_scores)[::-1]\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log(f\"Semantic search failed: {e}\", \"error\")\n",
    "            return []\n",
    "        \n",
    "        # Collect results (with deduplication)\n",
    "        results = []\n",
    "        seen_concepts = set()  # Track seen concept names\n",
    "        \n",
    "        # Iterate through all results until desired count is reached or all results are processed\n",
    "        for i in all_indices:\n",
    "            if len(results) >= top_k:\n",
    "                break\n",
    "                \n",
    "            if i < len(self.flattened_concepts):\n",
    "                concept = self.flattened_concepts[i]\n",
    "                score = cosine_scores[i]\n",
    "                \n",
    "                # Skip scores below threshold\n",
    "                if score < min_score:\n",
    "                    continue\n",
    "                \n",
    "                # Get and clean concept name\n",
    "                concept_name = concept.get('name', '')\n",
    "                cleaned_name = clean_definiendum(concept_name)\n",
    "                \n",
    "                # Check for duplicates\n",
    "                if cleaned_name and cleaned_name not in seen_concepts:\n",
    "                    results.append({\n",
    "                        'concept': concept,\n",
    "                        'semantic_score': float(score),\n",
    "                        'index': int(i)\n",
    "                    })\n",
    "                    seen_concepts.add(cleaned_name)\n",
    "        \n",
    "        if self.verbose:\n",
    "            self.log(f\"   {len(results)} unique results above threshold {min_score}\")\n",
    "        return results\n",
    "    \n",
    "    def murp_search(self, concept_names, top_k=10):\n",
    "        \"\"\"Phase 2: MuRP retrieval\"\"\"\n",
    "        if self.verbose:\n",
    "            self.log(f\"Phase 2: MuRP search for {len(concept_names)} concepts\")\n",
    "        \n",
    "        if not self.murp_embeddings:\n",
    "            self.log(\"No MuRP embeddings available\", \"error\")\n",
    "            return {}\n",
    "        \n",
    "        murp_results = {}\n",
    "        found_concepts = []\n",
    "        missing_concepts = []\n",
    "        \n",
    "        for concept_name in concept_names:\n",
    "            # Since all names are lowercase, we only need to try possible variants\n",
    "            possible_names = [\n",
    "                concept_name,\n",
    "                concept_name.replace('.', '_'),  # Ensure consistent dot handling\n",
    "                concept_name.replace('_', '.'),\n",
    "                concept_name.split('.')[-1] if '.' in concept_name else concept_name\n",
    "            ]\n",
    "            \n",
    "            query_name = None\n",
    "            for name in possible_names:\n",
    "                if name in self.murp_embeddings:\n",
    "                    query_name = name\n",
    "                    break\n",
    "            \n",
    "            if query_name is None:\n",
    "                missing_concepts.append(concept_name)\n",
    "                continue\n",
    "            \n",
    "            found_concepts.append((concept_name, query_name))\n",
    "            \n",
    "            try:\n",
    "                neighbors = self.retrieve_murp_neighbors(query_name, top_k)\n",
    "                murp_results[concept_name] = {\n",
    "                    'query_name_used': query_name,\n",
    "                    'neighbors': neighbors\n",
    "                }\n",
    "            except Exception as e:\n",
    "                self.log(f\"MuRP search failed for {concept_name}: {e}\", \"error\")\n",
    "        \n",
    "        return murp_results\n",
    "    \n",
    "    def retrieve_murp_neighbors(self, query_word, top_k=10):\n",
    "        \"\"\"MuRP neighbor retrieval\"\"\"\n",
    "        if query_word not in self.murp_embeddings:\n",
    "            raise ValueError(f\"Query word '{query_word}' not found in MuRP vocabulary\")\n",
    "        \n",
    "        query_vec = self.murp_embeddings[query_word]\n",
    "        distances = []\n",
    "        \n",
    "        for word in self.murp_vocab:\n",
    "            vec = self.murp_embeddings[word]\n",
    "            if self.murp_model_type == \"poincare\":\n",
    "                dist = poincare_distance(query_vec, vec)\n",
    "            else:\n",
    "                dist = euclidean(query_vec, vec)\n",
    "            distances.append((word, dist))\n",
    "        \n",
    "        # Sort distances and exclude the query word itself\n",
    "        distances_sorted = sorted(distances, key=lambda x: x[1])\n",
    "        distances_sorted = [item for item in distances_sorted if item[0] != query_word]\n",
    "        \n",
    "        # Take top-k nearest neighbors\n",
    "        top_neighbors = distances_sorted[:top_k]\n",
    "        \n",
    "        # Normalize distances\n",
    "        if top_neighbors:\n",
    "            dists = np.array([dist for _, dist in top_neighbors])\n",
    "            min_dist, max_dist = dists.min(), dists.max()\n",
    "            if max_dist > min_dist:\n",
    "                norm_dists = (dists - min_dist) / (max_dist - min_dist)\n",
    "            else:\n",
    "                norm_dists = np.zeros_like(dists)\n",
    "            \n",
    "            return [(word, dist, norm) for (word, dist), norm in zip(top_neighbors, norm_dists)]\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def hybrid_search(self, query, semantic_top_k=20, murp_top_k=10, semantic_min_score=0.1, return_top_k=10):\n",
    "        \"\"\"Hybrid retrieval: semantic search + MuRP secondary retrieval\"\"\"\n",
    "        if self.verbose:\n",
    "            self.log(f\"Starting hybrid search for: '{query[:50]}...'\")\n",
    "        \n",
    "        # Phase 1: Semantic search (with deduplication)\n",
    "        semantic_results = self.semantic_search(query, semantic_top_k, semantic_min_score)\n",
    "        \n",
    "        if not semantic_results:\n",
    "            return []\n",
    "        \n",
    "        # Extract and clean concept names for MuRP retrieval\n",
    "        raw_concept_names = [result['concept'].get('name', '') for result in semantic_results]\n",
    "        raw_concept_names = [name for name in raw_concept_names if name and name != 'Unknown']\n",
    "        \n",
    "        # Apply cleaning function (includes dot correction)\n",
    "        cleaned_concept_names = []\n",
    "        for name in raw_concept_names:\n",
    "            cleaned = clean_definiendum(name)\n",
    "            if cleaned:\n",
    "                cleaned_concept_names.append(cleaned)\n",
    "        \n",
    "        if not cleaned_concept_names:\n",
    "            return []\n",
    "        \n",
    "        # Phase 2: MuRP retrieval (using cleaned lowercase names)\n",
    "        murp_results = self.murp_search(cleaned_concept_names, murp_top_k)\n",
    "        \n",
    "        # Create a dictionary to store modules and their relevance scores\n",
    "        module_scores = defaultdict(float)\n",
    "        \n",
    "        # 1. Assign scores to modules from semantic search results\n",
    "        for result in semantic_results:\n",
    "            module = result['concept'].get('module_path', '')\n",
    "            score = result.get('semantic_score', 0)\n",
    "            if module:\n",
    "                module_scores[module] += score  # Accumulate scores\n",
    "        \n",
    "        # 2. Assign scores to modules from MuRP results (based on distance)\n",
    "        for concept_name, murp_data in murp_results.items():\n",
    "            for neighbor, _, norm_dist in murp_data['neighbors']:\n",
    "                # Find the module to which the neighbor belongs\n",
    "                neighbor_module = None\n",
    "                for module_name, module_data in self.original_dataset.items():\n",
    "                    for definition in module_data.get(\"definitions\", []):\n",
    "                        def_name = definition.get(\"name\", \"\")\n",
    "                        if def_name and clean_definiendum(def_name) == neighbor:\n",
    "                            neighbor_module = module_name\n",
    "                            break\n",
    "                    if neighbor_module:\n",
    "                        break\n",
    "                \n",
    "                if neighbor_module:\n",
    "                    # Smaller distance means higher score (1 - normalized distance)\n",
    "                    module_scores[neighbor_module] += (1 - norm_dist)\n",
    "        \n",
    "        # 3. Sort by score and select top return_top_k\n",
    "        sorted_modules = sorted(module_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_modules = [module for module, score in sorted_modules[:return_top_k]]\n",
    "        \n",
    "        return top_modules\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Model evaluator for comparing pure semantic search and hybrid search\"\"\"\n",
    "    def __init__(self, json_path, murp_embedding_path=None):\n",
    "        self.json_path = json_path\n",
    "        self.murp_embedding_path = murp_embedding_path\n",
    "        self.evaluation_data = None\n",
    "        \n",
    "        # Prepare evaluation data\n",
    "        self.prepare_evaluation_data()\n",
    "    \n",
    "    def prepare_evaluation_data(self, sample_size=1000):\n",
    "        \"\"\"\n",
    "        Prepare evaluation data: randomly sample definitions from JSON file\n",
    "        Return format: [\n",
    "            {\n",
    "                \"query\": \"informal description\",\n",
    "                \"target_module\": \"definition module\",\n",
    "                \"definition_name\": \"definition name\"\n",
    "            }\n",
    "        ]\n",
    "        \"\"\"\n",
    "        print(\"Preparing evaluation data...\")\n",
    "        \n",
    "        # Load original dataset\n",
    "        if not os.path.exists(self.json_path):\n",
    "            raise FileNotFoundError(f\"File not found: {self.json_path}\")\n",
    "        \n",
    "        with open(self.json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            original_dataset = json.load(f)\n",
    "        \n",
    "        evaluation_samples = []\n",
    "        \n",
    "        # Collect all valid definitions\n",
    "        all_definitions = []\n",
    "        for module_name, module_data in original_dataset.items():\n",
    "            for definition in module_data.get(\"definitions\", []):\n",
    "                if not isinstance(definition, dict):\n",
    "                    continue\n",
    "                \n",
    "                informal = definition.get(\"semantic_analysis\", {}).get(\"informal\", \"\")\n",
    "                def_name = definition.get(\"name\", \"\")\n",
    "                \n",
    "                # Ensure valid non-empty informal description and definition name\n",
    "                if informal and def_name:\n",
    "                    all_definitions.append({\n",
    "                        \"module\": module_name,\n",
    "                        \"name\": def_name,\n",
    "                        \"query\": informal\n",
    "                    })\n",
    "        \n",
    "        print(f\"   Found {len(all_definitions)} valid definitions with informal descriptions\")\n",
    "        \n",
    "        # Random sampling\n",
    "        if len(all_definitions) < sample_size:\n",
    "            print(f\"Warning: Only {len(all_definitions)} definitions available, using all\")\n",
    "            sample_size = len(all_definitions)\n",
    "        \n",
    "        random.shuffle(all_definitions)\n",
    "        self.evaluation_data = all_definitions[:sample_size]\n",
    "        \n",
    "        print(f\"Prepared {len(self.evaluation_data)} evaluation samples\")\n",
    "        return self.evaluation_data\n",
    "    \n",
    "    def evaluate_model(self, model, model_name, semantic_top_k=10, murp_top_k=5, semantic_min_score=0.1, return_top_k=10):\n",
    "        \"\"\"\n",
    "        Evaluate model performance\n",
    "        \"\"\"\n",
    "        if not self.evaluation_data:\n",
    "            self.prepare_evaluation_data()\n",
    "        \n",
    "        sample_size = len(self.evaluation_data)\n",
    "        \n",
    "        print(f\"Evaluating {model_name} with {sample_size} samples...\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        results = {\n",
    "            \"model_name\": model_name,\n",
    "            \"total_samples\": sample_size,\n",
    "            \"correct_predictions\": 0,\n",
    "            \"incorrect_predictions\": 0,\n",
    "            \"module_recall\": 0.0,\n",
    "            \"query_times\": [],\n",
    "            \"module_coverage\": defaultdict(int),\n",
    "            \"detailed_results\": []\n",
    "        }\n",
    "        \n",
    "        # Progress counter\n",
    "        progress_interval = max(1, sample_size // 10)  # Show progress every 10%\n",
    "        \n",
    "        # Evaluate each sample\n",
    "        for i, sample in enumerate(self.evaluation_data):\n",
    "            if i % progress_interval == 0:\n",
    "                print(f\"   Processing sample {i+1}/{sample_size} ({((i+1)/sample_size)*100:.1f}%)\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Execute search\n",
    "                if model_name == \"Semantic Search\":\n",
    "                    retrieved_modules = model.semantic_search(\n",
    "                        sample['query'], \n",
    "                        top_k=semantic_top_k,\n",
    "                        min_score=semantic_min_score,\n",
    "                        return_top_k=return_top_k\n",
    "                    )\n",
    "                else:  # Hybrid Search\n",
    "                    retrieved_modules = model.hybrid_search(\n",
    "                        query=sample['query'],\n",
    "                        semantic_top_k=semantic_top_k,\n",
    "                        murp_top_k=murp_top_k,\n",
    "                        semantic_min_score=semantic_min_score,\n",
    "                        return_top_k=return_top_k\n",
    "                    )\n",
    "                \n",
    "                elapsed_time = time.time() - start_time\n",
    "                results[\"query_times\"].append(elapsed_time)\n",
    "                \n",
    "                # Get retrieved modules\n",
    "                retrieved_modules_set = set(retrieved_modules)\n",
    "                target_module = sample[\"module\"]\n",
    "                \n",
    "                # Check if target module is in retrieval results\n",
    "                is_correct = target_module in retrieved_modules_set\n",
    "                \n",
    "                # Update results\n",
    "                if is_correct:\n",
    "                    results[\"correct_predictions\"] += 1\n",
    "                    results[\"module_coverage\"][target_module] += 1\n",
    "                else:\n",
    "                    results[\"incorrect_predictions\"] += 1\n",
    "                \n",
    "                # Store detailed results\n",
    "                detailed_result = {\n",
    "                    \"sample_id\": i,\n",
    "                    \"definition_name\": sample[\"name\"],\n",
    "                    \"target_module\": target_module,\n",
    "                    \"query\": sample[\"query\"],\n",
    "                    \"retrieved_modules\": list(retrieved_modules_set),\n",
    "                    \"is_correct\": is_correct,\n",
    "                    \"time_taken\": elapsed_time\n",
    "                }\n",
    "                results[\"detailed_results\"].append(detailed_result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                results[\"query_times\"].append(elapsed_time)\n",
    "                results[\"incorrect_predictions\"] += 1\n",
    "                print(f\"Error during search for sample {i+1}: {e}\")\n",
    "                detailed_result = {\n",
    "                    \"sample_id\": i,\n",
    "                    \"definition_name\": sample[\"name\"],\n",
    "                    \"target_module\": target_module,\n",
    "                    \"query\": sample[\"query\"],\n",
    "                    \"error\": str(e),\n",
    "                    \"is_correct\": False,\n",
    "                    \"time_taken\": elapsed_time\n",
    "                }\n",
    "                results[\"detailed_results\"].append(detailed_result)\n",
    "        \n",
    "        # Calculate recall\n",
    "        results[\"module_recall\"] = results[\"correct_predictions\"] / sample_size if sample_size > 0 else 0\n",
    "        \n",
    "        # Calculate average query time\n",
    "        results[\"avg_query_time\"] = sum(results[\"query_times\"]) / sample_size if sample_size > 0 else 0\n",
    "        \n",
    "        # Calculate module coverage\n",
    "        total_correct = results[\"correct_predictions\"]\n",
    "        if total_correct > 0:\n",
    "            for module, count in results[\"module_coverage\"].items():\n",
    "                results[\"module_coverage\"][module] = count / total_correct\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{model_name.upper()} EVALUATION SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Total Samples: {results['total_samples']}\")\n",
    "        print(f\"Correct Predictions: {results['correct_predictions']}\")\n",
    "        print(f\"Incorrect Predictions: {results['incorrect_predictions']}\")\n",
    "        print(f\"Module Recall: {results['module_recall']:.4f}\")\n",
    "        print(f\"Average Query Time: {results['avg_query_time']:.2f} seconds\")\n",
    "        \n",
    "        # Print top modules by coverage\n",
    "        if results[\"module_coverage\"]:\n",
    "            print(\"Top Modules by Coverage:\")\n",
    "            sorted_modules = sorted(results[\"module_coverage\"].items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "            for module, coverage in sorted_modules:\n",
    "                print(f\"  {module}: {coverage:.2f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_evaluation_results(self, results, output_path=\"model_evaluation.json\"):\n",
    "        \"\"\"Save evaluation results to JSON file\"\"\"\n",
    "        print(f\"Saving evaluation results to {output_path}\")\n",
    "        \n",
    "        # Prepare data to save\n",
    "        save_data = {\n",
    "            \"evaluation_summary\": {\n",
    "                \"model_name\": results[\"model_name\"],\n",
    "                \"total_samples\": results[\"total_samples\"],\n",
    "                \"correct_predictions\": results[\"correct_predictions\"],\n",
    "                \"incorrect_predictions\": results[\"incorrect_predictions\"],\n",
    "                \"module_recall\": results[\"module_recall\"],\n",
    "                \"avg_query_time\": results[\"avg_query_time\"],\n",
    "                \"module_coverage\": dict(results[\"module_coverage\"])\n",
    "            },\n",
    "            \"detailed_results\": results[\"detailed_results\"]\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(save_data, f, indent=2, ensure_ascii=False)\n",
    "            print(\"Evaluation results saved successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save evaluation results: {e}\")\n",
    "    \n",
    "    def compare_models(self, semantic_top_k=10, murp_top_k=5, semantic_min_score=0.1, return_top_k=10):\n",
    "        \"\"\"Compare performance of pure semantic search and hybrid search\"\"\"\n",
    "        # Initialize models\n",
    "        semantic_model = SemanticSearch(\n",
    "            self.json_path, \n",
    "            model_name=\"sentence-transformers/sentence-t5-large\",\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        hybrid_model = HybridSemanticSearch(\n",
    "            self.json_path,\n",
    "            self.murp_embedding_path,\n",
    "            semantic_model=\"sentence-transformers/sentence-t5-large\",\n",
    "            murp_model_type=\"poincare\",\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Evaluate pure semantic search\n",
    "        semantic_results = self.evaluate_model(\n",
    "            semantic_model,\n",
    "            \"Semantic Search\",\n",
    "            semantic_top_k=semantic_top_k,\n",
    "            semantic_min_score=semantic_min_score,\n",
    "            return_top_k=return_top_k\n",
    "        )\n",
    "        \n",
    "        # Evaluate hybrid search\n",
    "        hybrid_results = self.evaluate_model(\n",
    "            hybrid_model,\n",
    "            \"Hybrid Search\",\n",
    "            semantic_top_k=semantic_top_k,\n",
    "            murp_top_k=murp_top_k,\n",
    "            semantic_min_score=semantic_min_score,\n",
    "            return_top_k=return_top_k\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        self.save_evaluation_results(semantic_results, \"semantic_search_evaluation.json\")\n",
    "        self.save_evaluation_results(hybrid_results, \"hybrid_search_evaluation.json\")\n",
    "        \n",
    "        # Print comparison results\n",
    "        print(\"=\" * 80)\n",
    "        print(\"ABLATION STUDY RESULTS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{'Model':<20} | {'Recall':<10} | {'Avg Time (s)':<12} | {'Improvement':<12}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        recall_diff = hybrid_results[\"module_recall\"] - semantic_results[\"module_recall\"]\n",
    "        time_diff = hybrid_results[\"avg_query_time\"] - semantic_results[\"avg_query_time\"]\n",
    "        \n",
    "        print(f\"{'Semantic Search':<20} | {semantic_results['module_recall']:.4f}    | {semantic_results['avg_query_time']:.4f}      | {'-':<12}\")\n",
    "        print(f\"{'Hybrid Search':<20} | {hybrid_results['module_recall']:.4f}    | {hybrid_results['avg_query_time']:.4f}      | +{recall_diff:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Recall Improvement: {recall_diff:.4f} ({recall_diff/semantic_results['module_recall']:.2%})\")\n",
    "        print(f\"Time Cost Increase: {time_diff:.4f} seconds ({time_diff/semantic_results['avg_query_time']:.2%})\")\n",
    "        \n",
    "        return semantic_results, hybrid_results\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Usage Example\n",
    "# ----------------------\n",
    "def main():\n",
    "    # Configure paths\n",
    "    json_path = \"./Informalisation_and_Mathematical_DSRL/merged_with_embeddings_and_triples.json\"\n",
    "    murp_embedding_path = \"./Mathlib4_embeddings/outputs_cleaned/embeddings/model_dict_poincare_300_my_dataset_cleaned\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize evaluator\n",
    "        print(\"Initializing Model Evaluator...\")\n",
    "        evaluator = ModelEvaluator(json_path, murp_embedding_path)\n",
    "        \n",
    "        # Execute ablation study comparison\n",
    "        print(\"Starting ablation study...\")\n",
    "        semantic_results, hybrid_results = evaluator.compare_models(\n",
    "            semantic_top_k=10,\n",
    "            murp_top_k=5,\n",
    "            semantic_min_score=0.1,\n",
    "            return_top_k=10  # Control returning 10 modules\n",
    "        )\n",
    "        \n",
    "        # Execute single query example\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Running single query example with Hybrid Search...\")\n",
    "        hybrid_model = HybridSemanticSearch(\n",
    "            json_path,\n",
    "            murp_embedding_path,\n",
    "            semantic_model=\"sentence-transformers/sentence-t5-large\",\n",
    "            murp_model_type=\"poincare\",\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        query = \"For any real matrix A: Matrix m × n, if the columns of A are pairwise orthogonal, then the matrix Aᵀ * A is a diagonal matrix.\"\n",
    "        retrieved_modules = hybrid_model.hybrid_search(\n",
    "            query=query,\n",
    "            semantic_top_k=10,\n",
    "            murp_top_k=5,\n",
    "            semantic_min_score=0.1,\n",
    "            return_top_k=10  # Ensure returning 10 modules\n",
    "        )\n",
    "        \n",
    "        print(f\"Retrieved {len(retrieved_modules)} modules:\")\n",
    "        for i, module in enumerate(retrieved_modules, 1):\n",
    "            print(f\" {i}. {module}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
